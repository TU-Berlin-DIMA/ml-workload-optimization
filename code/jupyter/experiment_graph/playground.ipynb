{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import networkx as nx\n",
    "import os\n",
    "import warnings\n",
    "# matplotlib and seaborn for plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "ROOT_PACKAGE_DIRECTORY = '/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter'\n",
    "ROOT_DATA_DIRECTORY = ROOT_PACKAGE_DIRECTORY + '/data'\n",
    "GRAPH_DATABASE_PATH = ROOT_PACKAGE_DIRECTORY + '/data/graph/exp-reuse-same-workload.graph'\n",
    "sys.path.append(ROOT_PACKAGE_DIRECTORY)\n",
    "\n",
    "# Experiment Graph\n",
    "from experiment_graph.execution_environment import ExecutionEnvironment\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "try:\n",
    "    import pygraphviz\n",
    "    from networkx.drawing.nx_agraph import graphviz_layout\n",
    "except ImportError:\n",
    "    try:\n",
    "        import pydot\n",
    "        from networkx.drawing.nx_pydot import graphviz_layout\n",
    "    except ImportError:\n",
    "        raise ImportError(\"This example needs Graphviz and either \"\n",
    "                          \"PyGraphviz or pydot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import hashlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# Experiment Graph\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from importlib import import_module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = ROOT_PACKAGE_DIRECTORY + '/data'\n",
    "DATABASE_PATH = ROOT_PACKAGE_DIRECTORY + '/data/environment_same_workload'\n",
    "execution_environment = ExecutionEnvironment('naive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bureau = execution_environment.load(root_data + '/home-credit-default-risk/bureau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_categorical(df, group_var, df_name):\n",
    "    \"\"\"Computes counts and normalized counts for each observation\n",
    "    of `group_var` of each unique category in every categorical variable\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe\n",
    "        The dataframe to calculate the value counts for.\n",
    "\n",
    "    group_var : string\n",
    "        The variable by which to group the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "\n",
    "    df_name : string\n",
    "        Variable added to the front of column names to keep track of columns\n",
    "\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with counts and normalized counts of each unique category in every categorical variable\n",
    "        with one row for every unique value of the `group_var`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Select the categorical columns\n",
    "    categorical = df.select_dtypes('object').onehot_encode()\n",
    "\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical = categorical.add_columns(group_var, df[group_var])\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n",
    "\n",
    "    column_names = [group_var]\n",
    "    # Need to create new column names\n",
    "    columns = categorical.data().columns\n",
    "    for c in columns:\n",
    "        if c != group_var:\n",
    "            column_names.append('{}_{}'.format(df_name, c))\n",
    "\n",
    "    return categorical.set_columns(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_counts = count_categorical(bureau, group_var='SK_ID_CURR', df_name='bureau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_BUREAU</th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5715448</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-2</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5715448</td>\n",
       "      <td>-4</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_BUREAU  MONTHS_BALANCE STATUS\n",
       "0       5715448               0      C\n",
       "1       5715448              -1      C\n",
       "2       5715448              -2      C\n",
       "3       5715448              -3      C\n",
       "4       5715448              -4      C"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in bureau balance\n",
    "bureau_balance = execution_environment.load(root_data + '/home-credit-default-risk/bureau_balance.csv')\n",
    "bureau_balance.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'e7c6b65f56f09d375806de4f5f6d4aea'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a8ccfc2b15c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbureau_balance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m             \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_graph.pyc\u001b[0m in \u001b[0;36mcompute_result\u001b[0;34m(self, v_id, verbose)\u001b[0m\n\u001b[1;32m    160\u001b[0m                         \u001b[0;31m# TODO: check if a shallow copy is enough\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                         cur_node['data'].c_name, cur_node['data'].c_hash = copy.deepcopy(\n\u001b[0;32m--> 162\u001b[0;31m                             self.compute_next(self.graph.nodes[pair[0]], edge))\n\u001b[0m\u001b[1;32m    163\u001b[0m                         \u001b[0mcur_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# all the other node types they contain the data themselves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_graph.pyc\u001b[0m in \u001b[0;36mcompute_next\u001b[0;34m(node, edge)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'oper'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'args'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.py\u001b[0m in \u001b[0;36mp_head\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mp_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_and_store_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/data_storage.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self, names, hashes)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mcombined_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombined_hash\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'e7c6b65f56f09d375806de4f5f6d4aea'"
     ]
    }
   ],
   "source": [
    "bureau_balance.select_dtypes('object').head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 'e7c6b65f56f09d375806de4f5f6d4aea' already exist\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'b8d67a3152485c9805340c4a79616203_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d0ee739a7653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbureau_balance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.pyc\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m             \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_graph.pyc\u001b[0m in \u001b[0;36mcompute_result\u001b[0;34m(self, v_id, verbose)\u001b[0m\n\u001b[1;32m    161\u001b[0m                         cur_node['data'].c_name, cur_node['data'].c_hash = copy.deepcopy(\n\u001b[1;32m    162\u001b[0m                             self.compute_next(self.graph.nodes[pair[0]], edge))\n\u001b[0;32m--> 163\u001b[0;31m                         \u001b[0mcur_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# all the other node types they contain the data themselves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.pyc\u001b[0m in \u001b[0;36mcompute_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/data_storage.pyc\u001b[0m in \u001b[0;36mget_size\u001b[0;34m(self, column_hashes)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mcombined_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_hashes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombined_hash\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'b8d67a3152485c9805340c4a79616203_size'"
     ]
    }
   ],
   "source": [
    "bureau_balance.select_dtypes('object').data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'b8d67a3152485c9805340c4a79616203_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-643e93f2ccb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Counts of each type of status for each previous loan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbureau_balance_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbureau_balance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SK_ID_BUREAU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bureau_balance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbureau_balance_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-48b80d707b2b>\u001b[0m in \u001b[0;36mcount_categorical\u001b[0;34m(df, group_var, df_name)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mcolumn_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgroup_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Need to create new column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgroup_var\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m             \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_graph.pyc\u001b[0m in \u001b[0;36mcompute_result\u001b[0;34m(self, v_id, verbose)\u001b[0m\n\u001b[1;32m    161\u001b[0m                         cur_node['data'].c_name, cur_node['data'].c_hash = copy.deepcopy(\n\u001b[1;32m    162\u001b[0m                             self.compute_next(self.graph.nodes[pair[0]], edge))\n\u001b[0;32m--> 163\u001b[0;31m                         \u001b[0mcur_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# all the other node types they contain the data themselves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/execution_environment.py\u001b[0m in \u001b[0;36mcompute_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExecutionEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/jupyter/experiment_graph/data_storage.py\u001b[0m in \u001b[0;36mget_size\u001b[0;34m(self, column_hashes)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mcombined_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_hashes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombined_hash\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'b8d67a3152485c9805340c4a79616203_size'"
     ]
    }
   ],
   "source": [
    "# Counts of each type of status for each previous loan\n",
    "bureau_balance_counts = count_categorical(bureau_balance, group_var='SK_ID_BUREAU', df_name='bureau_balance')\n",
    "bureau_balance_counts.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SK_ID_CURR'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau['SK_ID_CURR'].c_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'640e1ddef2c2af1a9a0ff5b77797795b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-54266ef74fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexecution_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'640e1ddef2c2af1a9a0ff5b77797795b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '640e1ddef2c2af1a9a0ff5b77797795b'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.c_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = encode.add_columns('SK_ID_CURR', bureau['SK_ID_CURR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.c_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ead040746fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'previous_loan_counts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'previous_loan_counts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train = train.replace_columns('previous_loan_counts', train['previous_loan_counts'].fillna(0))\n",
    "train.head().data()\n",
    "\n",
    "# Plots the distribution of a variable colored by value of the target\n",
    "def kde_target(var_name, df):\n",
    "    # Calculate the correlation coefficient between the new variable and the target\n",
    "    corr = df['TARGET'].corr(df[var_name])\n",
    "\n",
    "    # Calculate medians for repaid vs not repaid\n",
    "    avg_repaid = df[df['TARGET'] == 0][var_name].median()\n",
    "    avg_not_repaid = df[df['TARGET'] == 1][var_name].median()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot the distribution for target == 0 and target == 1\n",
    "    sns.kdeplot(df[df['TARGET'] == 0][var_name].dropna().data(), label='TARGET == 0')\n",
    "    sns.kdeplot(df[df['TARGET'] == 1][var_name].dropna().data(), label='TARGET == 1')\n",
    "\n",
    "    # label the plot\n",
    "    plt.xlabel(var_name)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('%s Distribution' % var_name)\n",
    "    plt.legend()\n",
    "\n",
    "    # print out the correlation\n",
    "    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr.data()))\n",
    "    # Print out average values\n",
    "    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid.data())\n",
    "    print('Median value for loan that was repaid =     %0.4f' % avg_repaid.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_target('EXT_SOURCE_3', train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['previous_loan_counts'].data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace_columns('previous_loan_counts', train['previous_loan_counts'].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct\n",
    "def missing_values_table(dataset):\n",
    "    # Total missing values\n",
    "    mis_val = dataset.isnull().sum().data()\n",
    "\n",
    "    mis_val_percent = 100 * mis_val / len(dataset.data())\n",
    "\n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(columns={\n",
    "        0: 'Missing Values',\n",
    "        1: '% of Total Values'\n",
    "    })\n",
    "    # Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    # Print some summary information\n",
    "    print(\"Your selected dataframe has \" + str(dataset.shape().data()[1]) + \" columns.\\n\"\n",
    "                                                                          \"There are \" + str(\n",
    "        mis_val_table_ren_columns.shape[0]) +\n",
    "          \" columns that have missing values.\")\n",
    "\n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "\n",
    "missing_values = missing_values_table(app_train)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train.dtypes().data().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train.select_dtypes('object').nunique().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "columns = app_train.select_dtypes('object').data().columns\n",
    "for col in columns:\n",
    "    # we are not using nunique because it discard nan\n",
    "    if app_train[col].nunique(dropna=False).data() <= 2:\n",
    "        model = app_train[col].fit_sk_model(le)\n",
    "\n",
    "        transformed_train = model.transform_col(app_train[col], col)\n",
    "        app_train = app_train.drop(col)\n",
    "        app_train = app_train.add_columns(col, transformed_train)\n",
    "\n",
    "        transformed_test = model.transform_col(app_test[col], col)\n",
    "        app_test = app_test.drop(col)\n",
    "        app_test = app_test.add_columns(col, transformed_test)\n",
    "\n",
    "        # Keep track of how many columns were label encoded\n",
    "        le_count += 1\n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train = app_train.onehot_encode()\n",
    "app_test = app_test.onehot_encode()\n",
    "\n",
    "print('Training Features shape: ', app_train.shape().data())\n",
    "print('Testing Features shape: ', app_test.shape().data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = app_train['TARGET']\n",
    "train_columns = app_train.data().columns\n",
    "test_columns = app_test.data().columns\n",
    "for c in train_columns:\n",
    "    if c not in test_columns:\n",
    "        app_train = app_train.drop(c)\n",
    "\n",
    "app_train = app_train.add_columns('TARGET', train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features shape: ', app_train.shape().data())\n",
    "print('Testing Features shape: ', app_test.shape().data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(app_train['DAYS_BIRTH'] / 365).describe().data()\n",
    "\n",
    "app_train['DAYS_EMPLOYED'].describe().data()\n",
    "\n",
    "app_train['DAYS_EMPLOYED'].data().plot.hist(title='Days Employment Histogram')\n",
    "plt.xlabel('Days Employment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n",
    "non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n",
    "print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean().data()))\n",
    "print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean().data()))\n",
    "print('There are %d anomalous days of employment' % anom.shape().data()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_employed_anom = app_train['DAYS_EMPLOYED'] == 365243\n",
    "app_train = app_train.add_columns('DAYS_EMPLOYED_ANOM', days_employed_anom)\n",
    "temp = app_train['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "app_train = app_train.drop('DAYS_EMPLOYED')\n",
    "app_train = app_train.add_columns('DAYS_EMPLOYED', temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train[\"DAYS_EMPLOYED\"].data().plot.hist(title='Days Employment Histogram');\n",
    "plt.xlabel('Days Employment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_employed_anom = app_test[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_test = app_test.add_columns('DAYS_EMPLOYED_ANOM', days_employed_anom)\n",
    "temp = app_test['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "app_test = app_test.drop('DAYS_EMPLOYED')\n",
    "app_test = app_test.add_columns('DAYS_EMPLOYED', temp)\n",
    "print('There are %d anomalies in the test data out of %d entries'\n",
    "      % (app_test['DAYS_EMPLOYED_ANOM'].sum().data(),\n",
    "         app_test.shape().data()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations = app_train.corr().data()\n",
    "# top = correlations['TARGET'].sort_values()\n",
    "# # Display correlations\n",
    "# print('Most Positive Correlations:\\n', top.tail(15))\n",
    "# print('\\nMost Negative Correlations:\\n', top.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_age = app_train['DAYS_BIRTH'].abs()\n",
    "app_train = app_train.drop('DAYS_BIRTH')\n",
    "app_train = app_train.add_columns('DAYS_BIRTH', abs_age)\n",
    "app_train['DAYS_BIRTH'].corr(app_train['TARGET']).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style of plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot the distribution of ages in years\n",
    "plt.hist((app_train['DAYS_BIRTH'] / 365).data(), edgecolor='k', bins=25)\n",
    "plt.title('Age of Client')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 8))\n",
    "# # KDE plot of loans that were repaid on time\n",
    "# sns.kdeplot((app_train[app_train['TARGET'] == 0]['DAYS_BIRTH'] / 365).data(), label='target == 0')\n",
    "# # KDE plot of loans which were not repaid on time\n",
    "# sns.kdeplot((app_train[app_train['TARGET'] == 1]['DAYS_BIRTH'] / 365).data(), label='target == 1')\n",
    "# # Labeling of plot\n",
    "# plt.xlabel('Age (years)')\n",
    "# plt.ylabel('Density')\n",
    "# plt.title('Distribution of Ages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age information into a separate dataframe\n",
    "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
    "years_birth = age_data['DAYS_BIRTH'] / 365\n",
    "age_data = age_data.add_columns('YEARS_BIRTH', years_birth)\n",
    "binned = age_data['YEARS_BIRTH'].binning(20, 70, 11)\n",
    "binned.setname('YEARS_BINNED')\n",
    "age_data = age_data.add_columns('YEARS_BINNED', binned)\n",
    "age_data.head(10).data()\n",
    "\n",
    "age_groups = age_data.groupby('YEARS_BINNED').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Graph the age bins and the average of the target as a bar plot\n",
    "plt.bar(age_groups.data().index.astype(str), age_groups.data()['TARGET'] * 100)\n",
    "\n",
    "# Plot labeling\n",
    "plt.xticks(rotation=75)\n",
    "plt.xlabel('Age Group (years)')\n",
    "plt.ylabel('Failure to Repay (%)')\n",
    "plt.title('Failure to Repay by Age Group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "# ext_data_corrs = ext_data.corr().data()\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "# # Heatmap of correlations\n",
    "# sns.heatmap(ext_data_corrs, cmap=plt.cm.RdYlBu_r, vmin=-0.25, annot=True, vmax=0.6)\n",
    "# plt.title('Correlation Heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 12))\n",
    "\n",
    "# # iterate through the sources\n",
    "# for i, column in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
    "#     # create a new subplot for each source\n",
    "#     plt.subplot(3, 1, i + 1)\n",
    "#     # plot repaid loans\n",
    "#     source_data = app_train[[column, 'TARGET']][app_train['TARGET'] == 0]\n",
    "#     sns.kdeplot(source_data[app_train[column].notna()][column].data(), label='target == 0')\n",
    "#     # plot loans that were not repaid\n",
    "#     source_data = app_train[[column, 'TARGET']][app_train['TARGET'] == 1]\n",
    "#     sns.kdeplot(source_data[app_train[column].notna()][column].data(), label='target == 1')\n",
    "\n",
    "#     # Label the plots\n",
    "#     plt.title('Distribution of %s by Target Value' % column)\n",
    "#     plt.xlabel('%s' % column)\n",
    "#     plt.ylabel('Density')\n",
    "\n",
    "# plt.tight_layout(h_pad=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copy the data for plotting\n",
    "# plot_data = ext_data.drop('DAYS_BIRTH')\n",
    "\n",
    "# # Add in the age of the client in years\n",
    "# plot_data = plot_data.add_columns('YEARS_BIRTH', age_data['YEARS_BIRTH'])\n",
    "# # Drop na values and limit to first 100000 rows\n",
    "# plot_data = plot_data.head(100000).dropna()\n",
    "\n",
    "# # Create the pair grid object\n",
    "# grid = sns.PairGrid(data=plot_data.data(), size=3, diag_sharey=False,\n",
    "#                     hue='TARGET',\n",
    "#                     vars=[x for x in list(plot_data.data().columns) if x != 'TARGET'])\n",
    "\n",
    "# # Upper is a scatter plot\n",
    "# grid.map_upper(plt.scatter, alpha=0.2)\n",
    "\n",
    "# # Diagonal is a histogram\n",
    "# grid.map_diag(sns.kdeplot)\n",
    "\n",
    "# # Bottom is density plot\n",
    "# grid.map_lower(sns.kdeplot, cmap=plt.cm.OrRd_r)\n",
    "\n",
    "# plt.suptitle('Ext Source and Age Features Pairs Plot', size=32, y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe for polynomial features\n",
    "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
    "poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "\n",
    "# imputer for handling missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy='median')\n",
    "\n",
    "poly_target = poly_features['TARGET']\n",
    "\n",
    "poly_features = poly_features.drop(columns=['TARGET'])\n",
    "\n",
    "# Need to impute missing values\n",
    "imputer_model = poly_features.fit_sk_model(imputer)\n",
    "poly_features = imputer_model.transform(poly_features)\n",
    "poly_features_test = imputer_model.transform(poly_features_test)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree=3)\n",
    "\n",
    "# Train the polynomial features\n",
    "poly_transformer_model = poly_features.fit_sk_model(poly_transformer)\n",
    "\n",
    "poly_features = poly_transformer_model.transform(poly_features)\n",
    "poly_features_test = poly_transformer_model.transform(poly_features_test)\n",
    "\n",
    "new_names = poly_transformer_model.data().get_feature_names(input_features=[\n",
    "    'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'\n",
    "])\n",
    "\n",
    "# Call manually before meta data update\n",
    "poly_features.set_columns(new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the target\n",
    "poly_features = poly_features.add_columns('TARGET', poly_target)\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr().data()['TARGET'].sort_values()\n",
    "\n",
    "# Display most negative and most positive\n",
    "print(poly_corrs.head(10))\n",
    "print(poly_corrs.tail(5))\n",
    "poly_features_test.set_columns(new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge polynomial features into training dataframe\n",
    "poly_features = poly_features.add_columns('SK_ID_CURR', app_train['SK_ID_CURR'])\n",
    "app_train_poly = app_train.merge(poly_features, on='SK_ID_CURR', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge polnomial features into testing dataframe\n",
    "poly_features_test = poly_features_test.add_columns('SK_ID_CURR', app_test['SK_ID_CURR'])\n",
    "app_test_poly = app_test.merge(poly_features_test, on='SK_ID_CURR', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the dataframes\n",
    "train_columns = app_train_poly.data().columns\n",
    "test_columns = app_test_poly.data().columns\n",
    "for c in train_columns:\n",
    "    if c not in test_columns:\n",
    "        app_train_poly = app_train_poly.drop(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the new shapes\n",
    "print('Training data with polynomial features shape: ',\n",
    "      app_train_poly.shape().data())\n",
    "print('Testing data with polynomial features shape:  ',\n",
    "      app_test_poly.shape().data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_domain = app_train.copy()\n",
    "app_test_domain = app_test.copy()\n",
    "\n",
    "app_train_domain = app_train_domain.add_columns(\n",
    "    'CREDIT_INCOME_PERCENT',\n",
    "    app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL'])\n",
    "app_train_domain = app_train_domain.add_columns(\n",
    "    'ANNUITY_INCOME_PERCENT',\n",
    "    app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL'])\n",
    "app_train_domain = app_train_domain.add_columns(\n",
    "    'CREDIT_TERM',\n",
    "    app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT'])\n",
    "app_train_domain = app_train_domain.add_columns(\n",
    "    'DAYS_EMPLOYED_PERCENT',\n",
    "    app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH'])\n",
    "\n",
    "app_test_domain = app_test_domain.add_columns(\n",
    "    'CREDIT_INCOME_PERCENT',\n",
    "    app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL'])\n",
    "app_test_domain = app_test_domain.add_columns(\n",
    "    'ANNUITY_INCOME_PERCENT',\n",
    "    app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL'])\n",
    "app_test_domain = app_test_domain.add_columns(\n",
    "    'CREDIT_TERM',\n",
    "    app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT'])\n",
    "app_test_domain = app_test_domain.add_columns(\n",
    "    'DAYS_EMPLOYED_PERCENT',\n",
    "    app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 20))\n",
    "# # iterate through the new features\n",
    "# for i, column in enumerate([\n",
    "#     'CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM',\n",
    "#     'DAYS_EMPLOYED_PERCENT'\n",
    "# ]):\n",
    "#     # create a new subplot for each source\n",
    "#     plt.subplot(4, 1, i + 1)\n",
    "#     # plot repaid loans\n",
    "#     negative = app_train_domain[[column, 'TARGET']][app_train['TARGET'] == 0]\n",
    "#     sns.kdeplot(\n",
    "#         negative[app_train_domain[column].notna()][column].data(),\n",
    "#         label='target == 0')\n",
    "#     # plot loans that were not repaid\n",
    "#     positive = app_train_domain[[column, 'TARGET']][app_train['TARGET'] == 1]\n",
    "#     sns.kdeplot(\n",
    "#         positive[app_train_domain[column].notna()][column].data(),\n",
    "#         label='target == 1')\n",
    "\n",
    "#     # Label the plots\n",
    "#     plt.title('Distribution of %s by Target Value' % column)\n",
    "#     plt.xlabel('%s' % column)\n",
    "#     plt.ylabel('Density')\n",
    "\n",
    "# plt.tight_layout(h_pad=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "\n",
    "# Drop the target from the training data\n",
    "columns = app_train.data().columns\n",
    "if 'TARGET' in columns:\n",
    "    train = app_train.drop(columns=['TARGET'])\n",
    "else:\n",
    "    train = app_train.copy()\n",
    "\n",
    "\n",
    "# Feature names\n",
    "features = list(train.data().columns)\n",
    "\n",
    "# Copy of the testing data\n",
    "test = app_test.copy()\n",
    "\n",
    "# Median imputation of missing values\n",
    "sk_imputer = Imputer(strategy='median')\n",
    "\n",
    "# Scale each feature to 0-1\n",
    "sk_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "imputer = train.fit_sk_model(sk_imputer)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test)\n",
    "\n",
    "# Repeat with the scaler\n",
    "scaler = train.fit_sk_model(sk_scaler)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "print('Training data shape: ', train.shape().data())\n",
    "print('Testing data shape: ', test.shape().data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Make the model with the specified regularization parameter\n",
    "sk_log_reg = LogisticRegression(C=0.0001)\n",
    "\n",
    "# Train on the training data\n",
    "log_reg = train.fit_sk_model_with_labels(sk_log_reg, train_labels)\n",
    "\n",
    "# Make predictions\n",
    "# Make sure to select the second column only\n",
    "log_reg_pred = log_reg.predict_proba(test)[1]\n",
    "\n",
    "# Submission data\n",
    "log_reg_pred.setname('TARGET')\n",
    "submit = app_test['SK_ID_CURR'].concat(log_reg_pred)\n",
    "submit.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Make the random forest classifier\n",
    "sk_random_forest = RandomForestClassifier(n_estimators=5, random_state=50, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Train on the training data\n",
    "random_forest = train.fit_sk_model_with_labels(sk_random_forest, train_labels)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = random_forest.feature_importances(features)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest.predict_proba(test)[1]\n",
    "\n",
    "# Score = 0.678\n",
    "# Submission dataframe\n",
    "predictions.setname('TARGET')\n",
    "submit = app_test['SK_ID_CURR'].concat(predictions)\n",
    "submit.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly_features_names = list(app_train_poly.data().columns)\n",
    "\n",
    "# # Impute the polynomial features\n",
    "# sk_imputer = Imputer(strategy='median')\n",
    "# imputer = app_train_poly.fit_sk_model(sk_imputer)\n",
    "\n",
    "# poly_features = imputer.transform(app_train_poly)\n",
    "# poly_features_test = imputer.transform(app_test_poly)\n",
    "\n",
    "# # Scale the polynomial features\n",
    "# sk_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler = poly_features.fit_sk_model(sk_scaler)\n",
    "\n",
    "# poly_features = scaler.transform(poly_features)\n",
    "# poly_features_test = scaler.transform(poly_features_test)\n",
    "\n",
    "# sk_random_forest_poly = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "# random_forest_poly = poly_features.fit_sk_model_with_labels(sk_random_forest_poly, train_labels)\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# predictions = random_forest_poly.predict_proba(poly_features_test)[1]\n",
    "\n",
    "# # Score = 0.678\n",
    "# # Submission dataframe\n",
    "# predictions.setname('TARGET')\n",
    "# submit = app_test['SK_ID_CURR'].concat(predictions)\n",
    "# submit.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_train_domain = app_train_domain.drop(columns='TARGET')\n",
    "\n",
    "# domain_features_names = list(app_train_domain.data().columns)\n",
    "\n",
    "# # Impute the domainnomial features\n",
    "# sk_imputer = Imputer(strategy='median')\n",
    "# imputer = app_train_domain.fit_sk_model(sk_imputer)\n",
    "\n",
    "# domain_features = imputer.transform(app_train_domain)\n",
    "# domain_features_test = imputer.transform(app_test_domain)\n",
    "\n",
    "# # Scale the domainnomial features\n",
    "# sk_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler = domain_features.fit_sk_model(sk_scaler)\n",
    "\n",
    "# domain_features = scaler.transform(domain_features)\n",
    "# domain_features_test = scaler.transform(domain_features_test)\n",
    "\n",
    "# # Train on the training data\n",
    "# sk_random_forest_domain = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "# random_forest_domain = domain_features.fit_sk_model_with_labels(sk_random_forest_domain, train_labels)\n",
    "\n",
    "# # Extract feature importances\n",
    "# feature_importances_domain = random_forest_domain.feature_importances(domain_features_names)\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# predictions = random_forest_domain.predict_proba(domain_features_test)[1]\n",
    "\n",
    "# # Score = 0.678\n",
    "# # Make a submission dataframe\n",
    "# predictions.setname('TARGET')\n",
    "# submit = app_test['SK_ID_CURR'].concat(predictions)\n",
    "# submit.head().data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    \"\"\"\n",
    "    Plot importances returned by a model. This can work with any measure of\n",
    "    feature importance provided that higher importance is better.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): feature importances. Must have the features in a column\n",
    "        called `features` and the importances in a column called `importance\n",
    "\n",
    "    Returns:\n",
    "        shows a plot of the 15 most importance features\n",
    "\n",
    "        df (dataframe): feature importances sorted by importance (highest to lowest)\n",
    "        with a column for normalized importance\n",
    "        \"\"\"\n",
    "\n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending=False)\n",
    "\n",
    "    # Normalize the feature importances to add up to one\n",
    "    df = df.add_columns('importance_normalized', df['importance'] / df['importance'].sum().data())\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.data().index[:15]))),\n",
    "            df['importance_normalized'].data().head(15),\n",
    "            align='center', edgecolor='k')\n",
    "\n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.data().index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].data().head(15))\n",
    "\n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance');\n",
    "    plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Show the feature importances for the default features\n",
    "feature_importances_sorted = plot_feature_importances(feature_importances)\n",
    "\n",
    "feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
