{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from openml import datasets, tasks, flows, config\n",
    "\n",
    "from openmlstudy14.preprocessing import ConditionalImputer\n",
    "from workloadoptimization.essentials import Component\n",
    "from workloadoptimization.essentials import ExperimentGraph, ExperimentParser\n",
    "from workloadoptimization.hyperopt_helper import TrialConverter\n",
    "from workloadoptimization.openml_helper import OpenMLReader\n",
    "from workloadoptimization.search_space import SearchSpaceDesigner\n",
    "% matplotlib inline\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=2)\n",
    "\n",
    "config.apikey = '8e2079508005379adb9a60798696134e'\n",
    "config.server = 'https://www.openml.org/api/v1'\n",
    "config.set_cache_directory(os.path.expanduser('~/openml-cache'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants \n",
    "LOGISTIC_REGRESSION_FLOW = 5981\n",
    "RANDOM_FOREST_1_FLOW = 8568\n",
    "RANDOM_FOREST_2_FLOW = 8315\n",
    "SVC_1_FLOW = 8353\n",
    "SVC_2_FLOW = 7707\n",
    "CREDIT_G_DATASET = 31\n",
    "FLOW_IDS = [LOGISTIC_REGRESSION_FLOW, RANDOM_FOREST_1_FLOW, RANDOM_FOREST_2_FLOW, SVC_1_FLOW, SVC_2_FLOW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "task = tasks.get_task(task_id=CREDIT_G_DATASET)\n",
    "dataset = datasets.get_dataset(dataset_id=task.dataset_id)\n",
    "data = dataset.get_data()\n",
    "train_indices, test_indices = task.get_train_test_split_indices()\n",
    "X, y, attribute_names = dataset.get_data(\n",
    "    target=dataset.default_target_attribute,\n",
    "    return_attribute_names=True)\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_experiments(file_path, task_ids, flow_ids):\n",
    "    openMLReader = OpenMLReader()\n",
    "    if os.path.isfile(file_path):\n",
    "        return pd.read_pickle(file_path)\n",
    "    frames = []\n",
    "    for t in task_ids:\n",
    "        for f in flow_ids:\n",
    "            frames.append(openMLReader.getTopRuns(-1, f, t))\n",
    "    experiments = pd.concat(frames).reset_index(drop=True)\n",
    "    experiments.to_pickle(file_path)\n",
    "    return experiments\n",
    "\n",
    "\n",
    "# This is time consuming, so it is better to persist the list of the runs to disk\n",
    "# If you are changing the tasks or flow ids, remember to change the name of the file \n",
    "Experiments = extract_experiments('meta/database-experiment-reuse-openml', [CREDIT_G_DATASET], FLOW_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline_lr = Pipeline([('dualimputer', ConditionalImputer()),\n",
    "                        ('standardscaler', StandardScaler()),\n",
    "                        ('logisticregression', LogisticRegression())])\n",
    "\n",
    "\n",
    "flow = flows.get_flow(flow_id=7707)\n",
    "flow.dependencies = u'sklearn==0.19.1\\nnumpy>=1.6.1\\nscipy>=0.9'\n",
    "for v in flow.components.itervalues():\n",
    "    v.dependencies = u'sklearn==0.19.1\\nnumpy>=1.6.1\\nscipy>=0.9'\n",
    "pipeline_svc_2 = flows.flow_to_sklearn(flow)\n",
    "\n",
    "flow = flows.get_flow(flow_id=8568)\n",
    "flow.components['conditionalimputer'].class_name = 'hyperimp.utils.preprocessing.ConditionalImputer2'\n",
    "pipeline_rf_1 = flows.flow_to_sklearn(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The openml api is not backward compatible for this specific pipeline (5981), so we are recreating it in scikit learn\n",
    "OPENML_FLOWS = {LOGISTIC_REGRESSION_FLOW: pipeline_lr}\n",
    "parser = ExperimentParser()\n",
    "experimentObjects = parser.extract_sklearn_pipelines(Experiments[Experiments.flow_id == LOGISTIC_REGRESSION_FLOW]\n",
    "                                                     , OPENML_FLOWS)\n",
    "other_pipelines = {RANDOM_FOREST_1_FLOW: flows.get_flow(RANDOM_FOREST_1_FLOW), \n",
    "                   RANDOM_FOREST_2_FLOW: flows.get_flow(RANDOM_FOREST_2_FLOW),\n",
    "                   SVC_1_FLOW: flows.get_flow(SVC_1_FLOW),\n",
    "                   SVC_2_FLOW: flows.get_flow(SVC_2_FLOW)}\n",
    "experimentObjects = experimentObjects + parser.extract_openml_flows(\n",
    "    Experiments[Experiments.flow_id.isin(other_pipelines.keys())], other_pipelines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {LOGISTIC_REGRESSION_FLOW: pipeline_lr,\n",
    "             SVC_2_FLOW: pipeline_svc_2,\n",
    "             RANDOM_FOREST_1_FLOW: pipeline_rf_1,\n",
    "             RANDOM_FOREST_2_FLOW: flows.flow_to_sklearn(flows.get_flow(RANDOM_FOREST_2_FLOW)),\n",
    "             SVC_1_FLOW: flows.flow_to_sklearn(flows.get_flow(SVC_1_FLOW))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateParameters = {}\n",
    "copiedObject = list(experimentObjects)\n",
    "for ex in copiedObject:\n",
    "    params = []\n",
    "    if ex.flow not in candidateParameters.keys():\n",
    "        for com in ex.components:\n",
    "            params.append(com.params)\n",
    "        candidateParameters[ex.flow] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8568\n7707\n8315\n5981\n8353\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import ast\n",
    "import time\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def load_existing_profile(path):\n",
    "    if os.path.isfile(path):\n",
    "        read = open(path, 'r')\n",
    "        content = read.read()\n",
    "        read.close()\n",
    "        return ast.literal_eval(content)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "training = X[train_indices]\n",
    "label = y[train_indices]\n",
    "\n",
    "run_profiles = load_existing_profile('meta/reuse-profiles.txt')\n",
    "\n",
    "pipelinesCopy = copy.deepcopy(pipelines)\n",
    "\n",
    "for k, v in pipelinesCopy.iteritems():\n",
    "    for e in experimentObjects:\n",
    "        if e.flow == k:\n",
    "            v.set_params(**e.extractParams())\n",
    "            break\n",
    "\n",
    "for pipelineKey, p in pipelinesCopy.iteritems():\n",
    "    transformed = training\n",
    "    if pipelineKey not in run_profiles.keys():\n",
    "        comp = []\n",
    "        i = 0\n",
    "        for k, c in p.steps:\n",
    "            is_transformation = getattr(c, \"fit_transform\", 'no')\n",
    "            start = time.time()\n",
    "            if is_transformation == 'no':\n",
    "                c.fit(transformed, label)\n",
    "            else:\n",
    "                isSparse = getattr(transformed, \"todense\", False)\n",
    "                if isSparse:\n",
    "                    transformed = transformed.todense()\n",
    "                transformed = c.fit_transform(transformed, label)\n",
    "            end = time.time()\n",
    "            comp.append(end - start)\n",
    "            i = i + 1\n",
    "        run_profiles[pipelineKey] = comp\n",
    "\n",
    "# Write the profiles back to file\n",
    "target = open('meta/reuse-profiles.txt', 'w')\n",
    "target.write(str(run_profiles))\n",
    "target.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in experimentObjects:\n",
    "    runProfile = run_profiles[ex.flow]\n",
    "    total = 0\n",
    "    for i in range(len(runProfile)):\n",
    "        total = total + runProfile[i]\n",
    "        ex.components[i].estimatedRunTime = runProfile[i]\n",
    "    ex.estimatedRunTime = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentGraph = ExperimentGraph()\n",
    "for ex in experimentObjects:\n",
    "    experimentGraph.insertExperimentObject(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Graph\n",
    "def draw_graph(graph):\n",
    "    def pretty_label(edgeLabel):\n",
    "        if edgeLabel['weight'] <= 1:\n",
    "            return ''\n",
    "        else:\n",
    "            return '{} , ({})'.format(edgeLabel['name'], edgeLabel['weight'])\n",
    "    edge_labels = dict([((u, v,), pretty_label(d)) for u, v, d in graph.edges(data=True)])\n",
    "    labels = dict({k for k in graph.nodes(data='label')})\n",
    "    pos = graphviz_layout(graph, prog='dot')\n",
    "    gp = plt.figure(figsize=(20, 20))\n",
    "    nx.draw_networkx_edge_labels(graph, pos, alpha=1, edge_labels=edge_labels, font_size=12)\n",
    "    nx.draw_networkx_nodes(graph, pos, node_size=200)\n",
    "    nx.draw_networkx_edges(graph, pos, alpha=1, arrows=True)\n",
    "    nx.draw_networkx_labels(graph, pos, labels=labels, font_size=16)\n",
    "    gp.savefig(\"graph.png\")\n",
    "\n",
    "\n",
    "draw_graph(experimentGraph.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
