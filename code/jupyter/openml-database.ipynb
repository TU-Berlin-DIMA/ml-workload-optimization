{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openml import datasets, tasks, runs, flows, setups, config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "config.apikey = '8e2079508005379adb9a60798696134e'\n",
    "config.server = 'https://www.openml.org/api/v1'\n",
    "config.set_cache_directory(os.path.expanduser('~/openml-cache'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select and filter flows made by scikit learn\n",
    "flowsJson = flows.list_flows()\n",
    "flowList = pd.DataFrame.from_dict(flowsJson, orient='index')[['id','name']]\n",
    "pipelines = flowList.loc[flowList.name.str.startswith('sklearn.pipeline')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse pipelines and find valid pipelines \n",
    "pipelines = pipelines.assign(parsed = pipelines.name.map(lambda x: x[x.find('(') + 1:x.find(')') + 1]).map(lambda s : s.split(',')))\n",
    "pipelines = pipelines.assign(p_length = pipelines.parsed.map(lambda p: len(p)))\n",
    "pipelines = pipelines.query('p_length > 1')\n",
    "#pipelines.drop(['name'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove step names and extra punctuations\n",
    "pipelines.parsed = pipelines.parsed.map(lambda pi : map(lambda p: p[p.find('=') + 1 :], pi))\n",
    "pipelines.parsed = pipelines.parsed.map(lambda pi : map(lambda p: p.strip('()'), pi))\n",
    "flow_ids = pipelines.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendRun(df, nextBatch):\n",
    "    nb = pd.DataFrame.from_dict(nextBatch, orient='index')\n",
    "    return (pd.concat([df,nb]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_RUNS_LOC = '../../data/training-runs.csv'\n",
    "def readOrDownloadRuns():\n",
    "    if (os.path.exists(TRAINING_RUNS_LOC)):\n",
    "        trainingRuns = pd.read_csv(TRAINING_RUNS_LOC)\n",
    "        return (trainingRuns)\n",
    "    else:\n",
    "        size = 10000\n",
    "        offset = 0\n",
    "        rl = runs.list_runs(flow=flow_ids, size = size, offset = offset)\n",
    "        experiments = pd.DataFrame.from_dict(rl, orient='index')\n",
    "        try:\n",
    "            while(0 < 1):\n",
    "                offset = offset + size\n",
    "                rl = runs.list_runs(flow=flow_ids, size = size, offset = offset)\n",
    "                experiments = appendRun(experiments, rl)\n",
    "        except Exception:\n",
    "            print('finished reading')\n",
    "        trainingRuns = experiments.groupby(['setup_id', 'flow_id','task_id']).size().reset_index(name='counts')\n",
    "        trainingRuns.to_csv(TRAINING_RUNS_LOC, index=False)\n",
    "        return (trainingRuns)\n",
    "\n",
    "trainingRuns = readOrDownloadRuns()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5432    5432\n",
       "5505    5505\n",
       "5507    5507\n",
       "5520    5520\n",
       "5591    5591\n",
       "5648    5648\n",
       "5653    5653\n",
       "5659    5659\n",
       "5662    5662\n",
       "5663    5663\n",
       "5668    5668\n",
       "5703    5703\n",
       "5730    5730\n",
       "5732    5732\n",
       "5733    5733\n",
       "5736    5736\n",
       "5738    5738\n",
       "5739    5739\n",
       "5740    5740\n",
       "5741    5741\n",
       "5742    5742\n",
       "5743    5743\n",
       "5744    5744\n",
       "5745    5745\n",
       "5747    5747\n",
       "5749    5749\n",
       "5752    5752\n",
       "5753    5753\n",
       "5755    5755\n",
       "5757    5757\n",
       "        ... \n",
       "7659    7659\n",
       "7665    7665\n",
       "7668    7668\n",
       "7671    7671\n",
       "7682    7682\n",
       "7688    7688\n",
       "7694    7694\n",
       "7695    7695\n",
       "7699    7699\n",
       "7701    7701\n",
       "7703    7703\n",
       "7705    7705\n",
       "7706    7706\n",
       "7707    7707\n",
       "7711    7711\n",
       "7717    7717\n",
       "7720    7720\n",
       "7723    7723\n",
       "7726    7726\n",
       "7728    7728\n",
       "7730    7730\n",
       "7732    7732\n",
       "7734    7734\n",
       "7736    7736\n",
       "7738    7738\n",
       "7740    7740\n",
       "7742    7742\n",
       "7744    7744\n",
       "7746    7746\n",
       "7748    7748\n",
       "Name: id, Length: 293, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingRuns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskIds = trainingRuns.task_id.unique()\n",
    "flowIds = trainingRuns.flow_id.unique()\n",
    "setupIds = trainingRuns.setup_id.unique()\n",
    "pipelines = pipelines.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in task: 3\n",
      "error in task: 20\n",
      "error in task: 21\n",
      "error in task: 26\n",
      "error in task: 45\n",
      "error in task: 49\n",
      "error in task: 206\n",
      "error in task: 233\n",
      "error in task: 250\n",
      "error in task: 251\n",
      "error in task: 256\n",
      "error in task: 24\n",
      "error in task: 41\n",
      "error in task: 3492\n",
      "error in task: 3493\n",
      "error in task: 3494\n",
      "error in task: 3560\n",
      "error in task: 34536\n",
      "error in task: 34537\n",
      "error in task: 34539\n",
      "error in task: 7\n",
      "error in task: 146195\n"
     ]
    }
   ],
   "source": [
    "MAPPING_LOC = '../../data/mapping.csv'\n",
    "def readOrDownloadMapping():\n",
    "    if (os.path.exists(MAPPING_LOC)):\n",
    "        mapping_table = pd.read_csv(MAPPING_LOC)\n",
    "        return (mapping_table)\n",
    "    else:\n",
    "        taskList = []\n",
    "        for t in taskIds:\n",
    "            #print 'retreiving task: ' + str(t)\n",
    "            try:\n",
    "                task = tasks.get_tasks(task_ids=[t])\n",
    "                taskList.append(task)\n",
    "            except Exception:\n",
    "                print 'error in task: ' + str (t)\n",
    "        datasetIds = []\n",
    "        mapping = dict()\n",
    "        for t in taskList:\n",
    "            mapping[t[0].task_id] = (t[0].dataset_id) \n",
    "\n",
    "        # mapping table to change the task id in Pipeline table to dataset\n",
    "        mapping_table = pd.DataFrame.from_dict(mapping,orient='index')\n",
    "        mapping_table['task'] = mapping_table.index\n",
    "        mapping_table = mapping_table.rename(columns = {0:'dataset'})\n",
    "        mapping_table.to_csv(MAPPING_LOC, index=False)\n",
    "    return (mapping_table)\n",
    "    \n",
    "Mapping = readOrDownloadMapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset  task\n",
       "1        1     1\n",
       "2        2     2\n",
       "4        4     4\n",
       "5        5     5\n",
       "6        6     6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Table\n",
    "The Transformation table include the list of all the available transformations.\n",
    "The current version only support transformation available in scikit-learn\n",
    "Currently, the table has the following format:\n",
    "\n",
    "|id            | full_name     |\n",
    "|:------------ |:--------------|\n",
    "|id of the Transformation| Full name of the Transformation|\n",
    "\n",
    "TODO:\n",
    "- For each transformation, extract the required parameters as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all different transformations from the trainingRuns table\n",
    "# investiage training runs and the pipeline table do not have matching flow (pipeline) ids\n",
    "# even though the training run is queried from pipeline id column\n",
    "currentPipelines = pipelines[pipelines.id.isin(flowIds)].reset_index(drop=True)\n",
    "allTrasformations = []\n",
    "currentPipelines.parsed.map(lambda ts : [allTrasformations.append(a) for a in ts])\n",
    "transformations = pd.DataFrame(allTrasformations, columns=['full_name'])\n",
    "Transformation = pd.DataFrame({'id': transformations.full_name.unique(), 'full_name': transformations.full_name.unique()}, columns = ['id','full_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sklearn.preprocessing.imputation.Imputer</td>\n",
       "      <td>sklearn.preprocessing.imputation.Imputer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sklearn.preprocessing.data.OneHotEncoder</td>\n",
       "      <td>sklearn.preprocessing.data.OneHotEncoder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sklearn.ensemble.forest.RandomForestClassifier</td>\n",
       "      <td>sklearn.ensemble.forest.RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>openml.utils.preprocessing.ConditionalImputer</td>\n",
       "      <td>openml.utils.preprocessing.ConditionalImputer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sklearn.feature_selection.variance_threshold.V...</td>\n",
       "      <td>sklearn.feature_selection.variance_threshold.V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0           sklearn.preprocessing.imputation.Imputer   \n",
       "1           sklearn.preprocessing.data.OneHotEncoder   \n",
       "2     sklearn.ensemble.forest.RandomForestClassifier   \n",
       "3      openml.utils.preprocessing.ConditionalImputer   \n",
       "4  sklearn.feature_selection.variance_threshold.V...   \n",
       "\n",
       "                                           full_name  \n",
       "0           sklearn.preprocessing.imputation.Imputer  \n",
       "1           sklearn.preprocessing.data.OneHotEncoder  \n",
       "2     sklearn.ensemble.forest.RandomForestClassifier  \n",
       "3      openml.utils.preprocessing.ConditionalImputer  \n",
       "4  sklearn.feature_selection.variance_threshold.V...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transformation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Table\n",
    "The Pipeline table contains information about the pipeline, consists of the model, the transformations, the dataset the pipeline is trained on, and parameters. \n",
    "The current version only support transformation available in scikit-learn Currently, the table has the following format:\n",
    "\n",
    "|id            | full_name     | model | transformations | hyperparameters | dataset |\n",
    "|:------------ |:--------------|:------|:----------------|:----------------|---------|\n",
    "|id | Full name| List of Transformations| learned model|hyper parameters of the transformations and the model| dataset|\n",
    "\n",
    "TODO:\n",
    "- Extract model\n",
    "- Extract hyperparameters\n",
    "- There are multiple setup ids for some combinatino of <flow,task>. Make sure that only the parameter values are different and not the actual parameter names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the pipeline info from the run and flow tables\n",
    "currentPipelines = pipelines[pipelines.id.isin(flowIds)].reset_index(drop=True)\n",
    "# Check the setup ids later. for now just ignore setup_id \n",
    "before = currentPipelines.merge(trainingRuns, left_on='id', right_on='flow_id')[['id','name','task_id','parsed']]\n",
    "dedulicated = before.drop_duplicates(subset=['id','task_id'])\n",
    "pipeline_table = dedulicated.rename(columns = {\"name\":\"full_name\", \"parsed\":\"transformations\", \"task_id\":\"task\"})[['id','full_name','transformations','task']]\n",
    "Pipeline = pipeline_table.merge(Mapping, on='task', how='inner')[['id','full_name','transformations','dataset']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>transformations</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5591</td>\n",
       "      <td>sklearn.pipeline.Pipeline(Imputer=sklearn.prep...</td>\n",
       "      <td>[sklearn.preprocessing.imputation.Imputer, skl...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5804</td>\n",
       "      <td>sklearn.pipeline.Pipeline(pca=sklearn.decompos...</td>\n",
       "      <td>[sklearn.decomposition.pca.PCA, sklearn.ensemb...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5873</td>\n",
       "      <td>sklearn.pipeline.Pipeline(Imputer=openml.utils...</td>\n",
       "      <td>[openml.utils.preprocessing.ConditionalImputer...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5983</td>\n",
       "      <td>sklearn.pipeline.Pipeline(dualimputer=helper.d...</td>\n",
       "      <td>[helper.dual_imputer.DualImputer, sklearn.prep...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6038</td>\n",
       "      <td>sklearn.pipeline.Pipeline(dualimputer=helper.d...</td>\n",
       "      <td>[helper.dual_imputer.DualImputer, sklearn.neig...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                          full_name  \\\n",
       "0  5591  sklearn.pipeline.Pipeline(Imputer=sklearn.prep...   \n",
       "1  5804  sklearn.pipeline.Pipeline(pca=sklearn.decompos...   \n",
       "2  5873  sklearn.pipeline.Pipeline(Imputer=openml.utils...   \n",
       "3  5983  sklearn.pipeline.Pipeline(dualimputer=helper.d...   \n",
       "4  6038  sklearn.pipeline.Pipeline(dualimputer=helper.d...   \n",
       "\n",
       "                                     transformations  dataset  \n",
       "0  [sklearn.preprocessing.imputation.Imputer, skl...       61  \n",
       "1  [sklearn.decomposition.pca.PCA, sklearn.ensemb...       61  \n",
       "2  [openml.utils.preprocessing.ConditionalImputer...       61  \n",
       "3  [helper.dual_imputer.DualImputer, sklearn.prep...       61  \n",
       "4  [helper.dual_imputer.DualImputer, sklearn.neig...       61  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipeline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Table\n",
    "The parameter table contains a list of all the parameters, (their type information), and their default values.\n",
    "The current version only support transformation available in scikit-learn Currently, the table has the following format:\n",
    "\n",
    "|id            | name     | full_name | default_value | type |\n",
    "|:------------ |:--------------|:------|:----------------|:----------------|\n",
    "|id |  name| Full Name| Default value|Type information|\n",
    "\n",
    "TODO:\n",
    "- Ensure that all that every setup for a specific flow has the exact same set of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETER_LOC = '../../data/parameter.csv'\n",
    "def readOrDownloadParams():\n",
    "    if (os.path.exists(PARAMETER_LOC)):\n",
    "        parameter_table = pd.read_csv(PARAMETER_LOC)\n",
    "        return (parameter_table)\n",
    "    else:\n",
    "        all_setups = Pipeline.merge(trainingRuns, left_on='id', right_on='flow_id')[['id','setup_id']]\n",
    "        # have to check these to make sure that every setup for a flow contains exactly the same set of parameters\n",
    "        unique_parameters = all_setups.drop_duplicates(subset=['id'])\n",
    "        all_setups = setups.list_setups(setup=unique_parameters.setup_id)\n",
    "        parameter_table = pd.DataFrame(columns=['id','name','full_name','default_value','type'])\n",
    "        for sk, sv in all_setups.iteritems():\n",
    "            for pk, pv in sv.parameters.iteritems():\n",
    "                parameter_table.loc[parameter_table.shape[0]] = [pv.id,pv.parameter_name,pv.full_name,pv.default_value,pv.data_type]\n",
    "        parameter_table.to_csv(PARAMETER_LOC, index=False)\n",
    "        return (parameter_table)\n",
    "Parameter = readOrDownloadParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>default_value</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53952</td>\n",
       "      <td>iterated_power</td>\n",
       "      <td>sklearn.decomposition.pca.PCA(1)_iterated_power</td>\n",
       "      <td>\"auto\"</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53953</td>\n",
       "      <td>n_components</td>\n",
       "      <td>sklearn.decomposition.pca.PCA(1)_n_components</td>\n",
       "      <td>null</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53954</td>\n",
       "      <td>random_state</td>\n",
       "      <td>sklearn.decomposition.pca.PCA(1)_random_state</td>\n",
       "      <td>null</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53955</td>\n",
       "      <td>svd_solver</td>\n",
       "      <td>sklearn.decomposition.pca.PCA(1)_svd_solver</td>\n",
       "      <td>\"auto\"</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53956</td>\n",
       "      <td>tol</td>\n",
       "      <td>sklearn.decomposition.pca.PCA(1)_tol</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            name                                        full_name  \\\n",
       "0  53952  iterated_power  sklearn.decomposition.pca.PCA(1)_iterated_power   \n",
       "1  53953    n_components    sklearn.decomposition.pca.PCA(1)_n_components   \n",
       "2  53954    random_state    sklearn.decomposition.pca.PCA(1)_random_state   \n",
       "3  53955      svd_solver      sklearn.decomposition.pca.PCA(1)_svd_solver   \n",
       "4  53956             tol             sklearn.decomposition.pca.PCA(1)_tol   \n",
       "\n",
       "  default_value  type  \n",
       "0        \"auto\"  None  \n",
       "1          null  None  \n",
       "2          null  None  \n",
       "3        \"auto\"  None  \n",
       "4           0.0  None  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parameter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Table\n",
    "The Dataset table contains metadata about the existing datasets in the training runs.\n",
    "First from the pipeline table, the tasks are extracted, then the correspoding dataset is retreived from OpenML.\n",
    "The table has the following schema:\n",
    "\n",
    "|id            | name     | NumberOfClasses | NumberOfFeatures | NumberOfInstances |\n",
    "|:------------ |:--------------|:------|:----------------|:----------------|\n",
    "|id |  name| Number of classes| Number of Features |Number of training instances|\n",
    "\n",
    "TODO:\n",
    "- OpenML contains more information about the Datasets, check if any other information are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LOC = '../../data/dataset.csv'\n",
    "def readOrDownloadDatasets():\n",
    "    if (os.path.exists(DATASET_LOC)):\n",
    "        dataset_table = pd.read_csv(DATASET_LOC)\n",
    "        return (dataset_table)\n",
    "    else:\n",
    "        ds = datasets.list_datasets()\n",
    "        datasetTable = pd.DataFrame.from_dict(ds, orient='index')\n",
    "        existingDatasets = datasetTable[datasetTable.did.isin(Pipeline.dataset.unique())]\n",
    "        dataset_table = existingDatasets.rename(columns = {\"did\":\"id\"})[['id','name', 'NumberOfClasses','NumberOfFeatures','NumberOfInstances']]\n",
    "        dataset_table.to_csv(DATASET_LOC, index=False)\n",
    "        return (dataset_table)\n",
    "Dataset = readOrDownloadDatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>NumberOfClasses</th>\n",
       "      <th>NumberOfFeatures</th>\n",
       "      <th>NumberOfInstances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>anneal</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>labor</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>arrhythmia</td>\n",
       "      <td>13</td>\n",
       "      <td>280</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>letter</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>lymph</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id        name  NumberOfClasses  NumberOfFeatures  NumberOfInstances\n",
       "2    2      anneal                5                39                898\n",
       "4    4       labor                2                17                 57\n",
       "5    5  arrhythmia               13               280                452\n",
       "6    6      letter               26                17              20000\n",
       "10  10       lymph                4                19                148"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Run Table\n",
    "The Training run table contains information about all the single execution of pipelines on different datasets, and the parameters setup used for those. Currently, we do not have information about the run time of each execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
