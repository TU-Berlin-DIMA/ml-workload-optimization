{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Pipeline hyperparameter optimization\n",
    "Before adding hyperparameter search option to the Experiment Graph, we perform a simple analysis where we select the one pipeline and for every execution we add the hyperparameters and the final accuracy to the Trial object of the hypernet. Afterwards, we perform a search using a predefined budget to find the best set of parameters.\n",
    "We compare the result, with the vanilla version, where the Trial object is empty and report the quality and time to achieve the certain level of quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openml import datasets, tasks, runs, flows, setups, config, evaluations\n",
    "from workloadoptimization.essentials import Component, ExperimentObject, ExperimentGraph, ExperimentParser\n",
    "from workloadoptimization.hyperopt_helper import TrialConverter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import uuid\n",
    "import networkx as nx\n",
    "import sklearn\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from openmlstudy14.preprocessing import ConditionalImputer\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "config.apikey = '8e2079508005379adb9a60798696134e'\n",
    "config.server = 'https://www.openml.org/api/v1'\n",
    "config.set_cache_directory(os.path.expanduser('~/openml-cache'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = flows.get_flow(flow_id=7707)\n",
    "task = tasks.get_task(task_id=31)\n",
    "pipeline = flows.flow_to_sklearn(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def getTopRuns(numberOfRuns, pipeline, task):\n",
    "    openMlEvaluations = evaluations.list_evaluations('predictive_accuracy', task= [task], flow = [pipeline])\n",
    "    evaluationData = pd.DataFrame.from_dict(openMlEvaluations, orient='index')\n",
    "    evaluationData['accuracy'] = evaluationData.apply(lambda eva: eva.values[0].value, axis = 1)\n",
    "    evaluationData['run_id'] = evaluationData.apply(lambda eva: eva.values[0].run_id, axis = 1)\n",
    "    # extracting the top ''numberOfRuns' runs\n",
    "    topRuns = evaluationData.sort_values('accuracy',ascending=False)\n",
    "    if (numberOfRuns>0):\n",
    "        topRuns = topRuns[0:numberOfRuns]\n",
    "    # retreiving the run objects from the top runs\n",
    "    openMLRuns = runs.list_runs(task=[task], flow=[pipeline])\n",
    "    experiments = pd.DataFrame.from_dict(openMLRuns,orient='index')\n",
    "    Experiment = experiments.merge(topRuns,on='run_id').drop(columns=['uploader',0])\n",
    "    # TODO: This is the limit on the api size, I should fix it to make calls in batches\n",
    "    Setup = pd.DataFrame.from_dict(setups.list_setups(setup=Experiment.setup_id[0:500], size = 500 ), orient='index').reset_index()\n",
    "    Setup.columns=['id', 'setup']\n",
    "\n",
    "    return pd.merge(Setup, Experiment, how = 'inner', left_on='id', right_on='setup_id').drop(columns = ['id','setup_id'])[['run_id','task_id','flow_id', 'accuracy','setup']]\n",
    "def extractExperiments(filePath, taskIds, flowIds):\n",
    "    if os.path.isfile(filePath):\n",
    "        return pd.read_pickle(filePath)\n",
    "    frames = []\n",
    "    for t in taskIds:\n",
    "        for f in flowIds:\n",
    "            frames.append(getTopRuns(100000,f,t))\n",
    "    Experiments = pd.concat(frames).reset_index(drop=True)\n",
    "    Experiments.to_pickle(filePath)\n",
    "    return Experiments\n",
    "\n",
    "# This is time consuming, so it is better to persist the list of the runs to disk\n",
    "# If you are changing the tasks or flow ids, remember to change the name of the file \n",
    "Experiments = extractExperiments('meta/hyper-opt-experiment-31-7707', [31], [7707])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENML_FLOWS = {}\n",
    "FLOW_IDS = Experiments.flow_id.unique()\n",
    "for f in FLOW_IDS:\n",
    "    try:\n",
    "        #print f\n",
    "        fl = flows.get_flow(f)\n",
    "        OPENML_FLOWS[f] = fl \n",
    "    except:\n",
    "        print 'error for {}'.format(f)\n",
    "parser = ExperimentParser()\n",
    "experimentObjects = parser.extractOpenMLFlows(Experiments, OPENML_FLOWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the range of all the hyperparameters for setting up the search space\n",
    "param_range = {}\n",
    "for e in experimentObjects:\n",
    "    for k,v in e.extractParams().iteritems():\n",
    "        if param_range.has_key(k):\n",
    "            param_range[k].append(v)\n",
    "        else:\n",
    "            param_range[k] = [v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic hyperparameters: ['classifier__tol', 'classifier__gamma', 'classifier__C', 'imputation__strategy', 'classifier__degree', 'classifier__coef0', 'classifier__shrinking', 'classifier__kernel']\n"
     ]
    }
   ],
   "source": [
    "# Find the hyperparameters with constant values so we can set them once and do not include them in the search space\n",
    "CONSTANT_PARAMS = {}\n",
    "DYNAMIC_PARAMS = []\n",
    "for k in param_range.keys():\n",
    "    try:\n",
    "        size = len(set(param_range[k]))\n",
    "        if size == 1:\n",
    "            CONSTANT_PARAMS[k] = param_range[k][0]\n",
    "        else:\n",
    "            DYNAMIC_PARAMS.append(k)\n",
    "    except: \n",
    "        CONSTANT_PARAMS[k] = param_range[k][0]    \n",
    "        #print k,'error'\n",
    "print 'Dynamic hyperparameters: {}'.format(DYNAMIC_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the list of the parameters in the experiments these are the feasable ranges (or categories) of the parameters.\n",
    "- classifier__tol 1.0509652110524482e-05 0.09706102908291375\n",
    "- classifier__gamma 3.122280314190532e-05 7.998532268538166\n",
    "- classifier__C 0.03213680700039348 32547.418063576853\n",
    "- imputation__strategy {u'\"mean\"', u'\"median\"', u'\"most_frequent\"'}\n",
    "- classifier__degree {u'1', u'2', u'3', u'4', u'5'}\n",
    "- classifier__coef0 -0.9942534412466477 0.9975887639931769\n",
    "- classifier__shrinking True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the search space\n",
    "from hyperopt import hp\n",
    "\n",
    "# Parameter search space\n",
    "space = {}\n",
    "# between 1.0509652110524482e-05 0.09706102908291375\n",
    "space['classifier__tol'] = hp.lognormal('classifier__tol',-7, 1)\n",
    "# between 3.122280314190532e-05 7.998532268538166\n",
    "space['classifier__gamma'] = hp.lognormal('classifier__gamma',0.0001, 1.3)\n",
    "# One of True or False\n",
    "space['classifier__C'] = hp.lognormal('classifier__C',2.5, 3)\n",
    "# choice\n",
    "space['imputation__strategy'] = hp.choice('imputation__strategy', [u'mean', u'median', u'most_frequent'])\n",
    "# choice\n",
    "space['classifier__degree'] = hp.choice('classifier__degree', [1, 2, 3, 4, 5])\n",
    "# Between -0.9942534412466477 0.9975887639931769\n",
    "space['classifier__coef0'] = hp.uniform('classifier__coef0', -1, 1)\n",
    "# True or False\n",
    "space['classifier__shrinking'] = hp.choice('classifier__shrinking',[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the constant parameters and define the objective function\n",
    "pipeline.set_params(**CONSTANT_PARAMS)\n",
    "def objective(params):\n",
    "    #print params\n",
    "    pipeline.set_params(**params)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    accuracy = pipeline.score(X_test, y_test)\n",
    "    #print 'accuracy = {}'.format(accuracy)\n",
    "    return 1 - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = datasets.get_dataset(dataset_id=task.dataset_id)\n",
    "data = dataset.get_data()\n",
    "train_indices,test_indices = task.get_train_test_split_indices()\n",
    "X, y, attribute_names = dataset.get_data(\n",
    "target=dataset.default_target_attribute,\n",
    "return_attribute_names=True)\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort based on the loss\n",
    "sortedObjects = sorted(experimentObjects, key=lambda eo: 1 - eo.quality, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "trialConverter = TrialConverter()\n",
    "warmTrials = trialConverter.trialsFromExperimentObjects(space, sortedObjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the search process with trials database already set\n",
    "from hyperopt import fmin, tpe, Trials, rand\n",
    "\n",
    "# Run the hyperparameter search using the tpe algorithm\n",
    "best = fmin(objective,\n",
    "            space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=1500,\n",
    "            trials=warmTrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the search process with trials database already set\n",
    "from hyperopt import fmin, tpe, Trials, rand\n",
    "\n",
    "coldTrials = Trials()\n",
    "# Run the hyperparameter search using the tpe algorithm\n",
    "best = fmin(objective,\n",
    "            space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=1000,\n",
    "            trials=coldTrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawPlots(targetTrial):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(3,3,1)\n",
    "    ax = sns.regplot(x = np.array([x['misc']['vals']['classifier__tol'][0] for x in targetTrial.trials]), y = np.array(targetTrial.losses()), fit_reg=False)\n",
    "    ax.set_title('classifier__tol')\n",
    "    plt.subplot(3,3,2)\n",
    "    ax = sns.regplot(x = np.array([x['misc']['vals']['classifier__gamma'][0] for x in targetTrial.trials]), y = np.array(targetTrial.losses()), fit_reg=False)\n",
    "    ax.set_title('classifier__gamma')\n",
    "    plt.subplot(3,3,3)\n",
    "    ax = sns.regplot(x = np.array([x['misc']['vals']['classifier__C'][0] for x in targetTrial.trials]), y = np.array(targetTrial.losses()), fit_reg=False)\n",
    "    ax.set_title('classifier__C')\n",
    "    plt.subplot(3,3,4)\n",
    "    ax = sns.regplot(x = np.array([x['misc']['vals']['classifier__degree'][0] for x in targetTrial.trials]), y = np.array(targetTrial.losses()), fit_reg=False)\n",
    "    ax.set_title('classifier__degree')\n",
    "    plt.subplot(3,3,5)\n",
    "    ax = sns.regplot(x = np.array([x['misc']['vals']['classifier__coef0'][0] for x in targetTrial.trials]), y = np.array(targetTrial.losses()), fit_reg=False)\n",
    "    ax.set_title('classifier__coef0')\n",
    "    plt.subplot(3,1,3)\n",
    "    ax = sns.regplot(np.array(range(len(targetTrial.losses()))), np.array(targetTrial.losses()), fit_reg=False)\n",
    "    ax.set_title('Trials vs Quality over time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cold vs Warm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawPlots(warmTrials)\n",
    "trials = Trials()\n",
    "trials.insert_trial_docs(warmTrials.trials[500:])\n",
    "trials.refresh()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,1,1)\n",
    "ax = sns.regplot(np.array(range(len(trials.losses()))), np.array(trials.losses()), fit_reg=False)\n",
    "ax.set_title('Warm Trials')\n",
    "plt.subplot(2,1,2)\n",
    "ax = sns.regplot(np.array(range(len(coldTrials.losses()))), np.array(coldTrials.losses()), fit_reg=False)\n",
    "ax.set_title('Cold Trials')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Experiment Database to warmstart the Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawPlots(warmTrials)\n",
    "trials = Trials()\n",
    "trials.insert_trial_docs(warmTrials.trials[500:600])\n",
    "trials.refresh()\n",
    "drawPlots(trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Warmstarting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawPlots(coldTrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book_time': datetime.datetime(2018, 7, 1, 11, 18, 0, 93000),\n",
       " 'exp_key': None,\n",
       " 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'idxs': {'classifier__C': [20],\n",
       "   'classifier__coef0': [20],\n",
       "   'classifier__degree': [20],\n",
       "   'classifier__gamma': [20],\n",
       "   'classifier__shrinking': [20],\n",
       "   'classifier__tol': [20],\n",
       "   'imputation__strategy': [20]},\n",
       "  'tid': 20,\n",
       "  'vals': {'classifier__C': [0.6923023790257264],\n",
       "   'classifier__coef0': [0.4490707362102988],\n",
       "   'classifier__degree': [4],\n",
       "   'classifier__gamma': [0.008142826362265525],\n",
       "   'classifier__shrinking': [0],\n",
       "   'classifier__tol': [0.00024120800848078465],\n",
       "   'imputation__strategy': [2]},\n",
       "  'workdir': None},\n",
       " 'owner': None,\n",
       " 'refresh_time': datetime.datetime(2018, 7, 1, 11, 18, 0, 409000),\n",
       " 'result': {'loss': 0.18999999999999995, 'status': 'ok'},\n",
       " 'spec': None,\n",
       " 'state': 2,\n",
       " 'tid': 20,\n",
       " 'version': 0}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coldTrials.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book_time': datetime.datetime(2018, 7, 1, 11, 28, 41, 638000),\n",
       " 'exp_key': None,\n",
       " 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'idxs': {'classifier__C': [621],\n",
       "   'classifier__coef0': [621],\n",
       "   'classifier__degree': [621],\n",
       "   'classifier__gamma': [621],\n",
       "   'classifier__shrinking': [621],\n",
       "   'classifier__tol': [621],\n",
       "   'imputation__strategy': [621]},\n",
       "  'tid': 621,\n",
       "  'vals': {'classifier__C': [7.75292203371408],\n",
       "   'classifier__coef0': [0.7731936758562243],\n",
       "   'classifier__degree': [4],\n",
       "   'classifier__gamma': [0.0011317768583867197],\n",
       "   'classifier__shrinking': [1],\n",
       "   'classifier__tol': [0.0018070574299404],\n",
       "   'imputation__strategy': [0]},\n",
       "  'workdir': None},\n",
       " 'owner': None,\n",
       " 'refresh_time': datetime.datetime(2018, 7, 1, 11, 28, 41, 913000),\n",
       " 'result': {'loss': 0.19999999999999996, 'status': 'ok'},\n",
       " 'spec': None,\n",
       " 'state': 2,\n",
       " 'tid': 621,\n",
       " 'version': 0}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmTrials.best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This certainly shows some promising results. I have to investiage a bit more because it seems the search is stuck in some local point because the objective value of most of the proposed hyperparamters are constant (0.3) as you see from the figure above.\n",
    "Moreover, the behaviour is a bit random, sometimes s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
