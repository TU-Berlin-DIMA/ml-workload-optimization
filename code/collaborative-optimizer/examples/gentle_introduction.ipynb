{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d69227",
   "metadata": {},
   "source": [
    "# Start Here: A Gentle Introduction\n",
    "This Notebook is based on the following notebook: https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction from a Kaggle Competition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321830b4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Download the application_train from here: https://www.kaggle.com/c/home-credit-default-risk/data and run the following.\n",
    "The reason we need to do this is because the \"Home Credit Default Risk\" competition does not provide labels for their test data and the only way of getting the score is to send the submission file.\n",
    "To avoid that, we split the training data into train/test so we can compute the ROC curve without submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "app_train = pd.read_csv('./data/application_train.csv')\n",
    "train, test = train_test_split(app_train, test_size=0.2)\n",
    "test_labels = test[['SK_ID_CURR', 'TARGET']]\n",
    "test = test.drop(columns='TARGET', inplace=False)\n",
    "\n",
    "if not os.path.exists('./data/final'):\n",
    "    os.mkdir('./data/final')\n",
    "\n",
    "train.to_csv('./data/final/application_train.csv', index=False)\n",
    "test.to_csv('./data/final/application_test.csv', index=False)\n",
    "\n",
    "test_labels.to_csv('./data/final/application_test_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Import\n",
    "from experiment_graph.execution_environment import ExecutionEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_environment = ExecutionEnvironment()\n",
    "root_data = './data/final'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d592a580",
   "metadata": {},
   "source": [
    "## Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71089e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(root_data))\n",
    "app_train = execution_environment.load(root_data + '/application_train.csv')\n",
    "print('Training data shape: ', app_train.shape().data())\n",
    "app_train.head().data()\n",
    "\n",
    "app_test = execution_environment.load(root_data + '/application_test.csv')\n",
    "print('Testing data shape: ', app_test.shape().data())\n",
    "app_test.head().data()\n",
    "\n",
    "test_labels = execution_environment.load(root_data + '/application_test_labels.csv')\n",
    "\n",
    "app_train['TARGET'].value_counts().data()\n",
    "\n",
    "app_train['TARGET'].data().astype(int).plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03058b11",
   "metadata": {},
   "source": [
    "## Examine Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct\n",
    "def missing_values_table(dataset):\n",
    "    # Total missing values\n",
    "    mis_val = dataset.isnull().sum().data()\n",
    "\n",
    "    mis_val_percent = 100 * mis_val / len(dataset.data())\n",
    "\n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(columns={\n",
    "        0: 'Missing Values',\n",
    "        1: '% of Total Values'\n",
    "    })\n",
    "    # Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    # Print some summary information\n",
    "    print(\"Your selected dataframe has \" + str(dataset.shape().data()[1]) + \" columns.\\n\"\n",
    "                                                                                   \"There are \" + str(\n",
    "        mis_val_table_ren_columns.shape[0]) +\n",
    "          \" columns that have missing values.\")\n",
    "\n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "missing_values = missing_values_table(app_train)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15440627",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train.dtypes().data().value_counts()\n",
    "app_train.select_dtypes('object').nunique().data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e2a71",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c645ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_graph.sklearn_helper.preprocessing import LabelEncoder\n",
    "# Create a label encoder object\n",
    "le_count = 0\n",
    "\n",
    "columns = app_train.select_dtypes('object').data().columns\n",
    "for col in columns:\n",
    "    # we are not using nunique because it discard nan\n",
    "    if app_train[col].nunique(dropna=False).data() <= 2:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(app_train[col])\n",
    "\n",
    "        app_train = app_train.replace_columns(col, le.transform(app_train[col]))\n",
    "        app_test = app_test.replace_columns(col, le.transform(app_test[col]))\n",
    "\n",
    "        # Keep track of how many columns were label encoded\n",
    "        le_count += 1\n",
    "print('%d columns were label encoded.' % le_count)\n",
    "app_train.data()\n",
    "app_test.data()\n",
    "\n",
    "app_train = app_train.onehot_encode()\n",
    "app_test = app_test.onehot_encode()\n",
    "\n",
    "print('Training Features shape: ', app_train.shape().data())\n",
    "print('Testing Features shape: ', app_test.shape().data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b5478",
   "metadata": {},
   "source": [
    "## Aligning Training and Testing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = app_train['TARGET']\n",
    "app_train = app_train.align(app_test)\n",
    "app_test = app_test.align(app_train)\n",
    "\n",
    "app_train = app_train.add_columns('TARGET', train_labels)\n",
    "\n",
    "print('Training Features shape: ', app_train.shape().data())\n",
    "print('Testing Features shape: ', app_test.shape().data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b1963",
   "metadata": {},
   "source": [
    "## Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5703f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(app_train['DAYS_BIRTH'] / 365).describe().data()\n",
    "\n",
    "app_train['DAYS_EMPLOYED'].describe().data()\n",
    "\n",
    "app_train['DAYS_EMPLOYED'].data().plot.hist(title='Days Employment Histogram')\n",
    "plt.xlabel('Days Employment')\n",
    "\n",
    "anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n",
    "non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n",
    "print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean().data()))\n",
    "print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean().data()))\n",
    "print('There are %d anomalous days of employment' % anom.shape().data()[0])\n",
    "\n",
    "# Create an anomalous flag column\n",
    "days_employed_anom = app_train[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_train = app_train.add_columns('DAYS_EMPLOYED_ANOM', days_employed_anom)\n",
    "temp = app_train['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "app_train = app_train.drop('DAYS_EMPLOYED')\n",
    "app_train = app_train.add_columns('DAYS_EMPLOYED', temp)\n",
    "\n",
    "app_train[\"DAYS_EMPLOYED\"].data().plot.hist(title='Days Employment Histogram');\n",
    "plt.xlabel('Days Employment')\n",
    "\n",
    "days_employed_anom = app_test[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_test = app_test.add_columns('DAYS_EMPLOYED_ANOM', days_employed_anom)\n",
    "temp = app_test['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "app_test = app_test.drop('DAYS_EMPLOYED')\n",
    "app_test = app_test.add_columns('DAYS_EMPLOYED', temp)\n",
    "print('There are %d anomalies in the test data out of %d entries'\n",
    "      % (app_test['DAYS_EMPLOYED_ANOM'].sum().data(),\n",
    "         app_test.shape().data()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936e5fd",
   "metadata": {},
   "source": [
    "## Correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7dd607",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = app_train.corr().data()\n",
    "top = correlations['TARGET'].sort_values()\n",
    "# Display correlations\n",
    "print('Most Positive Correlations:\\n', top.tail(15))\n",
    "print('\\nMost Negative Correlations:\\n', top.head(15)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2eab6",
   "metadata": {},
   "source": [
    "## Effect of Age on Repayment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5280913",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_age = app_train['DAYS_BIRTH'].abs()\n",
    "app_train = app_train.drop('DAYS_BIRTH')\n",
    "app_train = app_train.add_columns('DAYS_BIRTH', abs_age)\n",
    "app_train['DAYS_BIRTH'].corr(app_train['TARGET']).data()\n",
    "\n",
    "# Set the style of plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot the distribution of ages in years\n",
    "plt.hist((app_train['DAYS_BIRTH'] / 365).data(), edgecolor='k', bins=25)\n",
    "plt.title('Age of Client')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# KDE plot of loans that were repaid on time\n",
    "sns.kdeplot((app_train[app_train['TARGET'] == 0]['DAYS_BIRTH'] / 365).data(), label='target == 0')\n",
    "# KDE plot of loans which were not repaid on time\n",
    "sns.kdeplot((app_train[app_train['TARGET'] == 1]['DAYS_BIRTH'] / 365).data(), label='target == 1')\n",
    "# Labeling of plot\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Ages')\n",
    "\n",
    "# Age information into a separate dataframe\n",
    "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
    "years_birth = age_data['DAYS_BIRTH'] / 365\n",
    "age_data = age_data.add_columns('YEARS_BIRTH', years_birth)\n",
    "binned = age_data['YEARS_BIRTH'].binning(20, 70, 11)\n",
    "binned.setname('YEARS_BINNED')\n",
    "age_data = age_data.add_columns('YEARS_BINNED', binned)\n",
    "age_data.head(10).data()\n",
    "\n",
    "age_groups = age_data.groupby('YEARS_BINNED').mean()\n",
    "age_groups.data()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Graph the age bins and the average of the target as a bar plot\n",
    "plt.bar(age_groups.data().index.astype(str), age_groups.data()['TARGET'] * 100)\n",
    "\n",
    "# Plot labeling\n",
    "plt.xticks(rotation=75)\n",
    "plt.xlabel('Age Group (years)')\n",
    "plt.ylabel('Failure to Repay (%)')\n",
    "plt.title('Failure to Repay by Age Group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b495d",
   "metadata": {},
   "source": [
    "## Exterior Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "ext_data_corrs = ext_data.corr().data()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Heatmap of correlations\n",
    "sns.heatmap(ext_data_corrs, cmap=plt.cm.RdYlBu_r, vmin=-0.25, annot=True, vmax=0.6)\n",
    "plt.title('Correlation Heatmap')\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "\n",
    "# iterate through the sources\n",
    "for i, column in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    source_data = app_train[[column, 'TARGET']][app_train['TARGET'] == 0]\n",
    "    sns.kdeplot(source_data[app_train[column].notna()][column].data(), label='target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    source_data = app_train[[column, 'TARGET']][app_train['TARGET'] == 1]\n",
    "    sns.kdeplot(source_data[app_train[column].notna()][column].data(), label='target == 1')\n",
    "\n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % column)\n",
    "    plt.xlabel('%s' % column)\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout(h_pad=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f282b",
   "metadata": {},
   "source": [
    "## Pairs Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59503c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data for plotting\n",
    "plot_data = ext_data.drop('DAYS_BIRTH')\n",
    "\n",
    "# Add in the age of the client in years\n",
    "plot_data = plot_data.add_columns('YEARS_BIRTH', age_data['YEARS_BIRTH'])\n",
    "# Drop na values and limit to first 100000 rows\n",
    "plot_data = plot_data.head(100000).dropna()\n",
    "\n",
    "# Create the pair grid object\n",
    "grid = sns.PairGrid(data=plot_data.data(), size=3, diag_sharey=False,\n",
    "                    hue='TARGET',\n",
    "                    vars=[x for x in list(plot_data.data().columns) if x != 'TARGET'])\n",
    "\n",
    "# Upper is a scatter plot\n",
    "grid.map_upper(plt.scatter, alpha=0.2)\n",
    "\n",
    "# Diagonal is a histogram\n",
    "grid.map_diag(sns.kdeplot)\n",
    "\n",
    "# Bottom is density plot\n",
    "grid.map_lower(sns.kdeplot, cmap=plt.cm.OrRd_r)\n",
    "\n",
    "plt.suptitle('Ext Source and Age Features Pairs Plot', size=32, y=1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac5379",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d5bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe for polynomial features\n",
    "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
    "poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "\n",
    "# imputer for handling missing values\n",
    "from experiment_graph.sklearn_helper.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy='median')\n",
    "\n",
    "poly_target = poly_features['TARGET']\n",
    "\n",
    "poly_features = poly_features.drop(columns=['TARGET'])\n",
    "\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "\n",
    "from experiment_graph.sklearn_helper.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree=3)\n",
    "\n",
    "# Train the polynomial features\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape: ', poly_features.shape().data())\n",
    "\n",
    "new_names = poly_transformer.get_feature_names(input_features=['EXT_SOURCE_1', 'EXT_SOURCE_2',\n",
    "                                                               'EXT_SOURCE_3', 'DAYS_BIRTH'])\n",
    "\n",
    "poly_features = poly_features.set_columns(new_names)\n",
    "\n",
    "# Add in the target\n",
    "poly_features = poly_features.add_columns('TARGET', poly_target)\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr().data()['TARGET'].sort_values()\n",
    "\n",
    "# Display most negative and most positive\n",
    "print(poly_corrs.head(10))\n",
    "print(poly_corrs.tail(5))\n",
    "\n",
    "poly_features_test = poly_features_test.set_columns(new_names)\n",
    "\n",
    "# Merge polynomial features into training dataframe\n",
    "poly_features = poly_features.add_columns('SK_ID_CURR', app_train['SK_ID_CURR'])\n",
    "app_train_poly = app_train.merge(poly_features, on='SK_ID_CURR', how='left')\n",
    "\n",
    "# Merge polnomial features into testing dataframe\n",
    "poly_features_test = poly_features_test.add_columns('SK_ID_CURR', app_test['SK_ID_CURR'])\n",
    "app_test_poly = app_test.merge(poly_features_test, on='SK_ID_CURR', how='left')\n",
    "\n",
    "# Align the dataframes\n",
    "app_train_poly = app_train_poly.align(app_test_poly)\n",
    "app_test_poly = app_test_poly.align(app_train_poly)\n",
    "\n",
    "# Print out the new shapes\n",
    "print('Training data with polynomial features shape: ',\n",
    "      app_train_poly.shape().data())\n",
    "print('Testing data with polynomial features shape:  ',\n",
    "      app_test_poly.shape().data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3370dd5",
   "metadata": {},
   "source": [
    "## Domain Knowledge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_domain = app_train.copy()\n",
    "app_test_domain = app_test.copy()\n",
    "\n",
    "app_train_domain = app_train_domain.add_columns('CREDIT_INCOME_PERCENT',\n",
    "                                                app_train_domain['AMT_CREDIT'] / app_train_domain[\n",
    "                                                    'AMT_INCOME_TOTAL'])\n",
    "app_train_domain = app_train_domain.add_columns('ANNUITY_INCOME_PERCENT',\n",
    "                                                app_train_domain['AMT_ANNUITY'] / app_train_domain[\n",
    "                                                    'AMT_INCOME_TOTAL'])\n",
    "app_train_domain = app_train_domain.add_columns('CREDIT_TERM',\n",
    "                                                app_train_domain['AMT_ANNUITY'] / app_train_domain[\n",
    "                                                    'AMT_CREDIT'])\n",
    "app_train_domain = app_train_domain.add_columns('DAYS_EMPLOYED_PERCENT',\n",
    "                                                app_train_domain['DAYS_EMPLOYED'] / app_train_domain[\n",
    "                                                    'DAYS_BIRTH'])\n",
    "\n",
    "app_test_domain = app_test_domain.add_columns('CREDIT_INCOME_PERCENT',\n",
    "                                              app_test_domain['AMT_CREDIT'] / app_test_domain[\n",
    "                                                  'AMT_INCOME_TOTAL'])\n",
    "app_test_domain = app_test_domain.add_columns('ANNUITY_INCOME_PERCENT',\n",
    "                                              app_test_domain['AMT_ANNUITY'] / app_test_domain[\n",
    "                                                  'AMT_INCOME_TOTAL'])\n",
    "app_test_domain = app_test_domain.add_columns('CREDIT_TERM',\n",
    "                                              app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT'])\n",
    "app_test_domain = app_test_domain.add_columns('DAYS_EMPLOYED_PERCENT',\n",
    "                                              app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a28e9c",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_graph.sklearn_helper.preprocessing import MinMaxScaler\n",
    "\n",
    "# Drop the target from the training data\n",
    "columns = app_train.data().columns\n",
    "if 'TARGET' in columns:\n",
    "    train = app_train.drop(columns=['TARGET'])\n",
    "else:\n",
    "    train = app_train.copy()\n",
    "\n",
    "# Feature names\n",
    "features = list(train.data().columns)\n",
    "\n",
    "# Copy of the testing data\n",
    "test = app_test.copy()\n",
    "\n",
    "# Median imputation of missing values\n",
    "imputer = Imputer(strategy='median')\n",
    "\n",
    "# Scale each feature to 0-1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "imputer.fit(train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test)\n",
    "\n",
    "# Repeat with the scaler\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "print('Training data shape: ', train.shape().data())\n",
    "print('Testing data shape: ', test.shape().data())\n",
    "\n",
    "from experiment_graph.sklearn_helper.linear_model import LogisticRegression\n",
    "\n",
    "# Make the model with the specified regularization parameter\n",
    "log_reg = LogisticRegression(C=0.0001)\n",
    "\n",
    "# Train on the training data\n",
    "log_reg.fit(train, train_labels)\n",
    "\n",
    "score = log_reg.score(test,\n",
    "                      test_labels['TARGET'],\n",
    "                      score_type='auc').data()\n",
    "print('Logistic Regression with AUC score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d66488",
   "metadata": {},
   "source": [
    "## Improved Model: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969c99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_graph.sklearn_helper.ensemble import RandomForestClassifier\n",
    "\n",
    "# Make the random forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Train on the training data\n",
    "random_forest.fit(train, train_labels)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importances = random_forest.feature_importances(features)\n",
    "\n",
    "score = random_forest.score(test,\n",
    "                            test_labels['TARGET'],\n",
    "                            score_type='auc').data()\n",
    "print('Random Forest Simple Data with AUC score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba604a0",
   "metadata": {},
   "source": [
    "## Make Predictions using Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382798fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features_names = list(app_train_poly.data().columns)\n",
    "\n",
    "# Impute the polynomial features\n",
    "imputer2 = Imputer(strategy='median')\n",
    "\n",
    "imputer2.fit(app_train_poly)\n",
    "\n",
    "app_train_poly = imputer2.transform(app_train_poly)\n",
    "app_test_poly = imputer2.transform(app_test_poly)\n",
    "\n",
    "# Scale the polynomial features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "app_train_poly = scaler.fit_transform(app_train_poly)\n",
    "app_test_poly = scaler.transform(app_test_poly)\n",
    "\n",
    "random_forest_poly = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "random_forest_poly.fit(app_train_poly, train_labels)\n",
    "\n",
    "score = random_forest_poly.score(app_test_poly,\n",
    "                                 test_labels['TARGET'],\n",
    "                                 score_type='auc').data()\n",
    "print('Random Forest Poly Data with AUC score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e62c8",
   "metadata": {},
   "source": [
    "## Testing Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_domain = app_train_domain.drop(columns='TARGET')\n",
    "\n",
    "domain_features_names = list(app_train_domain.data().columns)\n",
    "\n",
    "# Impute the domainnomial features\n",
    "imputer = Imputer(strategy='median')\n",
    "\n",
    "domain_features = imputer.fit_transform(app_train_domain)\n",
    "domain_features_test = imputer.transform(app_test_domain)\n",
    "\n",
    "# Scale the domainnomial features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "domain_features = scaler.fit_transform(domain_features)\n",
    "domain_features_test = scaler.transform(domain_features_test)\n",
    "\n",
    "# Train on the training data\n",
    "random_forest_domain = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "random_forest_domain.fit(domain_features, train_labels)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances_domain = random_forest_domain.feature_importances(domain_features_names)\n",
    "\n",
    "score = random_forest_domain.score(domain_features_test,\n",
    "                                   test_labels['TARGET'],\n",
    "                                   score_type='auc').data()\n",
    "print('Random Forest Domain Data with AUC score: {}'.format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
