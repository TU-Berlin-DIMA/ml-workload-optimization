{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materialization Correctness\n",
    "To ensure the materialization code is working correctly, in this notebook, we create a medium size graph with several ml models and manually check the potential, recreation cost, and |pipelines| to compare with the computed values from the materialization code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import sys\n",
    "import os\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import cPickle as pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ROOT = '/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/'\n",
    "ROOT_PACKAGE_DIRECTORY = '/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/collaborative-optimizer'\n",
    "root_data = ROOT + '/data'\n",
    "\n",
    "sys.path.append(ROOT_PACKAGE_DIRECTORY)\n",
    "from experiment_graph.executor import CollaborativeExecutor\n",
    "from experiment_graph.workload import Workload\n",
    "from experiment_graph.execution_environment import ExecutionEnvironment\n",
    "from experiment_graph.benchmark_helper import BenchmarkMetrics\n",
    "\n",
    "DATABASE_PATH = root_data + '/experiment_graphs/home-credit-default-risk/materialization-test'\n",
    "N_ESTIMATOR = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaterializationTest(Workload):\n",
    "    def run(self, execution_environment, root_data):\n",
    "        # Load Data\n",
    "        train = execution_environment.load(root_data +\n",
    "                                           '/openml/task_id=31/train.csv')\n",
    "        test = execution_environment.load(root_data +\n",
    "                                          '/openml/task_id=31/test.csv')\n",
    "\n",
    "        test_labels = test['class']\n",
    "        test = test.drop('class')\n",
    "\n",
    "        train_labels = train['class']\n",
    "        train = train.drop(columns=['class'])\n",
    "\n",
    "        train2 = train.drop('checking_status')\n",
    "        test2 = test.drop('checking_status')\n",
    "        from experiment_graph.sklearn_helper.preprocessing import MinMaxScaler\n",
    "        scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler2.fit(train2)\n",
    "        train2 = scaler2.transform(train2)\n",
    "        test2 = scaler2.transform(test2)\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler.fit(train)\n",
    "        train = scaler.transform(train)\n",
    "        test = scaler.transform(test)\n",
    "        # Random Forest 1 (n_estimator = 10)\n",
    "        from experiment_graph.sklearn_helper.ensemble import RandomForestClassifier\n",
    "        random_forest10 = RandomForestClassifier(\n",
    "            n_estimators=10, random_state=50, verbose=1, n_jobs=-1)\n",
    "        random_forest10.fit(train, train_labels)\n",
    "\n",
    "        # Execute\n",
    "        random_forest10.trained_node.data()\n",
    "\n",
    "        print 'random_forest10: {}'.format(\n",
    "            random_forest10.score(test, test_labels).data())\n",
    "\n",
    "        # Random Forest 2 (n_estimator = 100)\n",
    "        from experiment_graph.sklearn_helper.ensemble import RandomForestClassifier\n",
    "        random_forest100 = RandomForestClassifier(\n",
    "            n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "        random_forest100.fit(train, train_labels)\n",
    "\n",
    "        # Execute\n",
    "        random_forest100.trained_node.data()\n",
    "\n",
    "        print 'random_forest100: {}'.format(\n",
    "            random_forest100.score(test, test_labels).data())\n",
    "\n",
    "        # Logistic Regression\n",
    "        from experiment_graph.sklearn_helper.linear_model import LogisticRegression\n",
    "\n",
    "        # Make the model with the specified regularization parameter\n",
    "        log_reg = LogisticRegression(C=0.0001)\n",
    "\n",
    "        # Train on the training data\n",
    "        log_reg.fit(train, train_labels)\n",
    "\n",
    "        print 'log_reg: {}'.format(log_reg.score(test, test_labels).data())\n",
    "\n",
    "        # Gradient Boosted 1 (n_estimator = 60)\n",
    "        from experiment_graph.sklearn_helper.sklearn_wrappers import LGBMClassifier\n",
    "\n",
    "        model60 = LGBMClassifier(\n",
    "            n_estimators=60,\n",
    "            objective='binary',\n",
    "            class_weight='balanced',\n",
    "            learning_rate=0.05,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            subsample=0.8,\n",
    "            n_jobs=-1,\n",
    "            random_state=50)\n",
    "\n",
    "        # Train the model\n",
    "        model60.fit(\n",
    "            train,\n",
    "            train_labels,\n",
    "            custom_args={\n",
    "                'eval_metric': 'accuracy',\n",
    "                'verbose': 200\n",
    "            })\n",
    "\n",
    "        # Make predictions\n",
    "        print 'model60: '.format(model60.score(test, test_labels).data())\n",
    "\n",
    "        # Gradient Boosted 2 (n_estimator = 100)\n",
    "        from experiment_graph.sklearn_helper.sklearn_wrappers import LGBMClassifier\n",
    "\n",
    "        model100 = LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            objective='binary',\n",
    "            class_weight='balanced',\n",
    "            learning_rate=0.05,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            subsample=0.8,\n",
    "            n_jobs=-1,\n",
    "            random_state=50)\n",
    "\n",
    "        # Train the model\n",
    "        model100.fit(\n",
    "            train,\n",
    "            train_labels,\n",
    "            custom_args={\n",
    "                'eval_metric': 'accuracy',\n",
    "                'verbose': 200\n",
    "            })\n",
    "\n",
    "        # Make predictions\n",
    "        print 'model100: {}'.format(model100.score(test, test_labels).data())\n",
    "\n",
    "        # Gradient Boosted 3 (n_estimator = 60) modified train\n",
    "        from experiment_graph.sklearn_helper.sklearn_wrappers import LGBMClassifier\n",
    "\n",
    "        model60_2 = LGBMClassifier(\n",
    "            n_estimators=60,\n",
    "            objective='binary',\n",
    "            class_weight='balanced',\n",
    "            learning_rate=0.05,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            subsample=0.8,\n",
    "            n_jobs=-1,\n",
    "            random_state=50)\n",
    "\n",
    "        # Train the model\n",
    "        model60_2.fit(\n",
    "            train2,\n",
    "            train_labels,\n",
    "            custom_args={\n",
    "                'eval_metric': 'accuracy',\n",
    "                'verbose': 200\n",
    "            })\n",
    "\n",
    "        # Make predictions\n",
    "        print 'model60_2: {}'.format(\n",
    "            model60_2.score(test2, test_labels).data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a new root node\n",
      "creating a new root node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_forest10: {'accuracy': 0.79000000000000004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_forest100: {'accuracy': 0.76000000000000001}\n",
      "log_reg: {'accuracy': 0.69999999999999996}\n",
      "model60: \n",
      "model100: {'accuracy': 0.81000000000000005}\n",
      "model60_2: {'accuracy': 0.78000000000000003}\n",
      "details of test.csv{}\n",
      "test.csv{}:0\n",
      "details of 1F22A9E7687B4B9E5ED728910FC93F1A\n",
      "1F22A9E7687B4B9E5ED728910FC93F1A:0.085\n",
      "details of C42F2CDE6885202579E291D6BC09DAA6\n",
      "C42F2CDE6885202579E291D6BC09DAA6:1.039\n",
      "details of 0FC9E6AC48D4FCE5147BEB34DDDDFD50\n",
      "0FC9E6AC48D4FCE5147BEB34DDDDFD50:2.294\n",
      "details of train.csv{}\n",
      "train.csv{}:0\n",
      "details of DBDC4A474E8B084B17D4A95D82CBA082\n",
      "DBDC4A474E8B084B17D4A95D82CBA082:0.201\n",
      "details of 991F3690053BFE13CEDF335CFB510266\n",
      "991F3690053BFE13CEDF335CFB510266:1.021\n",
      "details of B6D925223AFD41F381EEEE2967E26DA5\n",
      "B6D925223AFD41F381EEEE2967E26DA5:2.185\n",
      "details of 1D4D55E9CF2EB86188615EFCAD918B8C\n",
      "1D4D55E9CF2EB86188615EFCAD918B8C:3.309\n",
      "details of 5C9271621A96AD66CB0D341714883C85\n",
      "5C9271621A96AD66CB0D341714883C85:5.494\n",
      "details of 596B0B62E30B223C703490CF5D545DAF\n",
      "596B0B62E30B223C703490CF5D545DAF:8.165\n",
      "details of B3CAE91C14A3EDABFF45495C43B1ABCB\n",
      "B3CAE91C14A3EDABFF45495C43B1ABCB:8.366\n",
      "details of 029FA5666CA1DD38EE8E4288463C631A\n",
      "029FA5666CA1DD38EE8E4288463C631A:75.42\n",
      "details of 8889791FA992D1AD0109D6485C3017B8\n",
      "8889791FA992D1AD0109D6485C3017B8:5.603\n",
      "details of F9AF74A654F14A9A8F31F5DAED9C02F0\n",
      "F9AF74A654F14A9A8F31F5DAED9C02F0:8.344\n",
      "details of 59E7543E0A134129D332EC003CBF3361\n",
      "59E7543E0A134129D332EC003CBF3361:83.849\n",
      "details of 9880C7344FDB8CE10D61B824886F173B\n",
      "9880C7344FDB8CE10D61B824886F173B:92.381\n",
      "details of 30BA78B1E3083C85ED55C85637140914\n",
      "30BA78B1E3083C85ED55C85637140914:1.875\n",
      "details of 448182324B46BA67CB20BAD4D5BAB73C\n",
      "448182324B46BA67CB20BAD4D5BAB73C:2.914\n",
      "details of B0B5D94F1B43F18EF00C461EC8EEDFAF\n",
      "B0B5D94F1B43F18EF00C461EC8EEDFAF:4.789\n",
      "details of 539A83F3935BADF7F9842FFABAFE2430\n",
      "539A83F3935BADF7F9842FFABAFE2430:2.896\n",
      "details of C2B2A08B3D0EFA08C2A06B7187FF500B\n",
      "C2B2A08B3D0EFA08C2A06B7187FF500B:5.012\n",
      "details of 28972A53A5F921894C4B56DA69ECE52F\n",
      "28972A53A5F921894C4B56DA69ECE52F:5.213\n",
      "details of 08EC727794D7B5A6622E108F368DAD1D\n",
      "08EC727794D7B5A6622E108F368DAD1D:10.04\n",
      "details of 429F92CD2E80CA30C32D096AF3E24A27\n",
      "429F92CD2E80CA30C32D096AF3E24A27:14.914\n",
      "details of 15CACA3BC2BC659A8F81EFE984D2D68A\n",
      "15CACA3BC2BC659A8F81EFE984D2D68A:15.921\n",
      "details of DAC9A51CCEDE8E31B16140D99E5E3669\n",
      "DAC9A51CCEDE8E31B16140D99E5E3669:204.829\n",
      "details of 5F282EF7E00BA14FDECDFC92D37BC3D8\n",
      "5F282EF7E00BA14FDECDFC92D37BC3D8:209.703\n",
      "details of 9702F0558BBE991A5F797648D924E60F\n",
      "9702F0558BBE991A5F797648D924E60F:218.622\n",
      "details of 72AB5AD853328590640757BF5E2568D8\n",
      "72AB5AD853328590640757BF5E2568D8:278.18\n",
      "details of 6875B7BDFA15F631ED92E497B59B1CFE\n",
      "6875B7BDFA15F631ED92E497B59B1CFE:283.054\n",
      "details of 448D4A612F200A65509F41E5FE4B3F77\n",
      "448D4A612F200A65509F41E5FE4B3F77:395.297\n",
      "details of 2CCB1EC7A9FF11E906F0C2558E6A6CFF\n",
      "2CCB1EC7A9FF11E906F0C2558E6A6CFF:174.79\n",
      "details of 0A7213C8E32C2A311DAF741D6EE50F05\n",
      "0A7213C8E32C2A311DAF741D6EE50F05:179.664\n",
      "details of AD4CB332A23AF19C1150A43876F369FD\n",
      "AD4CB332A23AF19C1150A43876F369FD:318.053\n",
      "details of D5F35493D6BEBF4BCE5EE70EACDF6617\n",
      "D5F35493D6BEBF4BCE5EE70EACDF6617:74.812\n",
      "details of 0A09DC527E40CB308EF45023B9041077\n",
      "0A09DC527E40CB308EF45023B9041077:79.686\n",
      "details of 791406BCA0FD8631232A21A8C5810F80\n",
      "791406BCA0FD8631232A21A8C5810F80:88.228\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'put'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-82796ac144da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mworkload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaterializationTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCollaborativeExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mee\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_to_end_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/collaborative-optimizer/experiment_graph/executor.pyc\u001b[0m in \u001b[0;36mend_to_end_run\u001b[0;34m(self, workload, **args)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_workload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/collaborative-optimizer/experiment_graph/executor.pyc\u001b[0m in \u001b[0;36mglobal_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_heuristics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         self.materializer.run_and_materialize(self.execution_environment.experiment_graph,\n\u001b[0;32m---> 98\u001b[0;31m                                               self.execution_environment.workload_dag)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/collaborative-optimizer/experiment_graph/materialization_algorithms/materialization_methods.pyc\u001b[0m in \u001b[0;36mrun_and_materialize\u001b[0;34m(self, experiment_graph, workload_dag, verbose)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[1;32m    100\u001b[0m         \u001b[0mshould_materialize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkload_dag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaterialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkload_dag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_materialize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/collaborative-optimizer/experiment_graph/materialization_algorithms/materialization_methods.pyc\u001b[0m in \u001b[0;36mmaterialize\u001b[0;34m(experiment_graph, workload_dag, should_materialize)\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0martifact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkload_dag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     experiment_graph.materialize(node_id=node_id,\n\u001b[0;32m---> 79\u001b[0;31m                                                  artifact=artifact)\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/collaborative-optimizer/experiment_graph/graph/graph_representations.py\u001b[0m in \u001b[0;36mmaterialize\u001b[0;34m(self, node_id, artifact)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Feature'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munderlying_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munderlying_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'put'"
     ]
    }
   ],
   "source": [
    "# Prepare the Data\n",
    "# Create Execution Environment with AllMaterializer\n",
    "ee = ExecutionEnvironment('dedup')\n",
    "workload = MaterializationTest()\n",
    "executor = CollaborativeExecutor(ee)\n",
    "executor.end_to_end_run(workload=workload, root_data=root_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor.execution_environment.experiment_graph.plot_graph(\n",
    "    plt,\n",
    "    figsize=(14, 20),\n",
    "    labels_for_vertex=['size'],\n",
    "    labels_for_edges=['name'],\n",
    "    vertex_size=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Based Materialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_graph.materialization_algorithms.materialization_methods import HeuristicsMaterializer\n",
    "budget = 1024\n",
    "heuristicsMat = HeuristicsMaterializer(storage_budget=budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare the Data\n",
    "# Create Execution Environment with AllMaterializer\n",
    "ee = ExecutionEnvironment('dedup')\n",
    "workload = MaterializationTest()\n",
    "executor = CollaborativeExecutor(ee, heuristicsMat)\n",
    "executor.end_to_end_run(workload=workload, root_data=root_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor.execution_environment.experiment_graph.plot_graph(\n",
    "    plt,\n",
    "    figsize=(14, 20),\n",
    "    labels_for_vertex=['size'],\n",
    "    labels_for_edges=['name'],\n",
    "    vertex_size=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Total size of all the artifacts: {}'.format(\n",
    "    executor.execution_environment.experiment_graph.get_total_size())\n",
    "\n",
    "print 'Total size of all the materialized artifacts: {}'.format(\n",
    "    executor.execution_environment.experiment_graph.get_real_size())\n",
    "\n",
    "print 'Sum of size of all the materialized artifacts: {}'.format(\n",
    "    executor.execution_environment.experiment_graph.\n",
    "    get_total_materialized_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage Aware Materialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_graph.materialization_algorithms.materialization_methods import StorageAwareMaterializer\n",
    "budget = 1024\n",
    "heuristicsMat = StorageAwareMaterializer(storage_budget=budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Data\n",
    "# Create Execution Environment with AllMaterializer\n",
    "ee = ExecutionEnvironment('dedup')\n",
    "workload = MaterializationTest()\n",
    "executor = CollaborativeExecutor(ee, heuristicsMat)\n",
    "executor.end_to_end_run(workload=workload, root_data=root_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor.execution_environment.experiment_graph.plot_graph(\n",
    "    plt,\n",
    "    figsize=(14, 20),\n",
    "    labels_for_vertex=['size'],\n",
    "    labels_for_edges=['name'],\n",
    "    vertex_size=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Total size of all the artifacts: {}'.format(\n",
    "    executor.execution_environment.experiment_graph.get_total_size())\n",
    "\n",
    "print 'Total size of all the materialized artifacts: {}'.format(\n",
    "    executor.execution_environment.experiment_graph.get_real_size())\n",
    "\n",
    "print 'Real size of all the materialized artifacts: {}'.format(\n",
    "    executor.execution_environment.experiment_graph.\n",
    "    get_total_materialized_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = executor.execution_environment.experiment_graph.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
