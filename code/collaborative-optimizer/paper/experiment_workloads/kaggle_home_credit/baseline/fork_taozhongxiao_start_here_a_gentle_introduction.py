#!/usr/bin/env python

"""Fork of baseline workload 1

A fork of the original workload 'start_here_a_gentle_introduction'.
The fork is submitted by user "taozhongxiao" to Kaggle's Home Credit Default Risk competition

Original notebook is located here:
    https://www.kaggle.com/taozhongxiao/begining-with-lightgbm-in-detail

Description:
    This workload modifies the baseline workload 1 and performs a grid search
    for lightgbm model on the original data.
    Then the users join the domain features and the poly features which were generated in the original workload
    and performs similar grid and random search for hyperparmaeters of lightgbm model

Necessary modifications to make the script work with Experiment Graph:
    1. A test dataset is generated by splitting the original data
    2. Cross validation is replaced with full training
    3. Improves the join of the domain and poly features as the user's script had many performance issues


"""

import os
# File system management
# Suppress warnings
import warnings
# plotting libraries
from datetime import datetime

import lightgbm as lgb
import matplotlib

matplotlib.use('ps')
import matplotlib.pyplot as plt
import seaborn as sns

from experiment_graph.executor import BaselineExecutor
from experiment_graph.workload import Workload
import itertools
# numpy and pandas for data manipulation
import numpy as np
import pandas as pd

# sklearn preprocessing for dealing with categorical variables

warnings.filterwarnings('ignore')


class fork_taozhongxiao_start_here_a_gentle_introduction(Workload):

    def run(self, root_data):
        MAX_EVALS = 5
        print(os.listdir(root_data))
        train_features = pd.read_csv(root_data + '/kaggle_home_credit/application_train.csv')
        train_features = train_features.sample(n=16000, random_state=42)
        print(train_features.shape)

        train_features.dtypes.value_counts()

        train_features = train_features.select_dtypes('number')
        train_labels = np.array(train_features['TARGET'])
        train_features = train_features.drop(columns=['TARGET', 'SK_ID_CURR'])

        test_features = pd.read_csv(root_data + '/kaggle_home_credit/application_test.csv')  # manually added
        test_features.head()  # manually added
        test_labels = pd.read_csv(root_data + '/kaggle_home_credit/application_test_labels.csv')  # manually added
        test_features = test_features.select_dtypes('number')  # manually added
        test_features = test_features.drop(columns=['SK_ID_CURR'])  # manually added

        print("Training features shape: ", train_features.shape)
        print("Testing features shape: ", test_features.shape)

        from sklearn.metrics import roc_auc_score

        # Train and make predicions with model
        model = lgb.LGBMClassifier()
        default_params = model.get_params()
        model.fit(train_features, train_labels)
        preds = model.predict_proba(test_features)[:, 1]
        baseline_auc = roc_auc_score(test_labels['TARGET'], preds)
        print('The model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))

        def objective(hyperparameters, iteration):

            if 'n_estimators' in hyperparameters.keys():
                del hyperparameters['n_estimators']
            model.set_params(**hyperparameters)
            model.fit(train_features, train_labels)
            preds = model.predict_proba(test_features)[:, 1]
            score = roc_auc_score(test_labels['TARGET'], preds)
            return [score, hyperparameters, iteration]

        score, params, iteration = objective(default_params, 1)

        print('The cross-validation ROC AUC was {:.5f}.'.format(score))

        param_grid = {
            'boosting_type': ['gbdt', 'goss', 'dart'],
            'num_leaves': list(range(20, 150)),
            'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base=10, num=1000)),
            'subsample_for_bin': list(range(20000, 300000, 20000)),
            'min_child_samples': list(range(20, 500, 5)),
            'reg_alpha': list(np.linspace(0, 1)),
            'reg_lambda': list(np.linspace(0, 1)),
            'colsample_bytree': list(np.linspace(0.6, 1, 10)),
            'subsample': list(np.linspace(0.5, 1, 100)),
            'is_unbalance': [True, False]
        }

        import random

        random.seed(50)

        # Randomly sample a boosting type
        boosting_type = random.sample(param_grid['boosting_type'], 1)[0]

        # Set subsample depending on boosting type
        subsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]

        print('Boosting type: ', boosting_type)
        print('Subsample ratio: ', subsample)

        # random_results = pd.DataFrame(columns=['score', 'params', 'iteration'],
        #                               index=list(range(MAX_EVALS)))
        #
        # grid_results = pd.DataFrame(columns=['score', 'params', 'iteration'],
        #                             index=list(range(MAX_EVALS)))

        def grid_search(param_grid, max_evals=MAX_EVALS):
            """Grid search algorithm (with limit on max evals)"""
            results = pd.DataFrame(columns=['score', 'params', 'iteration'],
                                   index=list(range(MAX_EVALS)))
            keys, values = zip(*param_grid.items())
            i = 0
            for v in itertools.product(*values):
                hyperparameters = dict(zip(keys, v))
                hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters[
                    'subsample']
                eval_results = objective(hyperparameters, i)
                results.loc[i, :] = eval_results
                i += 1
                if i > max_evals:
                    break
            results.sort_values('score', ascending=False, inplace=True)
            results.reset_index(inplace=True)
            return results

        grid_results = grid_search(param_grid)
        print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))
        print('\nThe best hyperparameters were:')
        import pprint
        pprint.pprint(grid_results.loc[0, 'params'])

        grid_search_params = grid_results.loc[0, 'params']
        grid_search_params['random_state'] = 42
        model = lgb.LGBMClassifier(**grid_search_params)
        model.fit(train_features, train_labels)

        preds = model.predict_proba(test_features)[:, 1]

        print('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(
            roc_auc_score(test_labels['TARGET'], preds)))

        random.seed(50)

        # Randomly sample from dictionary
        random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}
        # Deal with subsample ratio
        random_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']

        random_params

        # Function to calculate missing values by column# Funct
        def random_search(param_grid, max_evals=MAX_EVALS):
            """Random search for hyperparameter optimization"""

            # Dataframe for results
            results = pd.DataFrame(columns=['score', 'params', 'iteration'],
                                   index=list(range(MAX_EVALS)))

            # Keep searching until reach max evaluations
            for i in range(max_evals):
                # Choose random hyperparameters
                hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}
                hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters[
                    'subsample']

                # Evaluate randomly selected hyperparameters
                eval_results = objective(hyperparameters, i)

                results.loc[i, :] = eval_results

            # Sort with best score on top
            results.sort_values('score', ascending=False, inplace=True)
            results.reset_index(inplace=True)
            return results

        random_results = random_search(param_grid)

        print('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))
        print('\nThe best hyperparameters were:')

        import pprint
        pprint.pprint(random_results.loc[0, 'params'])

        random_search_params = random_results.loc[0, 'params']

        # Create, train, test model
        grid_search_params['random_state'] = 42
        model = lgb.LGBMClassifier(**random_search_params)
        model.fit(train_features, train_labels)

        preds = model.predict_proba(test_features)[:, 1]

        print('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(
            roc_auc_score(test_labels['TARGET'], preds)))

        train = pd.read_csv(root_data + '/kaggle_home_credit/application_train.csv')
        test = pd.read_csv(root_data + '/kaggle_home_credit/application_test.csv')

        # Extract the test ids and train labels
        test_ids = test['SK_ID_CURR']
        train_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1,))

        train = train.drop(columns=['SK_ID_CURR', 'TARGET'])
        test = test.drop(columns=['SK_ID_CURR'])

        print('Training shape: ', train.shape)
        print('Testing shape: ', test.shape)

        from sklearn.preprocessing import LabelEncoder

        le = LabelEncoder()
        le_count = 0
        for col in train:
            if train[col].dtype == 'object':
                if len(list(train[col].unique())) <= 2:
                    le.fit(train[col])
                    train[col] = le.transform(train[col])
                    test[col] = le.transform(test[col])
                    le_count += 1
        print('%d columns were label encoded.' % le_count)

        train = pd.get_dummies(train)
        test = pd.get_dummies(test)
        print(train.shape, test.shape)

        train, test = train.align(test, join='inner', axis=1)

        # Add the target back in
        # train['TARGET'] = train_labels

        print('Training Features shape: ', train.shape)
        print('Testing Features shape: ', test.shape)

        hyperparameters = dict(**random_results.loc[0, 'params'])

        model = lgb.LGBMClassifier(**hyperparameters)
        model.fit(train, train_labels)

        # Predictions on the test data
        preds = model.predict_proba(test)[:, 1]
        print('LGBM score with best hyperparameters from random search {}.'.format(
            roc_auc_score(test_labels['TARGET'], preds)))

        app_train = pd.read_csv(root_data + '/kaggle_home_credit/application_train.csv')
        print('Training data shape: ', app_train.shape)
        app_train.head()

        app_test = pd.read_csv(root_data + '/kaggle_home_credit/application_test.csv')
        print('Testing data shape: ', app_test.shape)
        app_test.head()

        print app_train['TARGET'].value_counts()

        plt.figure(figsize=(10, 5))
        sns.set(style="whitegrid", font_scale=1)
        g = sns.distplot(app_train['TARGET'], kde=False, hist_kws={"alpha": 1, "color": "#DA1A32"})
        plt.title('Distribution of target (1:default, 0:no default)', size=15)
        plt.show()

        def missing_values_table(df):
            # Total missing values
            mis_val = df.isnull().sum()

            # Percentage of missing values
            mis_val_percent = 100 * df.isnull().sum() / len(df)

            # Make a table with the results
            mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)

            # Rename the columns
            mis_val_table_ren_columns = mis_val_table.rename(columns={
                0: 'Missing Values',
                1: '% of Total Values'
            })

            # Sort the table by percentage of missing descending
            mis_val_table_ren_columns = mis_val_table_ren_columns[
                mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(
                '% of Total Values', ascending=False).round(1)

            # Print some summary information
            print("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"
                                                                      "There are " + str(
                mis_val_table_ren_columns.shape[0]) +
                  " columns that have missing values.")

            # Return the dataframe with missing information
            return mis_val_table_ren_columns

        missing_values = missing_values_table(app_train)
        print missing_values.head(20)

        app_train.dtypes.value_counts()

        app_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)

        # Create a label encoder object
        le = LabelEncoder()
        le_count = 0

        # Iterate through the columns
        for col in app_train:
            if app_train[col].dtype == 'object':
                # If 2 or fewer unique categories
                if len(list(app_train[col].unique())) <= 2:
                    # Train on the training data
                    le.fit(app_train[col])
                    # Transform both training and testing data
                    app_train[col] = le.transform(app_train[col])
                    app_test[col] = le.transform(app_test[col])

                    # Keep track of how many columns were label encoded
                    le_count += 1

        print('%d columns were label encoded.' % le_count)

        # one-hot encoding of categorical variables
        app_train = pd.get_dummies(app_train)
        app_test = pd.get_dummies(app_test)

        print('Training Features shape: ', app_train.shape)
        print('Testing Features shape: ', app_test.shape)

        train_labels = app_train['TARGET']

        # Align the training and testing data, keep only columns present in both dataframes
        app_train, app_test = app_train.align(app_test, join='inner', axis=1)

        # Add the target back in
        app_train['TARGET'] = train_labels

        print('Training Features shape: ', app_train.shape)
        print('Testing Features shape: ', app_test.shape)

        (app_train['DAYS_BIRTH'] / -365).describe()

        app_train['DAYS_EMPLOYED'].describe()

        app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')
        plt.xlabel('Days Employment')

        anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]
        non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]
        print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))
        print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))
        print('There are %d anomalous days of employment' % len(anom))

        # Create an anomalous flag column
        app_train['DAYS_EMPLOYED_ANOM'] = app_train["DAYS_EMPLOYED"] == 365243

        # Replace the anomalous values with nan
        app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)

        app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')
        plt.xlabel('Days Employment')

        app_test['DAYS_EMPLOYED_ANOM'] = app_test["DAYS_EMPLOYED"] == 365243
        app_test["DAYS_EMPLOYED"].replace({365243: np.nan}, inplace=True)
        print('There are %d anomalies in the test data out of %d entries' % (
            app_test["DAYS_EMPLOYED_ANOM"].sum(), len(app_test)))

        # Find correlations with the target and sort
        correlations = app_train.corr()['TARGET'].sort_values()

        # Display correlations
        print('Most Positive Correlations:\n', correlations.tail(15))
        print('\nMost Negative Correlations:\n', correlations.head(15))
        #
        # # Find the correlation of the positive days since birth and target
        app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])

        # Make a new dataframe for polynomial features
        poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]
        poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]

        # imputer for handling missing values
        from sklearn.preprocessing import Imputer

        imputer = Imputer(strategy='median')

        poly_target = poly_features['TARGET']

        poly_features = poly_features.drop(columns=['TARGET'])

        # Need to impute missing values
        poly_features = imputer.fit_transform(poly_features)
        poly_features_test = imputer.transform(poly_features_test)

        from sklearn.preprocessing import PolynomialFeatures

        # Create the polynomial object with specified degree
        poly_transformer = PolynomialFeatures(degree=3)

        # Train the polynomial features
        poly_transformer.fit(poly_features)

        # Transform the features
        poly_features = poly_transformer.transform(poly_features)
        poly_features_test = poly_transformer.transform(poly_features_test)
        print('Polynomial Features shape: ', poly_features.shape)

        poly_transformer.get_feature_names(
            input_features=['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])

        # Create a dataframe of the features
        poly_features = pd.DataFrame(poly_features,
                                     columns=poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2',
                                                                                 'EXT_SOURCE_3', 'DAYS_BIRTH']))

        # Add in the target
        poly_features['TARGET'] = poly_target

        # Find the correlations with the target
        poly_corrs = poly_features.corr()['TARGET'].sort_values()

        # Display most negative and most positive
        print(poly_corrs.head(10))
        print(poly_corrs.tail(5))

        # Put test features into dataframe
        poly_features_test = pd.DataFrame(poly_features_test,
                                          columns=poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2',
                                                                                      'EXT_SOURCE_3', 'DAYS_BIRTH']))

        # Merge polynomial features into training dataframe
        poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']
        app_train_poly = app_train.merge(poly_features, on='SK_ID_CURR', how='left')

        # Merge polnomial features into testing dataframe
        poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']
        app_test_poly = app_test.merge(poly_features_test, on='SK_ID_CURR', how='left')

        # Align the dataframes
        app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join='inner', axis=1)

        # Print out the new shapes
        print('Training data with polynomial features shape: ', app_train_poly.shape)
        print('Testing data with polynomial features shape:  ', app_test_poly.shape)

        app_train_domain = app_train.copy()
        app_test_domain = app_test.copy()

        app_train_poly['CREDIT_INCOME_PERCENT'] = app_train['AMT_CREDIT'] / app_train[
            'AMT_INCOME_TOTAL']
        app_train_poly['ANNUITY_INCOME_PERCENT'] = app_train['AMT_ANNUITY'] / app_train[
            'AMT_INCOME_TOTAL']
        app_train_poly['CREDIT_TERM'] = app_train['AMT_ANNUITY'] / app_train['AMT_CREDIT']
        app_train_poly['DAYS_EMPLOYED_PERCENT'] = app_train['DAYS_EMPLOYED'] / app_train['DAYS_BIRTH']

        app_test_poly['CREDIT_INCOME_PERCENT'] = app_test['AMT_CREDIT'] / app_test['AMT_INCOME_TOTAL']
        app_test_poly['ANNUITY_INCOME_PERCENT'] = app_test['AMT_ANNUITY'] / app_test['AMT_INCOME_TOTAL']
        app_test_poly['CREDIT_TERM'] = app_test['AMT_ANNUITY'] / app_test['AMT_CREDIT']
        app_test_poly['DAYS_EMPLOYED_PERCENT'] = app_test['DAYS_EMPLOYED'] / app_test['DAYS_BIRTH']

        app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join='inner', axis=1)

        app_train_poly.isnull().sum()

        print(app_train_poly.shape, app_test_poly.shape)

        from sklearn.preprocessing import Imputer
        col_2 = list(app_train_poly.columns)
        if 'TARGET' in app_train_poly:
            train = app_train_poly.drop(columns=['TARGET'])
        else:
            train = app_train_poly.copy()

        features = list(train.columns)

        test = app_test_poly.copy()

        imputer = Imputer(strategy='median')

        imputer.fit(train)

        train = imputer.transform(train)
        test = imputer.transform(test)

        print('Training data shape: ', train.shape)
        print('Testing data shape: ', test.shape)

        train = pd.DataFrame(train, columns=col_2)
        test = pd.DataFrame(test, columns=col_2)

        train['TARGET'] = app_train['TARGET']
        train.head()

        MAX_EVALS = 5

        features = train.sample(n=16000, random_state=42)
        print(features.shape)
        labels = np.array(features['TARGET'])
        features = features.drop(columns=['TARGET', 'SK_ID_CURR'])
        test = test.drop(columns=['SK_ID_CURR'])  # manually added
        # Train and make predicions with model
        model = lgb.LGBMClassifier()
        default_params = model.get_params()
        model.fit(features, labels)
        preds = model.predict_proba(test)[:, 1]
        baseline_auc = roc_auc_score(test_labels['TARGET'], preds)
        print('The model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))

        def objective(hyperparameters, iteration):

            if 'n_estimators' in hyperparameters.keys():
                del hyperparameters['n_estimators']
            model.set_params(**hyperparameters)
            model.fit(features, labels)
            preds = model.predict_proba(test)[:, 1]
            score = roc_auc_score(test_labels['TARGET'], preds)
            return [score, hyperparameters, iteration]

        score, params, iteration = objective(default_params, 1)

        print('The cross-validation ROC AUC was {:.5f}.'.format(score))

        param_grid = {
            'boosting_type': ['gbdt', 'goss', 'dart'],
            'num_leaves': list(range(20, 150)),
            'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base=10, num=1000)),
            'subsample_for_bin': list(range(20000, 300000, 20000)),
            'min_child_samples': list(range(20, 500, 5)),
            'reg_alpha': list(np.linspace(0, 1)),
            'reg_lambda': list(np.linspace(0, 1)),
            'colsample_bytree': list(np.linspace(0.6, 1, 10)),
            'subsample': list(np.linspace(0.5, 1, 100)),
            'is_unbalance': [True, False]
        }

        import random

        random.seed(50)

        # Randomly sample a boosting type
        boosting_type = random.sample(param_grid['boosting_type'], 1)[0]

        # Set subsample depending on boosting type
        subsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]

        print('Boosting type: ', boosting_type)
        print('Subsample ratio: ', subsample)

        # random_results = pd.DataFrame(columns=['score', 'params', 'iteration'],
        #                               index=list(range(MAX_EVALS)))
        #
        # grid_results = pd.DataFrame(columns=['score', 'params', 'iteration'],
        #                             index=list(range(MAX_EVALS)))

        def grid_search(param_grid, max_evals=MAX_EVALS):
            """Grid search algorithm (with limit on max evals)"""
            results = pd.DataFrame(columns=['score', 'params', 'iteration'],
                                   index=list(range(MAX_EVALS)))
            keys, values = zip(*param_grid.items())
            i = 0
            for v in itertools.product(*values):
                hyperparameters = dict(zip(keys, v))
                hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters[
                    'subsample']
                eval_results = objective(hyperparameters, i)
                results.loc[i, :] = eval_results
                i += 1
                if i > max_evals:
                    break
            results.sort_values('score', ascending=False, inplace=True)
            results.reset_index(inplace=True)
            return results

        grid_results = grid_search(param_grid)
        print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))
        print('\nThe best hyperparameters were:')
        import pprint
        pprint.pprint(grid_results.loc[0, 'params'])

        grid_search_params = grid_results.loc[0, 'params']
        grid_search_params['random_state'] = 42
        model = lgb.LGBMClassifier(**grid_search_params)
        model.fit(features, labels)

        preds = model.predict_proba(test)[:, 1]

        print('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(
            roc_auc_score(test_labels['TARGET'], preds)))

        random.seed(50)

        # Randomly sample from dictionary
        random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}
        # Deal with subsample ratio
        random_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']

        random_params

        # Function to calculate missing values by column# Funct
        def random_search(param_grid, max_evals=MAX_EVALS):
            """Random search for hyperparameter optimization"""

            # Dataframe for results
            results = pd.DataFrame(columns=['score', 'params', 'iteration'],
                                   index=list(range(MAX_EVALS)))

            # Keep searching until reach max evaluations
            for i in range(max_evals):
                # Choose random hyperparameters
                hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}
                hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters[
                    'subsample']

                # Evaluate randomly selected hyperparameters
                eval_results = objective(hyperparameters, i)

                results.loc[i, :] = eval_results

            # Sort with best score on top
            results.sort_values('score', ascending=False, inplace=True)
            results.reset_index(inplace=True)
            return results

        random_results = random_search(param_grid)

        print('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))
        print('\nThe best hyperparameters were:')

        import pprint
        pprint.pprint(random_results.loc[0, 'params'])

        random_search_params = random_results.loc[0, 'params']

        # Create, train, test model
        random_search_params['random_state'] = 42
        model = lgb.LGBMClassifier(**random_search_params)
        labels = app_train['TARGET']
        features = app_train_poly.drop(columns=['SK_ID_CURR'])
        model.fit(app_train_poly, labels)

        preds = model.predict_proba(app_test_poly)[:, 1]

        print('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(
            roc_auc_score(test_labels['TARGET'], preds)))

        return True


if __name__ == "__main__":
    ROOT = '/Users/bede01/Documents/work/phd-papers/ml-workload-optimization'
    ROOT_PACKAGE = '/Users/bede01/Documents/work/phd-papers/ml-workload-optimization/code/collaborative-optimizer'
    root_data = ROOT + '/data'
    import sys

    sys.path.append(ROOT_PACKAGE)

    executor = BaselineExecutor()
    execution_start = datetime.now()
    workload = fork_taozhongxiao_start_here_a_gentle_introduction()
    executor.end_to_end_run(workload=workload, root_data=root_data)

    execution_end = datetime.now()
    elapsed = (execution_end - execution_start).total_seconds()

    print('finished execution in {} seconds'.format(elapsed))
