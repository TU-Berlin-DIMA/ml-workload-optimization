{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenML model search pipelines\n",
    "In this notebook, we analyze the scikit learn model search pipelines to include them in the graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openml import datasets, tasks, runs, flows, setups, config, evaluations\n",
    "from workloadoptimization.essentials import Component, ExperimentObject, ExperimentGraph, ExperimentParser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import uuid\n",
    "import networkx as nx\n",
    "import sklearn\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from openmlstudy14.preprocessing import ConditionalImputer as openMLConditionalImputer\n",
    "from hyperimp.utils.preprocessing import ConditionalImputer as hyperimpConditionalImputer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "config.apikey = '8e2079508005379adb9a60798696134e'\n",
    "config.server = 'https://www.openml.org/api/v1'\n",
    "config.set_cache_directory(os.path.expanduser('~/openml-cache'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOW = 8365\n",
    "TASK = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def getTopRuns(numberOfRuns, pipeline, task):\n",
    "    openMlEvaluations = evaluations.list_evaluations('predictive_accuracy', task= [task], flow = [pipeline])\n",
    "    evaluationData = pd.DataFrame.from_dict(openMlEvaluations, orient='index')\n",
    "    evaluationData['accuracy'] = evaluationData.apply(lambda eva: eva.values[0].value, axis = 1)\n",
    "    evaluationData['run_id'] = evaluationData.apply(lambda eva: eva.values[0].run_id, axis = 1)\n",
    "    # extracting the top ''numberOfRuns' runs\n",
    "    topRuns = evaluationData.sort_values('accuracy',ascending=False)\n",
    "    if (numberOfRuns>0):\n",
    "        topRuns = topRuns[0:numberOfRuns]\n",
    "    # retreiving the run objects from the top runs\n",
    "    openMLRuns = runs.list_runs(task=[task], flow=[pipeline])\n",
    "    experiments = pd.DataFrame.from_dict(openMLRuns,orient='index')\n",
    "    Experiment = experiments.merge(topRuns,on='run_id').drop(columns=['uploader',0])\n",
    "    # TODO: This is the limit on the api size, I should fix it to make calls in batches\n",
    "    Setup = pd.DataFrame.from_dict(setups.list_setups(setup=Experiment.setup_id[0:500],size = 500 ), orient='index').reset_index()\n",
    "    Setup.columns=['id', 'setup']\n",
    "\n",
    "    return pd.merge(Setup, Experiment, how = 'inner', left_on='id', right_on='setup_id').drop(columns = ['id','setup_id'])[['run_id','task_id','flow_id', 'accuracy','setup']]\n",
    "def extractExperiments(filePath, taskIds, flowIds):\n",
    "    if os.path.isfile(filePath):\n",
    "        return pd.read_pickle(filePath)\n",
    "    frames = []\n",
    "    for t in taskIds:\n",
    "        for f in flowIds:\n",
    "            frames.append(getTopRuns(100000,f,t))\n",
    "    Experiments = pd.concat(frames).reset_index(drop=True)\n",
    "    Experiments.to_pickle(filePath)\n",
    "    return Experiments\n",
    "\n",
    "# This is time consuming, so it is better to persist the list of the runs to disk\n",
    "# If you are changing the tasks or flow ids, remember to change the name of the file \n",
    "#Experiments = extractExperiments('meta/massive-study-picked-experiment-task-31', TASK_IDS, FLOW_IDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#openMLRuns = runs.list_runs(task=[TASK], flow=[FLOW], size = 10)\n",
    "# openMLRuns{'flow_id': 8365,'run_id': 9191572, 'setup_id': 7122364, 'task_id': 31,  'uploader': 1935}\n",
    "s = setups.get_setup(7122509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7ba6936eb4e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for k,v in s.parameters.iteritems():\n",
    "    print v.full_name,v.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = flows.get_flow(FLOW)\n",
    "skPipeline = flows.flow_to_sklearn(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=u'raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[(u'imputation', ConditionalImputer(axis=0,\n",
       "          categorical_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
       "          copy=True, fill_empty=0, missing_values=u'NaN', strategy=u'mean',\n",
       "       ...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=50, n_jobs=-1,\n",
       "          param_distributions=OrderedDict([(u'clf__bootstrap', [True, False]), (u'clf__criterion', [u'gini', u'entropy']), (u'clf__max_features', [0.05685145935354432]), (u'clf__min_samples_leaf', <scipy.stats._distn_infrastructure.rv_frozen object at 0x115c0de50>), (u'clf__min_samples_split', <scipy.stats._distn_infrastructure.rv_frozen object at 0x115c19350>), (u'clf__n_estimators', [300]), (u'clf__random_state', <scipy.stats._distn_infrastructure.rv_frozen object at 0x115c19690>)]),\n",
       "          pre_dispatch=u'2*n_jobs', random_state=1, refit=True,\n",
       "          return_train_score=u'warn', scoring=None, verbose=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type('asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
