\section{Artifact Materialization}\label{sec-materialization}
Depending on the number of the executed workloads, the generated artifacts may require large amount of storage space.
For example, in the Home Credit Default Risk Kaggle competition\footnote{https://www.kaggle.com/c/home-credit-default-risk}, a popular script which analyzes a dataset of 160 MB, generates up to 17 GB of artifacts.
Therefore, materializing every artifact under a limited storage budget is not feasible.
In this section, we introduce two algorithms for materializing the artifacts of the experiment graph under limited storage.
The first algorithm utilizes both general metrics, such as the size and access frequency of vertices and execution time of the edges, and a machine learning specific metric, i.e., the quality of the terminal models, to decide what artifacts to materialize.
The second algorithm is an extension of the first algorithm which also considers how the artifacts are stored on the file system.
Since many of the existing operations in the experiment graph are operating on one or a small group of columns inside a data frame, the resulting artifacts have many duplicated columns.
We implement a compression strategy which avoids storing duplicated columns.
The second algorithm takes the duplication information into account when making the decision on what artifacts to materialize.

\subsection{Materialization Problem Formulation}\label{subsec-materialization-problem}
Bhattacherjee et al. \cite{bhattacherjee2015principles} proposes an algorithm for efficient storage of different versions of a dataset (i.e., artifacts) under limited storage.
The goal of their algorithm is to materialize the artifacts which result in the lowest recreation cost while ensuring the total size of the materialized artifacts does not exceed the storage capacity.
However, there are several reasons which render their solution inapplicable to the artifact materialization problem.
First, their approach only considers access frequency and reconstruction cost of an artifact.
For the experiment graph, we must also consider the effect of the materialized artifacts on the efficiency of machine learning workloads, i.e., materialize artifacts which result in high-quality machine learning models.
Second, their solution does not consider merge operations, e.g., join, concatenation, and model training, which are common in machine learning workloads.
Lastly, their solution considers a scenario where new artifacts are rarely added.
In the machine learning workload optimization, new artifacts are continuously added to the experiment graph.
As a result, a proper solution must accommodate the addition of new artifacts.

Here, we first formulate the problem of artifact materialization as a multi-objective optimization problem, with the goal of minimizing two functions which consider both the artifact recreation cost and the model quality of the artifacts, given the storage requirement constraint.

\textbf{Weighted Recreation Cost Function (WC).} 
The first function computes the weighted recreation cost of all the non-merged vertices in the graph:
\[
WC(G) =  \sum\limits_{v \in V\setminus merged\_vertices}  (1-v.mat) \times v.f \times in\_edge(v).t
\]
where $v.mat = 1$ if artifact $v$ is materialized and $0$ otherwise, $v.f$ is the frequency of the artifact $v$, and $in\_edge(v).t$ returns the edges with destination $v$ and run-time $t$.
Merged vertices (represented by $merged\_vertices$) have no impact on $WC(G)$ since they carry no actual data content except for pointers to the original vertices.
This also ensures $in\_edge(v)$ contains at most one edge.
Intuitively, the weighted recreation cost computes the total amount of execution time required to recompute all the vertices while considering their frequencies.
Materialized artifacts incur no cost since they are stored.
Non-materialized artifacts incur a cost equal to the execution time of the proceeding operations multiplied by their frequency.
For example, in Figure \ref{fig-experiment-graph}, if we do not materialize $v_4$ and assuming it has a frequency of 2, we must consider both executions of the operation \textit{vectorizer.f\_t}  when computing the weighted cost.
Whereas, if $v_4$ is materialized, the \textit{vectorizer.f\_t} operation has no impact on the weighted recreation cost.

\textbf{Estimated Quality Function (EQ).} 
In order to define the estimated quality function, we need to define the followings first.
\[
M(G) = \{v \in V \mid v \text{ is a terminal model}\}
\]
is the set of all the terminal models in the experiment graph.
For every vertex $v$ in the graph, 
\[
M(v) = \{m \in M(G) \mid (v = m) \vee (v \text{ is connected to } m)\}
\]
is either $v$ itself, when $v$ is a terminal model, or the set of all terminal models to which $v$ is connected.
\[
potential(v) = \max\limits_{m \in M(v)} ( \alpha ^ {\mid path(v,m) \mid} \times quality(m) )
\]
is the potential of an artifact, where $quality(m)$ represents the quality of a terminal model measured by the evaluation function of the task and $\alpha \in [0,1]$ is the damping factor.
If $v$ itself is a model, then $potential(v) = quality(v)$.
When $v$ is not a model, the further $v$ is from a model artifact the smaller the damping factor multiplier becomes, which reduces the quality gained by materializing the artifact $v$.
Intuitively, a high potential artifact is an artifact which results in a high-quality terminal model with only a few operations.

Now, we define the estimated quality function as:
\[
EQ(G) =  \sum\limits_{v \in V}  v.mat \times potential(v)
\]

\textbf{Multi-Objective Optimization.}
Given the two functions, i.e., weighted recreation cost and estimated quality, we would like to find the optimal set of vertices to materialize which minimizes the weighted recreation cost function and maximizes the estimated quality function under limited storage size, $\mathcal{B}$ (for ease of representation, we instead try to minimize the inverse of the estimated quality function):
\begin{equation}
\begin{split}
& minimize(WC(G), \dfrac{1}{EQ(G)}), \\
& \text{subject to:} \sum\limits_{v \in V} v.mat \times v.s \leq \mathcal{B}
\end{split}
\end{equation}

Bhattacherjee et al. prove minimizing the recreation cost alone is an NP-Hard problem \cite{bhattacherjee2015principles}.
While there are different approximate strategies for solving multi-objective optimization problems \cite{coello2007evolutionary}, they are time-consuming, which renders them inappropriate to our setting, where new artifacts are constantly added to the graph.
Execution of every workload results in an update to the experiment graph, which in turn, requires a recomputation of the materialized set.
As a result, existing solutions to multi-objective optimization problems are not suitable for artifact materializations of the experiment graph.

\subsection{ML-Based Greedy Algorithm}\label{subsec-ml-based-materialization}
We propose a greedy heuristic-based algorithm for materializing the artifacts in the experiment graph which aims to minimize the weighted recreation cost function and maximize the estimated quality function.
Every task $T$ is associated with an experiment graph.
Each task also has a storage budget and runs a separate instance of the materialization algorithm.
For example, in Figure \ref{improved-use-case}, there are three separate experiment graphs for the competitions A, B, and C, each having a dedicated storage budget and running a separate materializer component.

\begin{algorithm}[h]
\caption{Artifacts-Materialization}\label{algorithm-materialization}
\begin{algorithmic}[1]
\Require  $G(V,E)=$ experiment graph, $\mathcal{B}=$ storage budget of task
\Ensure experiment graph with materialized vertices
\State $S= 0$ \Comment {current size of the materialized artifacts}
\For  {$v$ in roots(G)} \Comment{materialize all the root nodes}
	\If{$v.mat= 0$}
		\State $v.mat = 1$
		\State $S = S + v.s$
	\EndIf
\EndFor
\State $Q = $ empty priority queue
\For {$v$ in $V$}
	\If{$v.mat = 0$}
		\State $utility\_ratio = \dfrac{\text{utility}(G, v)}{v.s}$
		\State insert $v$ into $Q$ sorted by the utility\_ratio
	\EndIf
\EndFor
%\For {$v$ in $Q$}
%	\If {$S+v.s \leq \mathcal{B}$}
%		\State $v.mat = 1$
%		\State $S = S + v.s$
%	\Else
%		\State \textbf{break} 		
%	\EndIf
%\EndFor
\While{$Q$ is not empty}
\State $v =$ pick vertex with highest $utility\_ratio$ 
\If {$S+v.s \leq \mathcal{B}$}
\State $v.mat = 1$
\State $S = S + v.s$
\Else
\State \textbf{break} 		
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}
Algorithm \ref{algorithm-materialization} shows the details of our method for selecting the vertices to materialize.
% is the definition of root artifact clear? I defined it in Section 3.3 Workload Optimizer for Kaggle Use Cases
First, we start by materializing all the root artifacts.
This is essential as many of the feature engineering and model building operations are not invertible.
As a result, we cannot reconstruct the raw datasets if they are not materialized.
Then, for every non-materialized artifact, we compute the utility ratio, which is defined as the value of the $utility$ function of an artifact divided by its size.
Then, we start materializing all the artifacts, sorted by their utility ratio, until the storage budget is exhausted.
The utility function computes the goodness of an artifact with respect to its recreation cost, how often it is used downstream, and the estimated quality gained from the artifact:
\begin{equation}
\begin{split}
utility(G,v) = 	& \hldel{|pipelines(v)|} \times \\
								&	potential(v) \times \\
								& recreation\_cost(G,v)  
 \end{split}
\end{equation}
where $pipelines(v)$ is the set of \hldel{pipeline subgraphs} which $v$ belongs to and $|pipelines(v)|$ is the cardinality of the set, $potential(v)$ computes the potential quality of artifact $v$, and $recreation\_cost(G,v)$ indicates the weighted cost of recreating the artifact $v$ computed as:
\[
\text{recreation\_cost}(G,v) = v.f \times \sum\limits_{e \in \bigcup\limits_{v_{0}\in roots} path(G, v_{0}, v)} e.t\]
, i.e., executing all the operations from the root nodes to $v$ multiplied by the frequency of $v$.
Intuitively, we would like to materialize vertices which are more costly to recompute and have larger impacts on the overall quality of the experiment graph.
The impact of $|pipelines(v)|$ is more implicit.
Intuitively, an artifact with a high $|pipelines(v)|$ has appeared in several pipelines each leading to a different terminal model.
An example of such artifact is a clean and preprocessed dataset with high-quality features, where multiple users have utilized it to train different terminal models with different training algorithms and hyperparameters.
Therefore, in the presence of similar estimated quality, recreation cost, and size, we are prioritizing artifacts with higher $|pipelines(v)|$.

%\begin{figure}
%\begin{subfigure}{0.5\linewidth}
%\centering
%\input{../images/tikz/graph-mat-step-1.tex}
%\caption{Original Graph}
%\end{subfigure}%
%\begin{subfigure}{0.5\linewidth}
%\centering
%\input{../images/tikz/graph-mat-step-2.tex}
%\caption{Materialized Graph}
%\end{subfigure}
%\begin{subfigure}{\linewidth}
%\setlength\tabcolsep{3.5pt} % This is to ensure the table does not go out of bound
%\begin{tabular}{l | | >{\bfseries}r | r  |>{\bfseries}r | r | r | >{\bfseries}r | >{\bfseries}r | >{\bfseries}r |>{\bfseries}r }
%\hline
%\textbf{vertex} & $\boldsymbol{v_0}$ & $v_1$ & $\boldsymbol{v_2}$ & $v_3$ & $v_4$ & $\boldsymbol{v_5}$ & $\boldsymbol{v_6}$ & $\boldsymbol{v_7}$ &$\boldsymbol{v_8}$ \\
%\hline
%\textbf{size (MB)}    & 10 & 8 & 2 & 40 & 42 & 1 & 30 & 2   & 3        \\
%\textbf{$\boldsymbol{\rho}$ (s)} & ---   & 3 & 3 & 78 & 81 & 52 & 141 & 107 & 154	  \\
%\textbf{ratio}& ---   & 0.37 & 1.5 & 1.95 & 1.93 & 52 & 4.7 & 53.5 & 51.3	\\
%\hline
%\end{tabular}
%\caption{List of vertices, their sizes, recreation costs, and the cost over size ratio (Bold vertices are materialized).}
%\end{subfigure}
%\caption{Artifact materialization based on Algorithm \ref{algorithm-materialization} when storage capacity is 55 (MB)}
%\label{fig-materialization-example}
%\end{figure}
\subsection{Storage-Aware Materialization Algorithm}
Since many feature engineering operations only operate on one or a few columns of a dataset, the resulting artifact of a feature engineering may contain many of the columns of the input artifact.
As a result, after materialization, there are many duplicated columns across different artifacts.
To further reduce the storage cost, we implement a deduplication mechanism.
We assign a unique hash to every columns of the artifacts.
When executing an operation on an artifact, all the columns of the resulting artifact, except for the ones affected by the operation carry the same hash value.
When storing an artifact, the storage manager unit examines the hash of every column, and only stores the columns that do not exist in the storage unit.
The storage manager tracks the column hashes of all the artifacts in the experiment graph.
When a specific artifact is requested, the storage manager combines all the columns which belong to the artifact into a data frame and returns the data frame.
This results in a large decrease in the storage cost (e.g., for the same script of the Home Credit Default Risk Kaggle competition\footnote{https://www.kaggle.com/c/home-credit-default-risk} which generates 17 GB of artifacts, deduplication result in only 8 GB of storage).

% maybe a better name
\textbf{Greedy Meta-Algorithm.}
We propose a storage aware materialization meta-algorithm (Algorithm \ref{algorithm-compression-aware-materialization}) which iteratively invokes Algorithm \ref{algorithm-materialization} (Artifact-Materialization).
\hladd{We define a variable to represent the remaining budget (Line 1).
While the budget is not exhausted, we proceed as follows.
We extract the current set of materialized nodes from the graph (Line 3), then we apply the Artifact-Materialization algorithm using the remaining budget to compute new vertices for materialization.
If the Artifact-Materialization algorithm did not find any new vertices to materialize, we return the current graph (Line 6).
We compute the compressed size of the graph artifacts (Line 8), by invoking the deduplicate method of the storage manager which computes the size of graph artifacts after deduplication. 
Next, we update the required storage size of the remaining artifacts (Line 9).
For example, if the materialized artifact $v_1$ contains some of the columns of the non-materialized artifact $v_2$, then we only need to store the remaining columns of $v_2$ to fully materialize it.
Therefore, we update the size of $v_2$ to indicate the amount of storage it requires to fully materialize.
Finally, we compute the remaining budget by deducting the compressed size from the initial budget.

\begin{algorithm}[h]
\caption{Storage-Aware-Artifact-Materialization}\label{algorithm-compression-aware-materialization}
\begin{algorithmic}[1]
\Require  $G(V,E)=$ experiment graph, $\mathcal{B}=$ storage budget of task
\Ensure experiment graph with materialized vertices
\State $R = \mathcal{B}$ 
\While {$R > 0$}
	\State $prev\_mats = materialized\_nodes(G)$
	\State $G$  = \textit{Artifact-Materialization}($G, R$)
	\If  {$prev\_mats = materialized\_nodes(G)$}
		\State return $G$
	\EndIf
	\State $compressed\_size$ = $deduplicate(G)$
	\State $storage\_manager.update\_required\_size(G)$
	\State  $R = \mathcal{B} -  compressed\_size$
\EndWhile
\end{algorithmic}
\end{algorithm}
}
%TODO if existing algorithms produce good results, this can be a good follow up work and we do not need 
%\textbf{Fractional Greedy Algorithm.}
%\todo[inline]{I have some rough ideas one what we can do here, but need to work on it a bit more. We can find all the artifacts that have common columns, and give some sort of weight to artifacts who have the highest amount of columns that are shared between other artifacts. }