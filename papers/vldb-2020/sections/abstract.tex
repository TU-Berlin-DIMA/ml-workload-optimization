\begin{abstract}
\hladd{Collaborative data science platforms, such as Google Colaboratory and Kaggle, affected the way users solve machine learning tasks.
Instead of solving a task in isolation, users write their machine learning workloads and execute them on these platforms and share the workloads with others.
This enables new users to learn from and modify existing workloads, which will ultimately lead to more efficient solutions for solving the task.
As a result, many executed operations in collaborative platforms, especially the data preprocessing operations, are repeatedly executed by different users.
However, current collaborative platforms only store scripts, raw training datasets, and in some cases the final machine learning models and ignore any intermediate artifacts such as preprocessed and cleaned data.
As a result, the platforms miss an opportunity for improving the execution time of future machine learning workloads.
%First, storing all the artifacts, , requires massive amounts of storage.
%As a result, only some of the artifacts such as raw data and machine learning models are stored and users must re-execute the scripts and operations to reconstruct the desired artifact.
%\hl{Second, even if all the artifacts are stored, manually finding desired artifacts is a time-consuming process.} % Tilmann: this is only a conclusion of the first problem, better say finding the artifacts is slow or similar... *Behrouz: I wanted to use the word manually (or something equal) to contrast between the current 'manual' querying or reading the scripts vs our way of reuse or warmstarting that doesn't require user's intervention.

The contributions of this paper are two-fold.
First, we utilize a graph to store the artifacts and operations of machine learning workloads as vertices and edges, which we refer to as the experiment graph.
Since storing all the artifacts is not feasible, as the size of the intermediate artifacts may grow exponentially, we propose two algorithms for materializing the artifacts with high likelihoods of future reuse.
The algorithms consider several metrics, such as access frequency, size of the artifact, and quality of machine learning models to decide what artifacts to materialize.
Second, using the experiment graph, we propose a novel approach for quickly finding artifacts to reuse or to warmstart model training operations in future workloads.}
%TODO one sentence the impact

%Machine learning workloads have varying characteristics.
%Some involve a large user base where a combination of experts and novice users are trying to design machine learning pipelines and execute them on specific tasks, such as online education, data science challenges.
%Some involve fewer users, typically experts, working together to solve a task.
%For example, a team of data scientists in a company trying to design a recommender system based on the available training data.
%Both workloads are interactive and require many iterations to improve the solution.
%In such scenarios, communication between the users involved is not optimal and as a result, many repetitions may occur.
%Repetitions can be of the form of repeated data preprocessing, hyperparameter search, and model training.
%
%Using experiment databases, where a log of previous machine learning experiments is stored, we propose a solution that utilizes the information in the experiment database to improve the process of design and execution of machine learning workloads.
%Specifically, we utilize the logs in the experiment databases to reduce the data processing and model training time by caching and reusing the preprocessed data and trained models.
%Moreover, we leverage the logs to enhance the hyperparameter optimization process and provide the users (both expert and novice) with better hyperparameter settings in a shorter amount of time.
\end{abstract}