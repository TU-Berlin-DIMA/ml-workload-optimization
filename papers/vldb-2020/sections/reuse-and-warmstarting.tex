\hladd{
\section{Reuse and Warmstarting Optimizations}\label{sec-reuse-and-warmstarting}
With the experiment graph constructed and materialized, we can look for optimization opportunities for feature engineering and model training operations.
In this section, we propose two optimizations, namely, \textit{Reuse} and \textit{Warmstarting}.
Both optimizations are part of the the remote optimization step of Figure \ref{system-workflow} and can greatly reduce the execution time of a workload.

\todo[inline]{Formulate Warmstarting and add a subsection.}
%Furthermore, if the workload execution subgraph contains a model training operation, the remote optimizer looks for similar models in the experiment graph, and sets the initial weight parameters of the model in the workload execution subgraph to the model in experiment graph before executing the training operation.
%This process, which is called model warmstarting, can greatly reduce the training time of machine learning models.

%Furthermore, We also look for warmstarting opportunities for any model training operations.
%The result of Step 3 is another execution subgraph with (possibly) more edges pruned.
%Finaly, we compute the execution plan using the the optimized execution subgraph (Step 4, Figure \ref{system-workflow}).

\subsection{Reuse}
After computing the workload execution subgraph (the result of step 2, Figure  \ref{system-workflow}), the remote optimizer tries to find any matching vertex in the experiment graph, and transfer them if they are materialized.
Since during the materialization process, we ensure the recomputation cost of a materialized vertex outweighs its transfer cost, we can safely transfer the materialized vertex to the machine executing the workload and guarantee the total execution time will improve.

The vertex hashing procedure in Section \ref{sec-ml-workloads} indicates that two vertexes from two different graphs are the same if they have the same id (hash value).
Since id of a vertex captures the exact list of proceeding operations (edges) from the root vertices, if two vertices have the same hash, they also have the same list of proceeding operations.
Therefore, to find matching vertices in the workload execution subgraph, remote optimizer queries their ids in the experiment graph.
However, in cases where the workload execution subgraph is very large and querying the experiment graph incurs a penalty (e.g., the experiment graph is stored on a remote machine), we must minimize the number of queries to the experiment graph.
In this section, we present three approaches for searching in the experiment graph, namely, BottomUp reuse, TopDown reuse, and Hybrid reuse.

\textbf{BottomUp reuse.}
The process of BottomUp reuse is similar to how we construct the execution subgraph.
Algorithm \ref{algorithm-bottomup} shows the details of the BottomUp reuse.
$v_t$ represent the terminal vertex whose data is requested.
The BottomUp reuse utilizes the following early-stopping principle. 
If a vertex from the workload execution subgraph ($SG$) exist in the experiment graph ($EG$) and is materialized, then we skip traversing its parents and add the vertex to the set of materialized vertices.
Therefore, in Line 1, if $v_t$ is materialized, we return it as the result and in Lines 8-11, we only continue the search if the vertex ($v$) is not materialized ($is\_mat(EG, v)$ function returns true if $v$ is materialized in $EG$).
For any other vertex which is not materialized, we recursively examine its parents until there are no more vertices left (i.e., we reach the root vertices).
\begin{algorithm}[h]
\caption{BottomUp Reuse}\label{algorithm-bottomup}
\begin{algorithmic}[1]
\Require $v_t$: terminal vertex, $SG$: workload execution subgraph, $EG$: experiment graph 
\Ensure set of materialized vertices $\mathcal{M}$ 
\If {$is\_mat(EG, v_t)$}
	\State return $\{v_t\}$
\EndIf
\State $Q \coloneqq  Queue(vt)$  
\State $\mathcal{M} \coloneqq \emptyset$
\While {$Q.not\_empty()$}
	\State $cur  \coloneqq  Q.pop()$
	\For {$v \in predecessors (SG, cur)$}
		\If {$is\_mat(EG, v)$}
			\State	$\mathcal{M}.append(v)$
		\Else
			\State $q.add(v)$
		\EndIf
	\EndFor
\EndWhile
\State return $\mathcal{M}$
\end{algorithmic}
\end{algorithm}
BottomUp reuse performs well when the terminal vertex or vertices close to the terminal are materialized in the experiment graph.
However, in extreme cases, where none of the vertices of the workload execution subgraph are in the experiment graph, BottomUp reuse still has to examine all the vertices.
Therefore, BottomUp reuse has a complexity of $\mathcal{O}(|SG|)$, where $|SG|$ is the number of vertices in the workload execution workload subgraph, which means the remote optimizer may make upto $|SG|$ calls to the experiment graph.

\textbf{TopDown reuse.}
Contrary to the BottomUp reuse, in TopDown, we start traversing the workload execution subgraph, from its root vertices.

\begin{algorithm}[h]
\caption{TopDown Reuse}\label{algorithm-topdown}
\begin{algorithmic}[1]
\Require $v_t$: terminal vertex, $SG$: workload execution subgraph, $EG$: experiment graph 
\Ensure set of materialized vertices $\mathcal{M}$ 
\State $R=roots(SG)$
\State $\mathcal{M} \coloneqq \emptyset$
\For {$r \in R$}
	\If {$r \notin EG$}
		\State continue \Comment{skip this iteration}
	\EndIf
	\State $Q \coloneqq  Queue(r)$  
	\If {$is\_mat(EG, r)$}
		 \State	 $\mathcal{M}.append(r)$
	\EndIf
		\While {$Q.not\_empty()$}
			\State $cur \coloneqq  Q.pop()$
			\For {$v \in children (SG, cur)$}
				\If {$is\_mat(EG, v)$}
					\State	$\mathcal{M}.append(v)$
				\EndIf
				\If {$v \in EG$} 
					\State $q.add(v)$
				\EndIf
			\EndFor
		\EndWhile
\EndFor
\State return $\mathcal{M}$
\end{algorithmic}
\end{algorithm}
Algorithm \ref{algorithm-topdown} shows the details of the TopDown reuse algorithm.
The TopDown reuse operates on the following early-stopping principle.
If a vertex from the workload execution subgraph ($SG$) does not exist in the experiment graph ($EG$), we skip the traversal of its children.
This follows from the graph construction procedure, where each vertex is derived from its parents, as a result, it is impossible for a child vertex to exist in a graph where its parents do not.
Therefore, in Lines 4 and 14, we continue the traversal for a vertex if and only if the vertex is in the experiment graph.
Since a workload execution graph may have multiple root vertices, the TopDown reuse algorithm first finds all the root vertices (Line 1).
Then, for every root vertex, TopDown examines all of its children and add them to the set of materialized vertices if they are in the experiment graph and are materialized ($is\_mat$ returns true).
Unlike the BottomUp approach, when a node is materialized, we cannot stop the traversal, since a materialized vertex may also have materialized children.

TopDown performs well when only the vertices close to the root are materialized.
If the current workload operates on a completely new root (which never appeared in the experiment graph) or the workload contains early data exploration which never appeared in experiment graph, TopDown reuse will quickly stop the search process.
However, in extreme cases, where a the terminal vertex is materialized in the experiment graph (i.e., a workload is re-executed), then TopDown must traverse the entire workload execution subgraph.
Therefore, TopDown reuse also has a complexity of $\mathcal{O}(|SG|)$ and makes at most $|SG|$ calls to the experiment graph.

\textbf{Hybrid reuse.}
Both BottomUp and TopDown reuse perform well in specific scenarios. 
However, neither of them can adapt to the different characteristics of a workload (e.g., how similar a work is to the previous workloads or how large the execution graph is).
We devise a dynamic reuse approach, called Hybrid reuse, which adapts to the current workload.
Algorithm \ref{algorithm-hybrid} shows the process of Hybrid reuse.
Hybrid reuse combines the two early-stopping principles utilized in TopDown and BottomUp reuse algorithms to prune as many vertices without traversing them.
The two methods $R\_BFS(v, G, n\_visits)$ and $F\_BFS(v, G, n\_visits)$, perform a graph breadth-first-search traversal starting from vertex $v$ and return the vertex after $n\_visits$ visits.
$R\_BFS(v, G, n\_visits)$ traverses backward (visiting the parents vertices) and $F\_BFS(v, G, n\_visits)$ traverses from top to bottom (visiting children vertices).
Unlike BottomUp and TopDown reuse, $R\_BFS$ and $F\_BFS$, do not make calls to the experiment graph.

Hyrbid reuse first traverses the vertices of the execution subgraph in reverse breadth-first-search order starting from the terminal vertex by calling the $R\_BFS$ (Line 4) and returns the vertex after it visits half of the vertices of the graph.
Hybrid reuse then iteratively prunes half of the remaining vertices and adds any materialized vertex from the experiment graph to the set of materialized vertices.
In Line 7, if the vertex is not in the experiment graph, using the TopDown early-stopping principle, we prune the bottom half of the graph and search in the top half (i.e., $R\_BFS$ on Line 8).
If the vertex is materialized (Line 16), the algorithm first adds it to the list of materialized vertices, then uses the BottomUp early-stopping principle to prune the top half the graph (i.e., $F\_BFS$ on Line 18).
When the vertex is in the experiment graph but it is not materialized (Line 9), the algorithm cannot safely prune the graph, as a materialized vertex can appear both in the top half or the bottom half.
Hybrid reuse utilizes the following heuristic to decide which half of the graph to prune.
First, it computes the average value of the utility function for the parents and children of the vertex (Lines 10 and 11).
If the average utility of the parents is larger, then it prunes the bottom half, otherwise, it prunes the top half.
The intuition behind this heuristic is the following.
The materialization algorithm always materializes vertices based on their utility value.
If the utility of parents of a vertex is larger than the utility of its children, then there is a higher probability that more vertices in the top half of the graph are materialized.


\begin{algorithm}[h]
\caption{Hybrid Reuse}\label{algorithm-hybrid}
\begin{algorithmic}[1]
\Require terminal vertex $v_t$, workload execution subgraph  $SG$, experiment graph $EG$ 
\Ensure set of materialized vertices $\mathcal{M}$ 
\State $N \coloneqq size(SG)$
\State $step \coloneqq 2$
\State $\mathcal{M} \coloneqq \emptyset$
\State $v \coloneqq R\_BFS(v_t, SG, N/step)$
\While {$step \leq log(N) $}
		\State $step = step \times 2$
		\If {$v \notin EG$}
				\State $v \coloneqq R\_BFS(v, SG, N/step)$
		\ElsIf {$v \in EG  \land \neg is\_mat(EG,v)$}
				\State $prev \coloneqq avg (\mathcal{U}(parents(EG, v))$
				\State $next \coloneqq avg (\mathcal{U}(children(EG, v))$
				\If{$prev \geq next $}
						 \State $v \coloneqq R\_BFS(v, SG, N/step)$
				\Else
					\State $v \coloneqq F\_BFS(v, SG, N/step)$
				\EndIf
		\ElsIf {$is\_math(EG, v)$}
				\State $\mathcal{M}.append(v)$
				\State $v \coloneqq F\_BFS(v, SG, N/step)$
		\EndIf
\EndWhile
\State return $\mathcal{M}$
\end{algorithmic}
\end{algorithm}
The advantage of the Hybrid reuse is that it requires at most $log(|SG|)$ calls to the experiment graph since in every iteration we are pruning half of the graph.
This is in contrast to the TopDown and BottomUp reuses, wherein certain scenarios they may require $|SG|$ calls to the experiment graph.
}

%We devise a strategy to detect overlapping operations in both the current workload and the experiment graph.
%If an operation already exists in the experiment graph, we directly access the resulting artifact in the experiment graph instead of executing the operation.
%Before executing a workload, we first transform it into its graph representation, which results in a directed graph, called $\mathcal{WG}$.
%Then, we traverse the experiment graph starting from the root node, using the edges of the workload graph.
%The result of the traversal is a subgraph of the experiment graph, which we refer to as $\mathcal{SG}$.
%The subgraph contains all the vertices and edges that exist in both the experiment graph and the workload graph.
%For every path in $\mathcal{SG}$ which originates at the root node, we return the furthest materialized vertex from the root node as the result and skip all the intermediate operations.
%
%To demonstrate with an example, let us assume a user has executed the code in Listing \ref{listing-experiment-graph} and Figure \ref{fig-experiment-graph}a shows the current experiment graph, where the set of materialized vertices is $\mathcal{MV} = \{v_0, v_2, v_3\}$.
%A new user submits the code in Listing \ref{listing-reuse}.
%Figure \ref{fig-reuse}a shows the workload graph ($\mathcal{WG}$) of the code in Listing \ref{listing-reuse}.
%As described in the Reuse procedure, we traverse the experiment graph, starting at $v_0$, with the edges of $\mathcal{WG}$ which results in the common subgraph, $\mathcal{SG}$, in Figure \ref{fig-reuse}b.
%In the subgraph $\mathcal{SG}$, the furthest vertices in each path originating at $v_0$ are $v_5$ and $v_3$.
%The Reuse procedure selects $v_2$ as the candidate since $v_5$ is not materialized.
%The Reuse procedure results in an optimized graph which skips operations p2 and p3 (Figure \ref{fig-reuse}c).
%% Tilmann: Can you elaborate the benefits? Also, what do the new reused nodes help? There does not seem to be a path to the final result from them. 
%
%\begin{lstlisting}[language=Python, firstnumber = 8,caption=New workload script (Imports are omitted),captionpos=b,label = {listing-reuse}]
%train = pd.read_csv('../input/train.csv') 
%selector =  SelectKBest(k=2)
%top_features = selector.fit_transform(train[['ts','u_id','price']], 
%				      train['y'])
%model = svm.SVC()
%model.fit(top_features, train['y'])
%\end{lstlisting}
%
%\begin{figure}
%\captionsetup[subfigure]{justification=centering}
%\begin{subfigure}[t]{0.33\linewidth}
%\centering
%\includegraphics{../images/tikz-standalone/reuse-example-wg}
%\caption{Workload Graph ($\mathcal{WG}$)}
%\end{subfigure}%
%\begin{subfigure}[t]{0.33\linewidth}
%\centering
%\includegraphics{../images/tikz-standalone/reuse-example-sg}
%\caption{Common Subgraph ($\mathcal{SG}$)}
%\end{subfigure}%
%\begin{subfigure}[t]{0.33\linewidth}
%\centering
%\includegraphics{../images/tikz-standalone/reuse-example-final}
%\caption{Optimized Graph}
%\end{subfigure}
%\begin{subfigure}[t]{\linewidth}
%\centering
%\includegraphics{../images/tikz-standalone/reuse-example-final-eg}
%\caption{Experiment graph after executing the workload (new nodes are highlighted)}
%\end{subfigure}
%\caption{Steps in the Reuse optimizations}
%\label{fig-reuse}
%\end{figure}
%
%\subsection{Warmstart Model Training Operations}
%Model training operations include extra hyperparameters that must be set before the training procedure begins.
%Two training operations on the same data artifact using the same training algorithm could potentially have very different results based on the values of the hyperparameters.
%Therefore, we cannot apply the Reuse optimization in cases the hyperparameters of model training operations are different.
%Instead, we apply the \textit{Warmstarting} optimization.
%We first need to describe the concept of model groups.
%For a vertex $v$, the model group $\mathcal{MG}_v$ refers to the set of all the machine learning models and their hyperparameters that are trained on $v$.
%If a workload contains the model training operation $e_{m}$ on the vertex $v_{m}$, before executing the workload, we proceed as follows.
%First, using the same traversal strategy as explained in the Reuse procedure, we look for $v_{m}$ in the experiment graph.
%If operation $e_{m}$ also exists in the experiment graph, then the Reuse procedure will return the resulting node as the result.
%However, if $e_{m}$ does not exist in the experiment graph, then we find the model group $\mathcal{MG}_{v_m}$.
%If $\mathcal{MG}_{v_m}$ is not empty, we warmstart the operation $e_{m}$ with the best performing model from the model group.
%% Tilmann: Does this always make sense? or do we get stuck in local minima this way?
%
%To demonstrate with an example, after executing both the code in Listings \ref{listing-experiment-graph} and \ref{listing-reuse}, a new users submits the same code as in Listing \ref{listing-reuse}, with a different model hyperparameters (Line 12) shown in Listing \ref{listing-warmstarting}.
%\begin{lstlisting}[language=Python, firstnumber=12, caption= Workload with different hyperparameters,captionpos=b,label = {listing-warmstarting}]
%model = svm.SVC(C=0.1)
%\end{lstlisting}
%Since both models (svm.SVC(C=0.1) and svm.SVC()) are trained on the same vertex ($v_x$ in Figure \ref{fig-reuse}), the Warmstarting procedure selects the existing model node ($v_y$ in Figure \ref{fig-reuse}) to warmstart the training process.
%
%Warmstarting can greatly reduce the total training time.
%However, the type of the machine learning model and the termination criteria play important roles in determining the effect of the warmstarting optimization.
%In the experiment section, we evaluate the effect of warmstarting on different types of models with different termination criteria.
%
%\subsubsection{Augmenting the experiment graph}
%When we utilize warmstarting, we extend the experiment graph with a merge operation which merges the dataset and the candidate model for warmstarting.
%The actual training operation is then applied to the merged node.
%As a result, we can keep track of the models that are utilized in warmstarting the training of other models which ensures reproducibility.
%
%Figure \ref{fig-warmstarting} shows the experiment graph after execution of the script from Listing \ref{listing-warmstarting}.
%The training operation, f1, is applied to the new vertex $v_n$, which is the result of merging the data artifact $v_x$ and the model $v_y$.
%The training operation f1 has a different hash from the existing operations since the hyperparameters are different.
%
%\begin{figure}[t]
%\centering
%\includegraphics{../images/tikz-standalone/warmstarting-example-final-eg}
%\caption{Experiment graph after warmstarting} % Tilmann: Again, if v9 is the result, how does vx-vz contribute to this?
%\label{fig-warmstarting}
%\end{figure}

%For iterative training algorithms that are minimizing a loss function, there are two termination criteria, namely, the convergence tolerance and the number of iterations.
%
%\subsubsection{Convergence tolerance termination criteria}
%When the termination criteria of the model training operation in the workload is set to a specific convergence tolerance value, two scenarios may occur.
%In the first scenario, an existing trained model in the experiment graph has already reached the convergence tolerance value.
%In this scenario, we expect a large improvement in the training time as the training procedure in the workload will immediately converge.
%In the second scenario, no model in the experiment graph has reached the convergence tolerance value.
%In this case, we warmstart the model in the workload, to the model in the experiment graph with the highest attained quality.
%Therefore, we ensure the training procedure will converge faster.

%\subsubsection{Augmenting the experiment graph}
%Once the training procedure is finished, we augment the experiment graph with an edge and node representing the new model building operation and resulting model, respectively.
%\todo[inline]{We may need a special edge so that we know the training operation was not run from scratch and is the result of warmstarting.}

%\subsubsection{Partial Warmstarting Optimization For Model Training Operations}
%A common approach in machine learning workloads is to repeatedly select a different subset of features or create new features from the existing ones and train models on the new features.
%As a result, many model training operations operate on overlapping or different set of features.
%In the partial warmstarting optimization, we aim to improve the training time (and the quality) by warmstarting only the features that exist in the experiment database.

%\subsection{Reuse Optimization for Model Building Operations}
%Reuse for model building operations is more complicated.
%There are two types of reuse opportunities in the model building operations.
%
%\subsubsection{Exact Reuse}\label{sub-sub-exact-reuse}
%For non-user-defined aggregation operations, we follow the same procedure as the feature engineering processes.
%When the corresponding edge in the experiment graph has the same vertex and (aggregation) operation type, we reuse the result of the operation directly.
%We can also reuse the existing model training operation, if the input columns, algorithm, and all the hyper-parameters are the same.
%
%\subsubsection{Model parameter and hyper parameter warmstarting}\label{sub-sub-model-reuse}
%For the model training operations, 3 scenarios can occur.
%In the \textit{first scenario}, the training algorithm used for training the model has never been used before, therefore no meta-data about it exists in the experiment graph.
%In this scenario, no optimization is possible and the model training operation has to be executed completely.
%In the \textit{second scenario}, the training algorithm and the input columns to the model already exist in the experiment graph, but the specific hyperparameter setting does not.
%In this scenario, we can warmstart the model using the parameters from the corresponding node in the experiment graph.
%This reduces the training time as the model \hl{may} converge faster.
%\todo[inline]{This requires experiment and some math ?}
%In the \textit{third scenario}, the training algorithm and the hyperparameters are the same, but all the input columns do not exist in the corresponding node in the experiment graph.
%In this scenario, we provide partial warmstarting.
%In partial warmstarting, the model parameters corresponding to the columns of the input data that already exist in the experiment graph are warmstarted, and the rest of the parameters are randomly initialized.
%\todo[inline]{This requires experiment and some math ?}


%\subsection{Materialization of Grid Search}
%\todo[inline]{Incomplete}
%In order to analyze whether or not we should materialize parts of the grid search, we first have to unpack it, and compare it with other grid search.
%Then, similar to Section \ref{sub-sec-materialization-of-transformed-data}, we materialize the parts that are executed frequently.
%
%%\subsection{Guided Grid-Search}
%%\todo[inline]{just an idea}
%%By extracting correlation between different parameters and the model quality we can provided a guided grid search, where we can provide some estimate or show the effects of a hyperparameter range on the model quality