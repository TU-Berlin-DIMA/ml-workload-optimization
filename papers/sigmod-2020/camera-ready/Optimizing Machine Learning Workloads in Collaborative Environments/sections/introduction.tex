\section{Introduction} \label{sec-introduction}
% Opening
Machine learning (ML) plays an essential role in industry and academia. 
Developing effective ML applications requires knowledge in statistics, big data, and ML systems as well as domain expertise.
Therefore, ML application development is not an individual effort and requires collaborations among different users.
Recent efforts attempt to enable easy collaboration among users.
Platforms such as AzureML \cite{team2016azureml} and Kaggle \cite{kagglewebsite} provide a collaborative environment where users share their scripts and results using Jupyter notebooks \cite{Kluyver:2016aa}.
Other platforms such as OpenML \cite{vanschoren2014openml} and ModelDB \cite{vartak2016m} enable collaboration by storing ML pipelines, hyperparameters, models, and evaluation results in experiment databases \cite{Vanschoren2012}.

% P
The collaborative platforms typically act as execution engines for ML workloads, i.e., ML scripts.
Some platforms also store artifacts.
Artifacts refer to raw or intermediate datasets or ML models.
By automatically exploiting the stored artifacts, the collaborative platforms improve the execution of future workloads by skipping redundant operations.
However, the existing collaborative platforms lack automatic management of the stored artifacts and require the users to manually search through the artifacts and incorporate them into their workloads.
In the current collaborative environments, we identify two challenges that prohibit the platforms from automatically utilizing the existing artifacts.
First, the quantity and size of the artifacts are large, which renders their storage unfeasible.
For example, we observe that three popular ML scripts in a Kaggle competition generate up to 125 GB of artifacts.
Second, ML workloads have a complex structure; thus, automatically finding artifacts for reuse is challenging.

% S
We propose a solution for optimizing the execution of ML workloads, which addresses these two challenges.
Our solution stores the artifacts with a high likelihood of reappearing in the future workloads.
Furthermore, our solution organizes the ML artifacts and offers a linear-time reuse algorithm.

We model an ML workload as a directed acyclic graph (DAG), where vertices represent the artifacts and edges represent the operations in the workload.
An artifact comprises of two components: meta-data and content.
Meta-data refers to the column names of a dataframe, hyperparameters of a model, and evaluation score of a model on a testing dataset.
Content refers to the actual data inside a dataframe or the weight vector of an ML model.
We refer to the union of all the workload DAGs as the \textit{Experiment Graph} (EG), which is available to all the users in the collaborative environment.
The size of the artifact meta-data is small.
Thus, EG stores the meta-data of all the artifacts.
The content of the artifacts is typically large.
Therefore, there are two scenarios where storing the content of the artifacts in EG is not suitable, i.e., storage capacity is limited and recomputing an artifact is faster than storing/retrieving the artifact.
We propose two novel algorithms for materializing the content of the artifacts given a storage budget.
Our materialization algorithms utilize several metrics such as the size, recreation cost, access frequency, operation run-time, and the score of the ML models to decide what artifacts to store.
To the best of our knowledge, this is the first work that considers the score of ML models in the materialization decision.

To optimize the execution of the incoming ML workloads, we propose a linear-time reuse algorithm that decides whether to retrieve or recompute an artifact.
Our reuse algorithm receives a workload DAG and generates an optimal execution plan that minimizes the total execution cost, i.e., the sum of the retrieval and the computation costs.
However, for some ML model artifacts, due to the stochasticity of the training operations and differences in hyperparameters, we cannot reuse an existing model.
Instead, we warmstart such training operations with a model artifact from EG.
Model warmstarting increases the convergence rate resulting in faster execution time of the model training operations.

In summary, we make the following contributions. (1) We propose a system to optimize the execution of ML workloads in collaborative environments. (2) We present Experiment Graph, a collection of the artifacts and operations of the ML workloads. (3) We propose novel algorithms for materializing the artifacts based on their likelihood of future reuse. The algorithms consider run-time, size, and the score of ML models. (4) We propose a linear-time reuse algorithm for generating optimal execution plans for the ML workloads.

% \todo{includes the new chapter}
The rest of this paper is organized as follows.
In Section \ref{sec-background}, we provide some background information.
We introduce our collaborative workload optimizer in Section \ref{sec-ml-workloads}.
In Section \ref{representation-and-api}, we discuss our data model and programming API.
In Sections \ref{sec-materialization} and \ref{sec-reuse-and-warmstarting}, we introduce the materialization and reuse algorithms.
In Section \ref{sec-evaluation}, we present our evaluations.
In Section \ref{sec-related-work}, we discuss the related work.
Finally, we conclude this work in Section \ref{sec-conclusion}.