\section{Introduction} \label{sec-introduction}
% Opening
Machine learning (ML) plays an important role in business, industry, and academia. 
Developing effective ML applications requires knowledge in statistics, big data, and ML systems as well as domain expertise.
Therefore, ML application development is rarely an individual effort and requires collaborations between different ML users.
To this end, recent efforts attempt to enable easy collaboration between ML users.
Platforms such as AzureML \cite{team2016azureml}, Kaggle \cite{kagglewebsite}, and Google Colabratory \cite{googlecolab} provide a collaborative environment where users share their scripts and results in Jupyter notebooks \cite{Kluyver:2016aa}.
Other platforms such as OpenML \cite{vanschoren2014openml} and ModelDB \cite{vartak2016m} enable collaboration by storing machine pipelines, hyperparameters, models, and evaluation results in databases, commonly referred to as experiment databases \cite{Vanschoren2012}.

% P
The collaborative platforms act as repositories of ML artifacts and execution platforms for ML workloads, i.e., scripts or pipelines.
ML artifacts refer to raw or intermediate datasets or ML models.
By automatically exploiting the stored ML artifacts, the collaborative platforms can improve the performance of future workloads by skipping redundant operations and by warmstarting the model training operations using existing models.
However, the existing collaborative platforms ignore the stored artifacts and require the users to manually search through the stored ML artifacts and incorporate them into their workloads.
In the current collaborative environments, we identify two challenges which prohibit the platforms from automatically utilizing the existing ML artifacts.
First, the quantity and size of the generated artifacts are large, which renders their storage not feasible.
Second, naively searching through all the stored artifacts for reuse incurs a substantial overhead.

% S
We propose a solution which addresses these two challenges.
Our solution only stores the ML artifacts with a high likelihood of reappearing in the future workloads.
Furthermore, our solution organizes the ML artifacts and offers a fast reuse procedure.

We model an ML workload with a directed acyclic graph (DAG), where vertices represent the ML artifacts and edges represent the operations in the workload.
%Each ML artifact is uniquely identified using the sequence of operations that generated it.
An ML artifact comprises of two components: meta-data and underlying content.
Meta-data refers to the column names of a dataframe, hyperparameters of a model, and evaluation score of a model on a testing dataset.
Underlying content refers to the actual data inside a dataframe or the weight vector of a machine learning model.
We refer to the union of all the workload DAGs as the \textit{Experiment Graph}.
The Experiment Graph is itself a DAG which contains all the vertices (ML artifacts) and edges (operations) of the past workload graphs.
The size of the artifact meta-data is small.
Thus, Experiment Graph stores the meta-data of all the artifacts.
However, there are two scenarios where storing the underlying data in the Experiment Graph is not suitable, i.e., storage capacity is limited and when recreating an artifact is faster than storage/retrieval of the artifact.
We propose two algorithms for materializing the underlying data of the artifacts given a storage budget.
Our materialization algorithms utilize several statistics such as the size, recreation cost, access frequency, operation run-time, and the score of the machine learning models to decide what artifacts to store.

To optimize the execution of incoming ML workloads, we propose an efficient reuse algorithm, which receives the DAG representation of the workloads and performs the following steps.
First, the algorithm queries the Experiment Graph for the list of the materialized artifacts belonging to the workload DAG.
Then, for each materialized artifact, the reuse algorithm decides whether to retrieve the artifact from the Experiment Graph or to recompute it locally using the other artifacts inside the workload DAG.
Our reuse algorithm has a linear time complexity and guarantees an optimal execution plan which minimizes the total execution cost, i.e., the sum of the retrieval and the computation costs.
For some ML model artifacts, due to the stochasticity of the training operations, we cannot always directly reuse an existing model from the Experiment Graph.
Instead, we warmstart the training operation of the workload DAG with a materialized model artifact from the Experiment Graph.
\todo[inline]{In our reuse algorithm, we assume a node in the graph either has load cost (retrieving from EG) or computation cost but not both. However, for warmstarting, we have the load cost + "reduced" computation cost (due to warmstarting, training would be faster than without warmstarting). Should we address this in the reuse algorithm or leave it for future work? This could potentially be an interesting contribution, but I think we do not have enough time to come up with a solution.}
Model warmstarting increases the convergence rate resulting in faster execution time of the model training operations.

In summary, we make the following contributions:
\begin{itemize}
\item We propose a system for optimizing the execution of machine learning workloads in collaborative environments.
\item We present Experiment Graph, a graph representation of the artifacts and operations of the ML workloads.
\item We propose algorithms for materializing the artifacts in the Experiment Graph under limited storage capacity.
\item We propose efficient reuse algorithm for generating optimal execution plan of the incoming ML workloads.
\end{itemize}

The rest of this paper is organized as follows.
In Section \ref{sec-background}, we provide some background information and show an example.
We introduce our proposed collaborative workload optimizer in Section \ref{sec-ml-workloads}.
In Sections \ref{sec-materialization} and \ref{sec-reuse-and-warmstarting}, we introduce the artifacts materialization algorithms, reuse strategy, and the warmstarting technique. 
In Section \ref{sec-evaluation}, we show the result of our evaluations.
In Section \ref{sec-background}, we discuss the related work and finally, we conclude this work in Section \ref{sec-conclusion}.