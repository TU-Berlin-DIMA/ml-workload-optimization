\section{Background} \label{sec-background}
In this section, we first define the required terms referring to different entities in collaborative environments which we use throughout the paper.
Next, we discuss a use case based on a real collaborative environment.

\subsection{Collaborative Environment for Data Science}
A typical collaborative environment consists of a client and server space.
Users write a script to fetch datasets from the server, analyze the data, and train machine learning models.
The client is responsible for executing the script.
Although the client can be a single machine, users typically utilize Jupyter notebooks  \cite{Kluyver:2016aa} to write and execute their scripts in isolated containers \cite{merkel2014docker} within the server itself \cite{kagglewebsite, googlecolab, netflix-notebook}.
Users can publish the results and the scripts on the server for other users to see.
Isolated execution environments enable collaborative environments to better allocate resources for running scripts.

\subsection{Use Case}\label{subsec-motivational-example}
Kaggle is a collaborative environment that enables users and organizations to publish datasets and organize machine learning competitions.
In every competition, the organizer defines a task.
Users submit their solutions to the task as ML scripts.
Such a collaborative environment utilizes docker containers, called kernels, to execute user workloads.
If the workload produces machine learning model artifacts, the users can choose to submit them and attain a score of how good their model performs on a test dataset.

For example, let's consider the competition \textit{Home Credit Default Risk}\footnote{https://www.kaggle.com/c/home-credit-default-risk/} of Kaggle.
The task is to train a classification model to predict whether a client can repay their loans.
There are a total of 9 datasets, 8 for training and 1 for evaluation, with a total size of 2.5 GB.
The goal of the submitted workloads is to train a machine learning model that maximizes the area under the ROC curve, which measures how well a classifier works.
Three of the most popular submitted workloads are copied and edited by different users more than 7000 times\footnote{Notebook titles are: ''Start Here: A Gentle Introduction'' and ''Introduction to Manual Feature Engineering'' part 1 and part 2}.
The three workloads produce 100s of intermediate dataset artifacts and several machine learning model artifacts with a total size of 125 GB.
The execution time of each workload is between 400 to 1000 seconds.
Kaggle does not provide any information on the number of workload executions.
However, the number of users who copied these workloads indicates the potentially large number of executions, i.e., at least 7000 times.

Kaggle does not store the artifacts, nor does it offer any automatic reuse capabilities.
Therefore, every time a user executes these workloads (or a modified version of them), Kaggle runs them from scratch.
Our system, which stores the artifacts and reuses them later, can save hundreds of hours of execution time only for the three workloads in this use case, which benefits such a collaborative environment by reducing the amount of required resource and operation cost.
In the next sections, we show how we selectively store artifacts, given a storage budget, and how we quickly find the relevant artifacts for reuse.