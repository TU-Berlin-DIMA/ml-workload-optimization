\section{Background} \label{sec-background}
In this section, we first define the required terms referring to different entities in collaborative data science platforms which we use throughout the paper.
Next, we discuss a use case based on a real collaborative data science platform, which we will use throughout the paper.

\subsection{Preliminaries and Definitions}
\textbf{ML Task.} 
To promote collaboration, the collaborative data science platforms must define the task which the users should solve.
A task specifies the requirements and the goal of the machine learning solutions.
An ML task contains the following.
(1) The type of the machine learning model, i.e., classification, regression, or clustering, (2) one or more training and test datasets, (3) an evaluation function which assigns a score to the user-provided solution.
An example of a task is to train a classification model on the training dataset $D_1$ which maximizes the F1 score on the test dataset $D_2$.

\textbf{Data and Operations.}
We support three types of data.
(1) A \textit{Dataset} which has one or more columns of data and is analogous to dataframe objects, such as Pandas dataframe \cite{mckinney-proc-scipy-2010}), (2) an \textit{Aggregate} which contains a single value or list of values, and (3) a \textit{Model} which represents a machine learning model.
The type of data is determined by the operations which generate them.
\textit{Data preprocessing and feature engineering operations}, which include simple data transformation and aggregation, feature selection, and feature extraction operations, generate either a Dataset (e.g., map, filter, or one-hot encoding operations)  or an Aggregate (e.g., reduce operation).
\textit{Model training operations} generate a Model.
A Model is used either in other feature engineering operations, e.g., PCA model for dimensionality reduction, or to perform predictions on a test dataset.
%(3) Hyperparameter tuning operations which have the task of finding the best hyperparameters for a machine learning model.
%The most common tuning approaches are grid search, random search, and Bayesian hyperparameter search \cite{bergstra2012random,snoek2012practical}.
%In all three approaches, several models with different hyperparameters are trained and the model which performs best is returned as the final result.
%In our system, instead of only returning the best performing model, we also capture all the runs with different hyperparameters.
%As a result, hyperparameter tuning operations return a set of Models.
 
\textbf{ML Workload.}
An ML workload is a script or a machine learning pipeline which performs several data preprocessing, feature engineering, and model training operations. 
The collaborative data science platforms execute a workload in either a long-running process or an interactive environment using Jupyter notebooks \cite{Kluyver:2016aa}.
%Collaborative data science platforms typically provide two modes of executions for solving tasks:
%\textit{long-running workloads} and \textit{interactive workloads}.
%The user can write an end-to-end script which performs data loading, data cleaning, and model training. We refer to this type of workloads as long-running workloads.
%Depending on the size of the initial data, a long-running workload may take anything from seconds up to many hours.
%The other type of workloads is more interactive.
%Typically, through Jupyter notebooks, collaborative data science platforms allow users to write their analysis in an interactive fashion, whereupon execution of each operation (or group of operations), they can view the result and fine-tune their analysis code.
%We refer to this type of workloads as interactive workloads.

\subsection{Use Case}\label{subsec-motivational-example}
Kaggle is a collaborative data science platform which enables users and organizations to publish datasets and organize machine learning competitions by defining an ML task where all the users can participate and submit ML workloads for solving the task \cite{kagglewebsite}.
Kaggle utilizes docker containers, which are called kernels, to execute user workloads.
If the workload produces machine learning model artifacts, the users can choose to submit them and attain a score of how good their model performs on a test dataset.

For our use case, we select the competition \textit{Home Credit Default Risk}\footnote{https://www.kaggle.com/c/home-credit-default-risk/}.
The task is to train a classification model to predict whether a client can repay their loans.
There are a total of 9 datasets, 8 for training, 1 for evaluation, with a total size of 2.5 GB.
The goal of the submitted workloads is to produce machine learning models which maximize the area under the ROC curve, which measures how well a classifier performs.
Two of the most popular submitted workloads are copied and edited by different users more than 6000 times\footnote{https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction}\footnote{https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering}.
Together, the workloads produce 100s of intermediate dataset artifacts and several machine learning model artifacts with a total size \hl{of ~60 GB}.
Each execution time of each workload is roughly 200 seconds.
Since Kaggle does not provide the total number of executions, we cannot estimate how many times the workloads are executed.
However, the number of users who copied these workloads indicates the potentially large number of executions, i.e., at least 6000 times.

Kaggle does not store the artifacts, nor does it offer any automatic reuse capabilities.
Therefore, every time these workloads (or a modified version of them) are executed, Kaggle runs them from scratch.
An intelligent system, which stores the artifacts and reuses them later, saves 100s of hours of execution time only for the two workloads in this use case, which can benefit Kaggle by reducing the amount of required resource and operation cost.
In the next sections, we show how we selectively store artifacts, given a storage budget, and how we quickly find the relevant artifacts for reuse.