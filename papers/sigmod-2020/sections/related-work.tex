\section{Related Work} \label{sec-related-work}
Our system lies at the intersection of 3 categories of existing work.
The first category contains data science platforms that enable collaboration between users.
The second category consists of data management and provenance systems that capture the relationship between data artifacts.
The last category contains machine learning and data processing systems that optimize workloads by storing intermediates.

\textbf{Collaborative Data Science Platforms.}
Cloud-based systems, such AzureML \cite{team2016azureml}, Google's AI platform \cite{googleai}, Kaggle \cite{kagglewebsite}, and Google Colaboratory \cite{googlecolab} provide the necessary tools for users to write ML workloads in Jupyter notebooks.
Furthermore, users can publish and share their notebooks with others which could result in higher quality workloads.
However, none of these systems manage the generated ML artifacts and do not utilize them to optimize the execution of the workloads.
Our system manages and stores the intermediate ML artifacts and offer reuse and warmstarting methods to decrease the execution time of the future workloads.

OpenML \cite{vanschoren2014openml}, ModelDB \cite{vartak2016m}, and MLflow \cite{zaharia2018accelerating} are platforms which extract and store ML artifacts, such as machine learning models and intermediate datasets, in a database \cite{schelter2017automatically, Vanschoren2012}.
These platforms provide APIs and primitives for users to query the details of the ML artifacts, such as model evaluation result, preprocessing and feature engineering operations, and dataset descriptions.
Contrary to our systems, none of these platforms offer automatic materialization and reuse of the ML artifacts and leave that task to the users.

\textbf{Data Management and Provenance.}
DataHub \cite{bhardwaj2014datahub, bhattacherjee2015principles}, Context \cite{garcia2018context}, Ground \cite{hellerstein2017ground}, ProvDB \cite{miao2018provdb}, Aurum \cite{fernandez2018aurum}, and JuNEAU \cite{ives2019dataset} are data management and provenance systems that efficiently store fine-grained lineage information about the data artifacts and operations.
Furthermore, some of these systems provide primitives for querying lineage and discovering datasets.
We design our Experiment Graph by utilizing the approaches discussed in these systems.
Specifically, we follow DataHub's graph representation.
However, contrary to these systems, we utilize the stored information to optimize the execution of future workloads.
Furthermore, our materialization algorithm extends the materialization approach of Bhattacherjee et al. \cite{bhattacherjee2015principles} to tailor it to the machine learning workloads by considering the quality of the model artifacts.

\textbf{Materialization and Reuse in ML and Data Processing Systems.}
Helix \cite{xin2018h, xin2018helix}, DeepDive \cite{zhang2015deepdive}, Columbus \cite{zhang2016materialization}, and Mistique \cite{vartak2018mistique} are machine learning systems which optimize workloads by materializing intermediate data for reuse.
These systems have three fundamental differences when compared to our system.
First, the workload DAGs are typically small as these systems work with machine learning pipelines of a few operations.
Therefore, these systems do not need to tackle the problem of searching for reuse opportunities in a large graph.
However, Helix offers a reuse procedure with a polynomial time-complexity, which can potentially perform well on larger DAGs.
However, our reuse procedure for finding the optimal execution plan has a linear time-complexity.
Second, the materialization decisions in this system only utilize run-time and size and do not take into account the model quality.
Third, our system operates in a collaborative and multi-tenant environment.
Whereas, the scope of optimization in these systems, except for Mistique, is limited to a single session.
However, Mistique is a model diagnosis tool, which enables users to query intermediate artifacts from an artifact store efficiently.
Whereas, we focus on generating optimal execution plans for future workloads by utilizing reusing the artifacts in the Experiment Graph.
Nectar \cite{gunda2010nectar} and ReStore \cite{elghandour2012restore} offer materialization and reuse of intermediate data generated in DryadLINQ \cite{fetterly2009dryadlinq} and MapReduce jobs.
However, Both of these systems only support simple data processing pipelines and do not offer any optimizations for machine learning workloads.