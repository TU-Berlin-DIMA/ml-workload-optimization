\section{Artifact Materialization}\label{sec-materialization}
Depending on the number of executed workloads, the generated artifacts may require a large amount of storage space.
For example, the three workloads in our motivating example generate up to 125 GB of artifacts.
Moreover, depending on the storage and retrieval costs of the artifacts from the Experiment Graph, it may be less costly to recompute an artifact from scratch.
In this section, we introduce two algorithms for materializing the artifacts with a high likelihood of future reuse while ensuring the storage does not surpass the recomputation cost.
The first algorithm (Section \ref{subsec-ml-based-materialization}) utilizes general metrics, i.e., size, access frequency, and compute times, and storage cost of the vertices, and a machine learning specific metric, i.e., the quality of the machine learning models, to decide what artifacts to materialize.
The second algorithm (Section \ref{subsec-storage-aware}) extends the first algorithm and considers any overlap between the artifacts, i.e., a data column appearing in multiple artifacts.
To address this issue, we implement a deduplication strategy to avoid storing duplicated columns.
The second algorithm takes the duplication information into account when deciding on what artifacts to materialize.

\textbf{Notations. }
We use the following notations in this section.
Graph $G_E = (V,E)$ is the Experiment Graph, where $V$ represents the set of artifacts and $E$ represents the set of operations.
We use the terms artifact and vertex interchangeably.
Each vertex $v \in V$ has the attributes $\langle f, t, s, mat \rangle$.
$f$, $t$, and $s$ refer to the frequency, computation time, and size while $mat=1$ indicates $v$ is materialized and $0$ otherwise.
We also define the set of all ML models in $G_E$ as:
\[
M(G_E) \coloneqq  \{v \in V \mid v \text{ is an ML model}\}
\] 
and the set of all ML models that are reachable from vertex $v$ as:
\[
M(v) \coloneqq  \{m \in M(G_E) \mid\text{there is path from } v \text{ to } m\}
\] 

\textbf{Assumptions. }
We assume there exists an evaluation function that assigns a score to user-created ML models.
This is a reasonable assumption as the success of any ML application can only be measured through an evaluation function (e.g., using a test dataset in a Kaggle competition or monitoring certain KPIs in a company).
In the Experiment Graph, any vertex that represents an ML model artifact contains an extra attribute $q$ representing the quality of the model, where $0 \leq q \leq 1$.
\subsection{Materialization Problem Formulation}\label{subsec-materialization-problem}
Existing work proposes different algorithms for the efficient storage of dataset versions and their storage and recomputation trade-off \cite{bhattacherjee2015principles}.
The goal of the existing algorithms is to materialize the artifacts that result in a small recomputation cost while ensuring the total size of the materialized artifacts does not exceed the storage capacity.
However, two reasons render the existing algorithms inapplicable to our artifact materialization problem.
First, existing approaches only consider access frequency and reconstruction cost of an artifact.
In collaborative workload optimization, we consider the effect of the materialized artifacts on the efficiency of machine learning workloads, i.e., materialize artifacts that result in high-quality machine learning models.
Second, existing solutions address an offline scenario where new artifacts do not arrive at the system.
In a collaborative environment, users continuously execute workloads resulting in new artifacts.

Here, we formulate the problem of artifact materialization as a multi-objective optimization problem.
The goal of artifact materialization is to materialize a subset of the artifacts which minimizes the weighted recomputation cost while maximizing the estimated quality.

\textbf{Weighted Recreation Cost Function (WC).} 
The first function computes the weighted recreation cost of all the vertices in the graph:
\[
WC(G_E) \coloneqq  \sum\limits_{v \in V}  (1-v.mat) \times v.f \times v.t
\]
Intuitively, the weighted recreation cost computes the total amount of execution time required to recompute all the vertices while considering their frequencies.
Materialized artifacts incur no cost since they are stored.
Unmaterialized artifacts incur a cost equal to their computation time multiplied by their frequencies.

\textbf{Estimated Quality Function (EQ).} 
EQ computes the estimated quality of all the materialized vertices in the graph.
To compute EQ, we first define the potential of a vertex:
\[
p(v) \coloneqq  
		\begin{cases}
		0 , & \text{if }M(v) = \emptyset  \\
		\max\limits_{m \in M(v)} m.q, & \text{otherwise}
		\end{cases}
\]
Intuitively, the potential of a vertex is equal to the quality of the best model that is reachable from the vertex.
Note that vertices that are not connected to any model have a potential of 0.
Now, we define the estimated quality function as:
\[
EQ(G_E) \coloneqq  \sum\limits_{v \in V}  v.mat \times p(v)
\]

\textbf{Multi-Objective Optimization.}
Given the two functions, i.e., weighted recreation cost and estimated quality, we would like to find the optimal set of vertices to materialize which minimizes the weighted recreation cost function and maximizes the estimated quality function under limited storage size, $\mathcal{B}$.
For ease of representation, we instead try to minimize the inverse of the estimated quality function.
We formulate the optimization problem as follows:
\begin{equation}\label{multi-obj-opt}
\begin{split}
& minimize(WC(G_E), \dfrac{1}{EQ(G_E)}), \\
& \text{subject to:} \sum\limits_{v \in V} v.mat \times v.s \leq \mathcal{B}
\end{split}
\end{equation}
Existing work prove that minimizing the recreation cost alone is an NP-Hard problem \cite{bhattacherjee2015principles}.
While there are different approximate strategies for solving multi-objective optimization problems \cite{coello2007evolutionary}, they are time-consuming, which renders them inappropriate to our setting, where new artifacts are constantly added to the graph.
Execution of every workload results in an update to the experiment graph, which in turn, requires a recomputation of the materialized set.
As a result, existing solutions to multi-objective optimization problems are not suitable for artifact materializations of the experiment graph.
\subsection{ML-Based Greedy Algorithm}\label{subsec-ml-based-materialization}
We propose a greedy heuristic-based algorithm for solving the multi-objective optimization problem.
Our algorithm aims to minimize the weighted recreation cost and maximize the estimated quality function.
\begin{algorithm}[h]
\KwData  {$G_E(V,E)$ experiment graph, $\mathcal{B}$ storage budget}
\KwResult {$\mathcal{M}$ set of vertices to materialize}
$S \coloneqq 0$\tcp*{size of the materialized artifacts}
$\mathcal{M} \coloneqq \emptyset$\tcp*{materialized set}

$PQ \coloneqq $ empty priority queue\;
\For {$v \leftarrow V$}{
	\If{$v.mat = 0$}{
		 $v.utility \coloneqq \mathcal{U}(v)$\;
		 $PQ.insert(v)$\tcp*{sorted by $utility$}
	}
}
\While{$PQ.not\_empty()$}{
$v \coloneqq PQ.pop()$\tcp*{vertex with max $utility$}
\If {$S+v.s \leq \mathcal{B}$}{
	$\mathcal{M}  \coloneqq  \mathcal{M} \cup v$\;
	$S \coloneqq S + v.s$\;		
	}
}
return $\mathcal{M}$\;
\caption{Artifacts-Materialization}\label{algorithm-materialization}
\end{algorithm}

Algorithm \ref{algorithm-materialization} shows the details of our method for selecting the vertices to materialize.
For every non-materialized vertex, we compute the utility value of the vertex (Lines 4-7).
Then, we start materializing the vertices, sorted by their utilities, until the storage budget is exhausted (Lines 8-12).
The utility function $\mathcal{U}(v)$ combines the potential, recreation cost, and size of a vertex.
We design the utility function in such a way that materializing vertices with larger utility values contributes more to minimizing the multi-objective optimization equation (Equation \ref{multi-obj-opt}).
Before we define $\mathcal{U}(v)$, we need to define 3 functions: the recreation cost of a vertex $C_r(v)$, the cost-size ratio $r_{cs}(v)$, and the load cost of a vertex $C_l(v)$.
The recreation cost of a vertex is:
\[
C_r(v) \coloneqq \sum\limits_{v' \in G_v} v'.t
\]
where $G_v \subseteq G_E$ is the compute graph of $v$, i.e., the set of all vertices and edges which one must execute to recreate the vertex $v$.
The compute graph of a vertex always starts at one or more source vertices of the Experiment Graph and ends at the vertex itself.
The weighted cost-size ratio is:
\[r_{cs}(v) = \frac{v.f  \times C_r(v)}{v.s}\]
The weighted cost-size ratio has the unit $\tfrac{s}{MB}$ and indicates how much time do we spend on computing 1 MB of an artifact.
Lastly, $C_l(v)$ is the cost (in seconds) of loading the vertex $v$ from the Experiment Graph. 
The $C_l(v)$ function depends on the size of the vertex and where the Experiment Graph resides (i.e., in memory, on disk, or in a remote location).
We now define the utility function as the linear combination: 
\[
\mathcal{U}(v) \coloneqq  	
		\begin{cases}
		0, & \text{if }  C_l(v) \ge C_r(v)  \\ 
		\alpha p'(v) + (1-\alpha)r'_{cs}(v), & \text{otherwise}
		\end{cases}
\],
where $p'(v)$ and $r'_{cs}$ are normalized values of $p(v)$ and $r_{cs}$ (i.e., for every vertex divide the value by the total sum).
If $C_l(v) \ge C_r(v)$, then recomputing the vertex is always more efficient than loading it from the Experiment Graph.
Therefore, we never materialize such vertices.
Taking the load cost into account enables us to adapt the materialization algorithm to different system architecture types (i.e., single node vs distributed) and storage unit types (i.e., memory or disk).
$ \alpha$ ($0 \leq \alpha \leq 1$) indicates the importance of potential.
For example, when $\alpha > 0.5$, we assign more importance to model quality than weighted cost-size.
Therefore, we materialize vertices with a high potential even if they have a small weighted cost-size ratio.
In collaborative environments, where users are working toward building high-quality models and data exploration is not the main goal, a larger $\alpha$ encourages faster materialization of high-quality models.

\textbf{Run-time and Complexity.}
We compute the recreation cost and potential of the nodes incrementally using one pass over the Experiment Graph.
Thus, the complexity of the materialization algorithm is $\mathcal{O}(|V|)$ where $|V|$ is the number of vertices in the Experiment Graph.
The size of the Experiment Graph increases as users execute more workloads.
This increases the execution cost of the materialization algorithm.
However, we only need to compute the utility for a subset of the vertices. 
First, we must compute the utility of the vertices belonging to the new workload.
The addition of the new vertices affects the normalized cost and potential of other vertices, thus requiring a recomputation.
However, we only need to recompute the utility of the materialized vertices and compare them with the utility of the workload vertices.
As a result, the complexity of each run of the materialization algorithm is $\mathcal{O}(|W| + |M|)$, where $|W|$ is the number of vertices in the new workload DAG and $|M|$ is the number of the materialized vertices.
\subsection{Storage-Aware Materialization Algorithm}\label{subsec-storage-aware}
Many feature engineering operations operate only on one or a few columns of a dataset artifact; thus, the output artifact of an operation may contain a subset of the input artifact of the operation.
In such a case, materializing both the input and output artifact may lead to many duplicated columns.
To further reduce the storage cost, we implement a deduplication mechanism.
We assign a unique id to every column of the dataset artifacts.
To compute the unique id of the columns after the execution of an operation, we first determine the columns which are affected by the operation.
Then, we use a hash function that receives the operation hash and the input column's unique id and outputs a new id.
Our approach for computing the unique column id ensures the following.
First, after the execution of an operation, all the columns which are not affected by the operation will carry the same unique id.
Second, two columns belonging to two different dataset artifacts have the same unique id, if and only if, the same operations have been applied to both columns.

We implement a storage manager unit which takes the deduplication information into account.
When materializing an artifact, the storage manager examines the id of every column, and only stores the columns that do not exist in the storage unit.
The storage manager tracks the column hashes of all the artifacts in the Experiment Graph.
When a specific artifact is requested, the storage manager combines all the columns which belong to the artifact into a data frame and returns the data frame.

\textbf{Greedy Meta-Algorithm.}
Using the storage manager, we propose a storage aware materialization meta-algorithm (Algorithm \ref{algorithm-compression-aware-materialization}) that iteratively invokes Algorithm \ref{algorithm-materialization} (Artifact-Materialization).
We define a variable to represent the remaining budget (Line 1).
While the budget is not exhausted, we proceed as follows.
We apply the Artifact-Materialization algorithm using the remaining budget to compute new vertices for materialization (Line 4).
If the Artifact-Materialization algorithm does not find any new vertices to materialize, we return the current set (Lines 5 and 6).
We compute the compressed size of the graph artifacts (Line 8), which computes the size of graph artifacts after deduplication. 
Next, we update the required storage size of the remaining artifacts, i.e., the set of unmaterialized artifacts (Line 9).
For example, if the materialized artifact $v_1$ contains some of the columns of the non-materialized artifact $v_2$, then we only need to store the remaining columns of $v_2$ to fully materialize it.
Therefore, we update the size of $v_2$ to indicate the amount of storage it requires to fully materialize.
Finally, we compute the remaining budget by deducting the compressed size from the initial budget (Line 10).
\begin{algorithm}[h]
\KwData  {$G_E(V,E)$ experiment graph, $\mathcal{B}$ storage budget}
\KwResult {$\mathcal{M}$ set of vertices to materialize}
$ \mathcal{B}' \coloneqq  \mathcal{B}$ \;
$\mathcal{M'}  \coloneqq \emptyset$\;
\While {$\mathcal{B}'  > 0$}{
	 $\mathcal{M} \coloneqq $ \textit{Artifact-Materialization}($G_E, \mathcal{B}' $)\;
	\If {$\mathcal{M'} = \mathcal{M}$}{
			return $\mathcal{M}$\;
	}
	$\mathcal{M}' \coloneqq \mathcal{M}$\;
	$compressed\_size \coloneqq  deduplicate(G_E)$\;
	$update\_required\_size(G_E)$\;
	$\mathcal{B}'  \coloneqq  \mathcal{B} -  compressed\_size$\;
}
return $\mathcal{M}$\;
\caption{Storage-aware Materialization}\label{algorithm-compression-aware-materialization}
\end{algorithm}

%TODO if existing algorithms produce good results, this can be a good follow up work and we do not need 
%\textbf{Fractional Greedy Algorithm.}
%\todo[inline]{I have some rough ideas one what we can do here, but need to work on it a bit more. We can find all the artifacts that have common columns, and give some sort of weight to artifacts who have the highest amount of columns that are shared between other artifacts. }