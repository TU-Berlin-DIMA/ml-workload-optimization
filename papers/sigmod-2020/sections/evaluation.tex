\section{Evaluation} \label{sec-evaluation} 
\todo[inline]{most of the figures need a bit of post-processing to get the font size on the same scale and improve their quality}
In this section, we evaluate the performance of our collaborative optimizer system.
We focus on investigating the effect of our materialization and reuse algorithms on the execution cost of workloads in collaborative environments.
We first describe the setup, i.e., the hardware specification and the experiment workloads.
Then, we show the run-time improvement of our optimizer.
Finally, we investigate the effect of the individual contributions, i.e., materialization and reuse algorithms, on the run-time and storage cost.
\begin{table*}[t]
\begin{tabular}{lp{0.84\textwidth}rr}
\hline
\textbf{$ID$} & \textbf{$Description$}& \textbf{$N$}& \textbf{$S$}   \\
\hline
1 &  \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small Real script titled 'start-here-a-gentle-introduction'. It includes several feature engineering operations before training logistic regression, random forest, and gradient boosted tree models.} & 397 & 14.5\\[0.4cm]

2 &   \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small Real script titled 'introduction-to-manual-feature-engineering'. It joins multiple tables to generate a dataset and trains gradient boosted tree models.} & 406 & 25\\[0.4cm]

3 &   \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small Real script titled 'introduction-to-manual-feature-engineering-p2'. It is similar to workload 2, but with larger datasets.} & 146 & 83.5\\[0.15cm]

4 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A modified version workload 1 submitted by the Kaggle user: 'crldata'. It modifies the hyperparameters of the gradient boosted tree.} & 280 & 10\\[0.4cm]

5 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A modified version workload 1 submitted by the Kaggle user: 'taozhongxiao'. It runs random and grid search for gradient boosted tree model on the dataset of workload 1.} & 402 & 13.8\\[0.4cm]

6 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small Custom script based on workload 2. It trains a gradient boosted tree on the dataset of workload 2.} & 121 & 21\\[0.15cm]

7 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small Custom script based on workload 3. It trains a gradient boosted tree on the dataset of workload 3.} & 145 & 83\\[0.15cm]

8 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small Custom script that joins the features of workload 1 and 2. Then, it trains a gradient boosted tree on the joined dataset.} & 341 & 21.1\\
\hline
\end{tabular}
\caption{Kaggle workloads description. $N$ is number of the artifacts and $S$ is total size of the artifacts in GB.}
\label{kaggle-workload}
\end{table*}

\subsection{Setup}
We execute all the experiments on a server running Linux Ubuntu with 128 GB of RAM.
We implement a prototype of our system in python 2.7.12.
In the prototype, we implement EG using python's NetworkX 2.2 \cite{hagberg2008exploring}.
Unless specified otherwise, we run every experiment 3 times and report the average results.
We evaluate our proposed optimizations on two different sets of workloads\footnote{workload and experiment scripts will be made available in the camera ready version}.
\todo[inline]{What's the best way of letting the reviewers know we will make the code available? check footnote 3}

\textbf{Kaggle workloads.} 
In the first set of workloads, we recreate the collaborative environment of the use case in Section \ref{sec-background}.
We introduced three workloads in the use case.
We retrieve two more workloads from the same Kaggle competition.
We also design three workloads based on the existing ones.
Table \ref{kaggle-workload} shows details of the workloads we utilize in our experiments.
The competition has nine datasets with a total size of 2.5 GB\footnote{https://www.kaggle.com/c/home-credit-default-risk/data}.
The total size of the artifacts after executing all the workloads is around 130 GB.
Unless specified otherwise, we set the materialization budget to 32 GB.
For all the experiments, we execute the workloads in order (from 1 to 8) and report the results.

\textbf{OpenML workloads.} In the OpenML workloads, we retrieve all the scikit-learn pipelines for task 31\footnote{https://www.openml.org/t/31}, i.e., classifying customers as good or bad credit risks using the German Credit data from the UCI repository \cite{asuncion2007uci}.
OpenML refers to ML pipelines as flows and the hyperparameter configurations as setups.
The execution of a flow with a specific setup on a task is called a run.
We extracted the first 2000 runs for task 31, which results in 16 unique flows. 
The dataset is small, and the total size of the artifacts after executing the 2000 runs is 1,5 GB.

\subsection{Execution Time}
\textbf{Kaggle Workloads.}
In this experiment, we evaluate the impact of our collaborative optimizer on the use case of Section \ref{sec-background}.
In the use case, we describe three workloads (Workloads 1-3 of Table \ref{kaggle-workload}).
Kaggle reports users have copied and modified these workloads a total of 7000 times.
Therefore, at the very least, users execute these workloads 7000 times.
Figures \ref{exp-reuse-kaggle-same-workload}(a)-(c) show the result of repeating the execution of each workload.
Before the first run, EG is empty; therefore, both the baseline and our collaborative optimizer have the same run-time.
By materializing and reusing the artifacts, our collaborative optimizer reduces the run-time of the consecutive executions by one order of magnitude for workloads 2 and 3.
Workload 1 executes an external visualization command that plots a bivariate kernel density estimate, which incurs a large overhead.
Since our collaborative optimizer does not store the visualization information, it must re-execute the visualization command; thus, resulting in a smaller run-time reduction.
\begin{figure}
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/start_here_a_gentle_introduction.pgf}%
}
\caption{Workload 1}
\end{subfigure}%
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering.pgf}%
}
\caption{Workload 2}
\end{subfigure}%
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering_p2.pgf}%
}
\caption{Workload 3}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
\centering
%\includegraphics[width=\linewidth]{../images/experiment-results/kaggle_home_credit/execution_time/different_workloads}
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/different_workloads.pgf}%
}
\caption{Execution of Kaggle workloads in sequence}
\end{subfigure}
\caption{Execution of Kaggle workloads (materialization budget = 32 GB). KG = Kaggle's infrastructure (our baseline), CO = Collaborative Optimizer.}
\label{exp-reuse-kaggle-same-workload}
\end{figure}

Figure \ref{exp-reuse-kaggle-same-workload}(d) shows the cumulative run-time of executing the workloads of Table \ref{kaggle-workload} in sequence.
This corresponds to a real scenario, where after some scripts in a collaborative environment gain popularity, e.g., Workloads 1-3, other users modify and try to improve them, e.g, Workloads 4-8.
The experiment shows that optimize even a single execution of each workload decreases the cumulative run-time by half.
In a real collaborative environment, there are hundreds of more modified scripts and possibly thousands of repeated execution of such scripts, resulting in 1000s of hours of reduction in the cumulative run-time.

\subsection{Materialization}
In this set of experiments, we show the impact of our materialization algorithms on the total size of the stored artifacts and total run-time of the workloads.
We run the workloads of Table \ref{kaggle-workload} under different materialization budgets using both our heuristics-based and storage-aware materialization algorithms.

Figures \ref{exp-sa-vs-simple-size}(a)-(d) show the real size of the stored artifacts under different materialization budgets.
For the heuristics-based algorithm, the maximum real size is always capped at the budget.
However, in the storage-aware algorithm, we observe that the real size of the stored artifacts can reach up to 8 times the budget.
With a materialization budget of 8 GB, the storage-aware algorithm materializes almost half of the artifacts.
When the materialization budget is more than 8 GB, storage-aware materialized nearly all the artifacts.
This indicates that there is considerable overlap between the artifacts of ML workloads.
By deduplicating the artifacts, our storage-aware materialization algorithm can materialize more artifacts.
As a result, the storage-aware achieves a much smaller total run-time when compared with the heuristics-based materialization.

\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-8.pgf}%
}
\caption{Budget = 8 GB}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-16.pgf}%
}

\caption{Budget = 16 GB}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-32.pgf}%
}

\caption{Budget = 32 GB}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-64.pgf}%
}
\caption{Budget = 64 GB}
\end{subfigure}
\caption{Real size of the stored artifact for storage aware and simple materialization under different budgets (SA = storage-aware materialization, HM = heuristics-based materialization)  \todo[inline]{add lines for total size and budget. Need to properly explain the sudden rise at workload 7 for SA budget = 16}}
\label{exp-sa-vs-simple-size}
\end{figure}

Figure \ref{exp-sa-vs-simple-size}(a) shows the total run-time of all the workloads under different budgets for both materialization algorithms.
Even with a materialization budget of 8 GB, the storage-aware performs similar to the scenario where all the artifacts are materialized (represented by ALL in the figure).
On the contrary, under a small materialization budget ($\leq 16$), the heuristics-based materialization performs 50\% worse than the other scenarios.
This is mainly because most of the preprocessed dataset artifacts are large, e.g., in Workload 3, some artifacts are more than 5 GB.
Most of these artifacts differ with each other in only a few columns.
However, the heuristics-based algorithm does not exploit this similarity and chooses not to materialize any of the large artifacts.
Recomputing these artifacts is costly, which results in a larger total run-time for heuristics-based materialization under a smaller budget.

\begin{figure}
\centering
 \resizebox{0.7\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/run-time.pgf}%
}
\caption{Total run time of the workloads with different materialization algorithms and budgets}
\label{run-time-vs-mat}
\end{figure}

\textbf{Model Materialization. }
One goal of our materialization algorithms is to materialize high-quality models as soon as they appear in a new workload.
In this experiment, we run a typical scenario, where users compare the score of their models with the score of the best performing model in the collaborative environment.
In this experiment, we execute all the OpenML workloads one by one and keep track of the best performing workload.
In every workload, as a post-processing step, we compare the score of the model with the score of the best performing model so far.
Figure \ref{exp-model-materialization} shows the result.
\todo[inline]{Will re-run this experiment with different $\alpha$ values (Utility function in Section \ref{subsec-ml-based-materialization}}

Figure \ref{exp-model-materialization}(a) shows the total run-time of executing all the OpenML workloads and comparing the model of each workload with the best performing model so far.
In the baseline (OML), for every workload, we have to rerun the pipeline of the best performing model.
As a result, OML has a much higher total run-time when compared with our collaborative optimizer (CO).
Figure \ref{exp-model-materialization}(b) shows why there is such a large difference between OML and CO.
In CO, when we encounter a model that performs better than all the existing models, the materialization algorithm immediately stores the pipeline and the model.
As a result, we reuse the model from EG, which incurs a small overhead.
The total overhead of rerunning the pipeline with the best model for all the 2000 OpenML workloads is around 65 seconds, while OML incurs an overhead of around 2000 seconds to re-execute the pipelines.

\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/model_materialization/cumulative-runtime.pgf}%
}
\caption{Combined run-time}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/model_materialization/bestpipeline-overhead.pgf}%
}
\caption{Best pipeline run-time}
\end{subfigure}
\caption{Effect of model materialization for OpenML workloads (CO = Collaborative Optimizer, OML = OpenML Default)}
\label{exp-model-materialization}
\end{figure}

\subsection{Reuse}
In this experiment, we evaluate our reuse and warmstarting algorithms. 
We compare our linear time reuse algorithm (LN) with two baselines.
In the first baseline (ALL\_M), all materialized artifacts are reused.
In the second baseline (ALL\_C), all the materialized artifacts are recomputed.
The performance of ALL\_C is very similar to when there is no Experiment Graph.
Thus, ALL\_C finishes the execution of all the Kaggle workloads in around 2000.
ALL\_M has a similar performance to LN until workload 7.
In workload 7, there are multiple large artifacts which have a larger load cost than compute cost.
Thus, our reuse algorithm ignores these materialized artifacts and chooses to recompute them.
In this experiment, EG is inside the memory of the machine; thus, load times are generally low.
In scenarios where EG is on disk, we expect LN to outperform ALL\_M with a larger margin due to the very large load cost of some of the materialized artifacts.
\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/reuse/simple-mat.pgf}%
}
\caption{Simple}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/reuse/storage-aware-mat.pgf}%
}

\caption{Storage-aware}
\end{subfigure}
\caption{Comparison of our linear reuse algorithm with two baselines under different materialization schemes (LN = linear, ALL\_C = all compute, ALL\_M = all materialized). \todo[inline]{Same experiment for on-disk. This, I will only do if I have time.}}
\end{figure}

\textbf{Warmstarting.}
In this experiment, we evaluate the effect of our warmstarting method.
We execute all the OpenML workloads with and without warmstarting.
Figure xxx shows the cumulative run-time of the OpenML workloads.
\todo[inline]{Still need to finish the experiment}
\subsection{Discussion}
