\section{Evaluation} \label{sec-evaluation} 
In this section, we evaluate the performance of our collaborative optimizer.
We focus on investigating the effect of our optimizations on execution cost of workloads in a typical collaborative environments.
We first describe the setup, including the hardware and the workloads we execute.
Then, we show the run time improvement of our optimizer.
Then, we investigate the effect of the individual contributions, i.e., materialization and reuse algorithms, on execution cost and storage cost.
\begin{table*}[t]
\begin{tabular}{lp{0.75\textwidth}rr}
\textbf{id} & \textbf{description}& \textbf{\# artifacts}& \textbf{total size}   \\
\hline
1 &  \parbox[c][][t]{0.75\textwidth}{\linespread{0.5}\selectfont Real script titled 'start-here-a-gentle-introduction'. It includes several feature engineering operations before training logistic regression, random forest, and gradient boosted tree models.} & 397 & 14.5GB \\[0.3cm]

2 &   \parbox{0.75\textwidth}{\linespread{0.5}\selectfont Real script titled 'introduction-to-manual-feature-engineering'. It joins multiple tables to generate a dataset and trains gradient boosted tree models.} & 406 & 25 GB\\[0.3cm]

3 &   \parbox{0.75\textwidth}{\linespread{0.5}\selectfont Real script titled 'introduction-to-manual-feature-engineering-p2'. It is similar to workload 2, but with larger datasets.} & 1000 & 80 GB\\[0.3cm]

4 & \parbox{0.75\textwidth}{\linespread{0.5}\selectfont A modified version workload 1 submitted by the Kaggle user: 'crldata'. It modifies the hyperparameters of the gradient boosted tree.} & 1000 & 14.5 GB\\[0.3cm]

5 & \parbox{0.75\textwidth}{\linespread{0.5}\selectfont A modified version workload 1 submitted by the Kaggle user: 'taozhongxiao'. It runs random and grid search for gradient boosted tree model on dataset of workload 1.} & 1000 & 14.5 GB\\[0.3cm]

6 & \parbox{0.75\textwidth}{\linespread{0.5}\selectfont Custom script based on workload 2. It trains a gradient boosted tree on dataset of workload 2.} & 1000 & 14.5 GB\\[0.3cm]

7 & \parbox{0.75\textwidth}{\linespread{0.5}\selectfont Custom script based on workload 3. It trains a gradient boosted tree on dataset of workload 3.} & 1000 & 14.5 GB\\[0.3cm]

8 & \parbox{0.75\textwidth}{\linespread{0.5}\selectfont Custom script that joins the features of workload 1 and 2. Then, it trains a gradient boosted tree on the joined dataset.} & 1000 & 14.5 GB\\[0.3cm]

\end{tabular}
\caption{Kaggle workloads description. \hl{only the first two rows have the correct artifacts number and size}}
\label{kaggle-workload}
\end{table*}

\subsection{Setup}
\hl{We execute all the experiments on a server running Linux xxx with 128 GB of RAM and xxx GB of SSD.
We implement a prototype of our system in python 2.7 supporting both in-memory and disk-based Experiment Graph.
In the prototype, we implement the Experiment Graph using python's networkX library.
Unless specified otherwise, we run every experiment 5 times and report the average results (error bar included).}
To evaluate our proposed optimizations, we provide two different set of workloads.

\textbf{OpenML workloads.} In the OpenML workloads, we retrieve all the scikit learn open ml pipelines for task 31, i.e., classifying customers as good or bad credit risks using the German Credit data from the UCI repository \cite{Dua:2017}\footnote{https://www.openml.org/t/31}.
\todo[inline]{more details after i finish kaggle experiments}
%Table \ref{tab-openml-pipelines} shows the id, components, and number of executions of the OpenML pipelines.\footnote{information about each pipeline is available at https://www.openml.org/f/id}
%
%\begin{table}
%\setlength\tabcolsep{1.5pt} % This is to ensure the table does not go out of bound
%\begin{tabular}{llr}
%\hline
%\textbf{id} & \textbf{operations} & \textbf{\#exec}   \\
%\hline
% 5981&  \makecell[l]{Imputer$\rightarrow$ Standard scaler$\rightarrow$Logistic regression} &11        \\
% 7707&  \makecell[l]{Imputer$\rightarrow$Onehot encoder$\rightarrow$Standard scaler\\$\rightarrow $Variance thresholder$\rightarrow$SVM }&594 \\
% 8315&  \makecell[l]{Imputer$\rightarrow$Onehot encoder\\ $\rightarrow$Variance thresholder$\rightarrow $Random Forest} &1084  \\
%8353 & \makecell[l]{Imputer$\rightarrow$Onehot encoder\\$\rightarrow$Variance thresholder $\rightarrow $Svm}  & 1000\\
%8568 &  \makecell[l]{Imputer$\rightarrow$Onehot encoder\\$\rightarrow$Variance thresholder$\rightarrow $Random Forest} &555 \\
%\hline
%\end{tabular}
%\caption{OpenML pipeline descriptions.}
%\label{tab-openml-pipelines}
%\end{table}

\textbf{Kaggle workloads.} In the second set of workloads, we use the collaborative environment of the use case in Section \ref{sec-background}.
In the use case, we introduced 3 popular scripts.
In our experiments, we retrieve 2 more scripts from the Home Credit Default Risk competition in Kaggle and designed 3 scripts based on the existing ones.
The competition has 9 datasets with a total size of 2.5 GB \footnote{https://www.kaggle.com/c/home-credit-default-risk/data}.
Table \ref{kaggle-workload} shows details of the workloads we use in our experiments.

\subsection{Execution Time}
\textbf{Kaggle Workloads.}
We start by showing the impact of our collaborative optimizer on the use case of Section \ref{sec-background}.
In the use case, we describe three workloads (workloads 1-3 of Table \ref{kaggle-workload}).
Kaggle reports different users have copied and modified these workloads a total of 7000 times.
Therefore, at the very least, users execute these workloads 7000 times.
Figures \ref{exp-reuse-kaggle-same-workload} (a)-(c) show the result repeating the execution of each workload.
For the first run, the Experiment Graph is empty, therefore both baseline our collaborative optimizer result in the same run-time.
By materializing and reusing artifacts, our collaborative optimizer manages to reduce the run-time of the consecutive executions by an order of magnitude.
Workload 1 executes an external visualization commands which plots bivariate kernel density estimate, which incurs a large overhead.
Since our collaborative optimizer does not store the visualization information, it must execute the visualization operation; thus, resulting in a smaller run-time reduction.
\begin{figure}
\begin{subfigure}[b]{0.33\linewidth}
\centering
%\includegraphics[width=\linewidth]{../images/experiment-results/kaggle_home_credit/execution_time/repetition/start_here_a_gentle_introduction}
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/start_here_a_gentle_introduction.pgf}%
}

\caption{Workload 1}
\end{subfigure}%
\begin{subfigure}[b]{0.33\linewidth}
\centering
%\includegraphics[width=\linewidth]{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering}
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering.pgf}%
}

\caption{Workload 2}
\end{subfigure}%
\begin{subfigure}[b]{0.33\linewidth}
\centering
%\includegraphics[width=\linewidth]{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering_p2}
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering_p2.pgf}%
}
\caption{Workload 3}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
\centering
%\includegraphics[width=\linewidth]{../images/experiment-results/kaggle_home_credit/execution_time/different_workloads}
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/different_workloads.pgf}%
}
\caption{Execution of several workloads one by one}
\end{subfigure}
\caption{Execution of Kaggle workloads (materialization budget = 32 GB). KG represents current Kaggle's infrastructure (our baseline) and CO represent our collaborative optimizer.}
\label{exp-reuse-kaggle-same-workload}
\end{figure}

Figure \ref{exp-reuse-kaggle-same-workload} (d) shows the cumulative run time executing the 8 workloads of Table \ref{kaggle-workload} one after another.
This corresponds to a real scenario, where after some scripts of a collaborative environment gain popularity, i.e., workloads 1-3, other users modify and try to improve them, i.e., workloads 4-8.
Even running every workload once decreases the cumulative run-time reduction by half.
In a real collaborative environment, there are hundreds of more modified scripts and possibly thousands of repeated execution of these scripts, resulting in 1000s of hours of reduction in cumulative run-time.


\subsection{Materialization}
In this experiment, we run the workloads of Table \ref{kaggle-workload} under different materialization rates.
Figure \ref{exp-sa-vs-simple-size} shows the total run-time of all the workloads under different materialization budgets for the storage-aware and simple materialization algorithms.
\todo[inline]{There seems to be a bug reporting the result.}
\begin{figure}
\centering
%\includegraphics[width=\linewidth]{../images/experiment-results/kaggle_home_credit/execution_time/repetition/start_here_a_gentle_introduction}
 \resizebox{0.5\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/run-time.pgf}%
}
\caption{Total run time of all the workloads with different budgets (SA = storage-aware materialization, SM = simple materialization)}
\label{exp-runtime-vs-mat-budget}
\end{figure}

\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-8.pgf}%
}
\caption{Budget = 8}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-16.pgf}%
}

\caption{Budget = 16}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-32.pgf}%
}

\caption{Budget = 32}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-64.pgf}%
}
\caption{Budget = 64}
\end{subfigure}
\caption{Actual size of the stored artifact for storage aware and simple materialization under different budgets \todo[inline]{add total size and line for budget}}
\label{exp-sa-vs-simple-size}
\end{figure}
Figure \ref{exp-sa-vs-simple-size}  shows the actual size of the stored artifacts under different budgets.
For the simple materialization algorithm, the maximum size of the stored artifact never surpassed the budget.
However, in the storage-aware algorithm, we observer that the actual size of the stored artifacts is 2 to 3 times more than the specified budget.
This shows that the rate of the duplicated columns in the artifacts of machine learning workloads is generally high, and our storage-aware algorithm is able to materializes more artifacts by de-duplicating the columns before storing them.

\textbf{Model Materialization. }
Our materialization algorithm can quickly identify high-quality models, even if they appear in only 1 workload.
In this experiment, we run a typical scenario for OpenML, where we run a series of workloads one by one.
We always keep track of the best performing model.
After encountering a model, all the consecutive workloads compare the quality of their model with the best performing model so far.
If the workload performs better, the best performing model is updated.
\todo[inline]{Since our algorithm materializes high-quality models the moment they appear in a new workload. I expect our collaborative optimizer executes the above scenario faster than baseline, since it always materializes the best performing model, but the baseline needs to re-execute the pipeline of the best performing model to recreate its result.}
\subsection{Reuse and Warmstarting}
In this section, we compare our reuse algorithm we 2 baselines.
Baseline 1 loads every materialized artifact.
Baseline 2 re-computes the materialized artifacts and do not load them from EG.
\todo[inline]{Even for in-memory Experiment Graph, I expect our reuse perform better. Ideally, I'd like to also run the experiment when EG is stored on disk. I haven't implemented the on-disk Experiment Graph yet and may not be able to do it for SIGMOD.}

\subsection{Discussion}
%
%\subsubsection{Feature Processing Reuse}  
%In Figure \ref{evaluation-reuse-open-figure}, we show the effect of the reuse optimization on the OpenML workflow.
%The number of executed steps drastically decreases as the majority of the pipelines have the exact same data transformation steps and they only differ in the hyperparameters of the model (Figure \ref{evaluation-reuse-open-figure}a).
%However, Figure \ref{evaluation-reuse-open-figure}b shows that the reuse optimization does not impact the total execution time.
%This is specific to the OpenML use case, as the model training time dominates the data transformation time.
%\todo[inline]{The figure only shows estimates}
%
%\begin{figure}
%\centering
%\begin{subfigure}{.5\linewidth}
%  \includegraphics[width=\linewidth]{../images/experiment-results/reuse-openml-steps.eps}
%  \caption{Executed Steps}
%  \label{fig:sub1}
%\end{subfigure}%
%\begin{subfigure}{.5\linewidth}
%  \includegraphics[width=\linewidth]{../images/experiment-results/reuse-openml-time.eps}
%  \caption{Execution Time}
%  \label{fig:sub1}
%\end{subfigure}
%\caption{Effect of the reuse optimization on the total number of executed transformations and the total execution time for every OpenML pipeline}
%\label{evaluation-reuse-open-figure}
%\end{figure}
%
%\subsubsection{Model Warmstarting}
%In this experiment, we study the effect of the model warmstarting optimization on two pipelines (pipelines 5891 and 8568) from the openml database.
%Pipeline 5891 has a logistic regression model.
%There a total of 11 configurations in the database.
%The stopping condition for the logistic regression model is the convergence tolerance.
%Pipelines 8568 has random forest model.
%There are a total 555 configurations for pipelines 8568.
%The training of the random forest stops when the number of samples in any leaf node is below a user-defined threshold.
%
%Figure \ref{evaluation-warmstarting-figure} shows the result of the model warmstarting optimization on two types of models in the experiment database.
%Figures \ref{evaluation-warmstarting-figure}a and \ref{evaluation-warmstarting-figure}b shows the effect of warmstarting on the logistic regression model.
%Since the data size is small, the training time is fast and the total time is mostly dominated by the data processing and start-up time.
%To better show the effect of the warmstarting on the model training, we also include the total number of iterations for training the model on all 11 configurations.
%The warmstarting optimization reduces the number of iterations by a factor of three.
%Figure \ref{evaluation-warmstarting-figure}c shows the total training time for all the 555 random forest models.
%The warmstarting optimization reduces the total training time by one order of magnitude (from 300 seconds to 30 seconds).
%
%\begin{figure}
%\centering
%\begin{subfigure}{.5\linewidth}
%  \includegraphics[width=\linewidth]{../images/experiment-results/warmstarting-lr-5981-runtime.eps}
%  \caption{total training time (lr)}
%  \label{fig:sub1}
%\end{subfigure}%
%\begin{subfigure}{.5\linewidth}
%  \includegraphics[width=\linewidth]{../images/experiment-results/warmstarting-lr-5981-iterations.eps}
%  \caption{total iteration count (lr)}
%  \label{fig:sub1}
%\end{subfigure}
%\begin{subfigure}{.5\linewidth}
%  \includegraphics[width=\linewidth]{../images/experiment-results/warmstarting-rf-8568-time.eps}
%  \caption{total training time (rf)}
%  \label{fig:sub1}
%\end{subfigure}
%\caption{Effect of the warmstarting optimization on the total training time and iteration count. In (a) and (b) we train 11 logistic regression models and in (c) we train 555 random forest model from the configurations that exist in the experiment data.}
%\label{evaluation-warmstarting-figure}
%\end{figure}



%\subsubsection{Search Space Proposal}
%\subsubsection{Improved Bayesian Hyperparameter Tuning}
%In this experiment, we focus on several of the popular machine learning pipelines (flow 7707, 8353, 8315) designed for solving task 31\footnote{https://www.openml.org/t/31}, classifying customers as good or bad credit risks using the German Credit data from the UCI repository \cite{Dua:2017}.
%We extracted the meta-data from the OpenML database which includes all the executions of the pipeline, the value of the hyperparameters, and the evaluation metrics.
%Using the meta-data, we initialize the hyperparameter optimization process with the values of the hyperparameters for each execution and the loss ($1- accuracy$) for the specific execution.
%We then execute the search with a budget of 100 trials, trying to minimize the loss of the OpenML pipelines.
%We repeat this experiment 10 times, for every pipeline.
%Figure \ref{fig-avg-warm-vs-cold-task-31} shows the average of losses of the 100 trials for the 10 experiments.
%Warm starting the search decreases the overall loss of the trials.
%
%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{../images/experiment-results/task31-cold-warm-trials.eps}
%\caption{Loss value of 100 Trials with and without warmstarting}
%\label{fig-avg-warm-vs-cold-task-31}
%\end{figure}

%\subsection{Avoiding Local Optima}
%
%\subsubsection{Adaptive Selection}
%In this experiment, we show the result of our adaptive selection methods on the hyperparameter search process.
%Figure \ref{fig-avg-adaptive-selection-task-31} shows the result of the histogram and random selection method on the hyperparameter search process.
%When compared to the vanilla approach (where full history is used in warmstarting), we see that random selection decrease the error rate of the trials where histogram increases them.
%To make the difference more visible we limit the scope of the loss (y axis) from 0.20 to 0.25.
%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{../images/experiment-results/t31-f7707-adaptive-method-random.eps}
%\caption{Effect of adaptive warmstarting on hyperparameter search process}
%\label{fig-avg-adaptive-selection-task-31}
%\end{figure}
%
%\begin{table}
%\centering
%\begin{tabular}{crrr}
%\hline
%	   Pipeline & Best Loss & Warm & Cold \\ \hline
%        7707 & 0.16 & 1 & 0 \\
%        8315 & 0.18 & 173 & 14\\
%        8353 & 0.18 &0& 1\\ 
%        8568 & 0.15 &0&2\\
%        \hline
%\end{tabular}
%\caption{Best loss and their occurrences for different pipelines using warm starting}
%\end{table}

%Table \ref{table-best-hyperparameters} shows the best loss achieved from the search process for every pipeline on the Task 31.
%Figure \ref{figure-best-hyperparameters} shows the number of time that the search process (with budget of 100) manages to find the best set of hyperparameters that results in the lowest loss value.
%While both with and without warm starting does find the best set of hyperparameters, using warmstarting outperforms the search without wamrstarting and has a higher probability of finding the best hyperparameters.
%
%\begin{minipage}{\columnwidth}
%  \begin{minipage}[m]{0.49\columnwidth}
%   \includegraphics[width=\columnwidth]{../images/experiment-results/task31-cold-starting-warm-besthyperparametersfound.eps}
%    \captionof{figure}{Occurrences of best hyperparameters}
%     \label{figure-best-hyperparameters}
%  \end{minipage}
%  \hspace{0.5cm}
%  \begin{minipage}[m]{0.49\columnwidth}
%    \begin{tabular}{cc}\hline
%      Pipeline & Best Loss \\ \hline
%        7077 & 0.189 \\
%        X1 & XXX \\
%        X2 & XXX \\ \hline
%      \end{tabular}
%      \captionof{table}{Best hyperparameters}
%      \label{table-best-hyperparameters}
%    \end{minipage}
%  \end{minipage}
%\subsection{Data Materialization}
