\section{Evaluation} \label{sec-evaluation} 
\todo[inline]{most of the figures need a bit of post-processing to get the font size on the same scale and improve their quality}
In this section, we evaluate the performance of our collaborative optimizer.
We focus on investigating the effect of our materialization and reuse algorithms on the execution cost of workloads in collaborative environments.
We first describe the setup, i.e., the hardware specification and the experiment workloads.
Then, we show the run-time improvement of our optimizer.
Finally, we investigate the effect of the individual contributions, i.e., materialization and reuse algorithms, on the run-time and storage cost.
\begin{table*}[t]
\begin{tabular}{lp{0.84\textwidth}rr}
\hline
\textbf{$ID$} & \textbf{$Description$}& \textbf{$N$}& \textbf{$S$}   \\
\hline
1 &  \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A real script titled 'start-here-a-gentle-introduction'. It includes several feature engineering operations before training logistic regression, random forest, and gradient boosted tree models.} & 397 & 14.5\\[0.4cm]

2 &   \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A real script titled 'introduction-to-manual-feature-engineering'. It joins multiple datasets, preprocesses the datasets to generate features, and trains gradient boosted tree models on the generated features.} & 406 & 25\\[0.4cm]

3 &   \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A real script titled 'introduction-to-manual-feature-engineering-p2'. It is similar to Workload 2, with the resulting preprocessed datasets having more features.} & 146 & 83.5\\[0.15cm]

4 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A modified version of Workload 1 submitted by the Kaggle user "crldata". It trains a gradient boosted tree with a different set of hyperparameters.} & 280 & 10\\[0.4cm]

5 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A modified version of Workload 1 submitted by the Kaggle user "taozhongxiao". It performs random and grid search for gradient boosted tree model using generated features of Workload 1.} & 402 & 13.8\\[0.4cm]

6 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A custom script based on Workloads 2 and 4. It trains a gradient boosted tree on the generated features of Workload 2.} & 121 & 21\\[0.15cm]

7 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A custom script based on Workload 3 and 4. It trains a gradient boosted tree on the generated features of Workload 3.} & 145 & 83\\[0.15cm]

8 & \parbox[t]{0.84\textwidth}{\linespread{0.5}\selectfont \small A custom script that joins the features of Workloads 1 and 2. Then, similar to Workload 4, it trains a gradient boosted tree on the joined dataset.} & 341 & 21.1\\
\hline
\end{tabular}
\caption{Description of Kaggle workloads. $N$ is number of the artifacts and $S$ is the total size of the artifacts in GB.}
\label{kaggle-workload}
\end{table*}

\subsection{Setup}
We execute all the experiments on a server running Linux Ubuntu with 128 GB of RAM.
We implement a prototype of our system in python 2.7.12.
In the prototype, we implement EG using python's NetworkX 2.2 \cite{hagberg2008exploring}.
Unless specified otherwise, we run every experiment 3 times and report the average results.
We evaluate our proposed optimizations on two different sets of workloads\footnote{workload and experiment scripts will be made available in the camera ready version}.
\todo[inline]{What's the best way of letting the reviewers know we will make the code available? check footnote 3}

\textbf{Kaggle workloads.} 
In the first set of workloads, we recreate the collaborative environment of the use case in Section \ref{sec-background}.
We use a total of 8 workloads.
We introduced three workloads in the use case.
We retrieve two more workloads from the same Kaggle competition.
We also design three workloads based on the existing ones.
Table \ref{kaggle-workload} shows details of the workloads we utilize in our experiments.
The use case has nine datasets with a total size of 2.5 GB\footnote{https://www.kaggle.com/c/home-credit-default-risk/data}.
The combined size of the artifacts after executing all the workloads is around 130 GB.
Unless specified otherwise, we set the materialization budget to 16 GB.
For all the experiments, we execute the workloads in order (from 1 to 8) and report the results.
We use the Kaggle workloads to show the effect of end-to-end optimization, storage-aware and heuristics-based materializations, and our reuse algorithm on executing workloads in collaborative environments.

\textbf{OpenML workloads.} In the OpenML workloads, we retrieve all the scikit-learn pipelines for task 31\footnote{https://www.openml.org/t/31}, i.e., classifying customers as good or bad credit risks using the German Credit data from the UCI repository \cite{asuncion2007uci}.
OpenML refers to ML pipelines as flows and the hyperparameter configurations as setups.
The execution of a flow with a specific setup on a dataset is called a run.
We extracted the first 2000 runs for task 31, which results in 16 unique flows. 
The dataset is small, and the total size of the artifacts after executing the 2000 runs is 1.5 GB.
For all the experiments, we execute the 2000 runs in order and report the result.
We use the OpenML workloads to show the effects of the quality ratio (i.e., $\alpha$ in the utility function for artifact materialization) and model warmstarting on workload execution in collaborative environments.

\subsection{End-to-end Optimization}
In this experiment, we evaluate the impact of our collaborative optimizer on the use case of Section \ref{sec-background}.
In the use case, we describe three workloads (Workloads 1-3 of Table \ref{kaggle-workload}).
Kaggle reports users have copied and modified these workloads a total of 7000 times.
Therefore, at the very least, users execute these workloads 7000 times.
Figures \ref{exp-reuse-kaggle-same-workload}(a)-(c) show the result of repeating the execution of each workload twice.
Before the first run, EG is empty; therefore, both the baseline (KG) and our collaborative optimizer (CO) have similar run-times.
\todo[inline]{I need to investigate why we are better for run 1 of Workload 2 and 3 but worse for run 1 of Workload 1, (I know why we are better in 2 and 3, once I figure out the behavior of workload 1 I will update the text).}
By materializing and reusing the artifacts, CO reduces the run-time of the consecutive runs by one order of magnitude for workloads 2 and 3.
Workload 1 executes an external visualization command that computes a bivariate kernel density estimate, which incurs a large overhead.
Since our collaborative optimizer does not materialize such external information, it must re-execute the visualization command; thus, resulting in a smaller run-time reduction.
\begin{figure}
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/start_here_a_gentle_introduction.pgf}%
}
\caption{Workload 1}
\end{subfigure}%
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering.pgf}%
}
\caption{Workload 2}
\end{subfigure}%
\begin{subfigure}[b]{0.33\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/repetition/introduction_to_manual_feature_engineering_p2.pgf}%
}
\caption{Workload 3}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/execution_time/different_workloads.pgf}%
}
\caption{Execution of Kaggle workloads in sequence}
\end{subfigure}
\caption{Execution of Kaggle workloads (materialization budget = 16 GB). KG = Kaggle's infrastructure (our baseline), CO = Collaborative Optimizer.}
\label{exp-reuse-kaggle-same-workload}
\end{figure}

Figure \ref{exp-reuse-kaggle-same-workload}(d) shows the cumulative run-time of executing the workloads of Table \ref{kaggle-workload} consecutively.
%This corresponds to a real scenario, where after some scripts in a collaborative environment gain popularity, i.e., Workloads 1-3 (after 7000 , other users modify and improve the scripts, i.e., Workloads 4-8.
Workloads 4-8 operate on the artifacts generated in Workloads 1-3; thus, instead of recomputing those artifacts, our collaborative optimizer reuses the existing artifacts.
As a result, the cumulative run-time of running the 8 workloads decreases by more than 50\%.
The experiment shows that optimizing even a single execution of each workload has an impact on the total run-time.
In a real collaborative environment, there are hundreds of more modified scripts and possibly thousands of repeated execution of such scripts, resulting in 1000s of hours of reduction in the cumulative run-time.

\subsection{Materialization}
In this set of experiments, we investigate the impact of our materialization algorithms on storage and run-time.

\textbf{Effect of Materialization on Storage.}
Typically, in a real collaborative environment, deciding on a reasonable materialization budget requires knowledge of the expected size of the artifacts, number of the users in a collaborative environment, and rate of incoming workloads.
In this experiment, we show that even with a small budget, our materialization algorithms, particularly our storage-aware algorithm, store a large portion of the artifacts that reappear in future workloads.
We hypothesize that there is considerable overlap between columns of different dataset artifacts in ML workloads.
Therefore, the actual total size of the artifacts our storage-aware algorithm materializes is larger than the specified budget.
We run the workloads of Table \ref{kaggle-workload} under different materialization budgets using both our heuristics-based and storage-aware materialization algorithms.
Figures \ref{exp-sa-vs-simple-size}(a)-(d) show the real size of the stored artifacts under different materialization budgets for the heuristics-based (HM) and storage-aware (SA) algorithms.
Furthermore, to show the total size of the materialized artifacts, we also implement an all materializer (represented by ALL in the figure).
The all-materializer materializes every artifact in EG.
In HM, the maximum real size is always equal to the budget.
However, in SA, we observe that the real size of the stored artifacts reaches up to 8 times the budget.
With a materialization budget of 8 GB, SA materializes almost half of the total artifacts.
When the materialization budget is more than 8 GB, SA materializes nearly all the artifacts.
This indicates that there is considerable overlap between the artifacts of ML workloads.
By deduplicating the artifacts, our storage-aware materialization can materialize more artifacts.

\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-8.pgf}%
}
\caption{Budget = 8 GB}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-16.pgf}%
}

\caption{Budget = 16 GB}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-32.pgf}%
}

\caption{Budget = 32 GB}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/size-64.pgf}%
}
\caption{Budget = 64 GB}
\end{subfigure}
\caption{Real size of the stored artifact for storage-aware and heuristics-based materializations under different budgets (SA = storage-aware, HM = heuristics-based)  \todo[inline]{Need to properly explain the sudden rise at workload 7 for SA budget = 16, also in budget 8, SA drops from 2 to 3.}}
\label{exp-sa-vs-simple-size}
\end{figure}

\textbf{Effect of Materialization on Run-time.}
Figure \ref{run-time-vs-mat} shows the total run-time of all the workloads under different budgets for heuristics-based and storage-aware algorithms.
We also plot the total run-time for the all-materializer.
Even with a materialization budget of 8 GB, SA performs similar to the scenario where all the artifacts are materialized.
On the contrary, under a small materialization budget ($\leq 16$), the HM performs 50\% worse than the other scenarios.
This is mainly because many of the preprocessed dataset artifacts are large, e.g., in Workload 3, some artifacts are more than 5 GB.
Most of these artifacts differ with each other in only a few columns.
However, the HM is unable to exploit this similarity and chooses not to materialize any of the large artifacts.
Recomputing these artifacts is costly, which results in a larger total run-time for HM under a smaller budget.

\begin{figure}
\centering
 \resizebox{0.7\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/materialization/run-time.pgf}%
}
\caption{Total run time of the workloads with different materialization algorithms and budgets}
\label{run-time-vs-mat}
\end{figure}

\textbf{Model Materialization. }
One goal of our materialization algorithms is to materialize high-quality models as soon as they appear in a new workload.
In this experiment, we run a typical scenario, where users compare the score of their models with the score of the best performing model in the collaborative environment.
We execute all the OpenML workloads one by one and keep track of the best performing workload.
In every workload, as a post-processing step, we compare the score of the model with the score of the best performing model so far.
\todo[inline]{Will re-run this experiment with different $\alpha$ values (Utility function in Section \ref{subsec-ml-based-materialization}}
Figure \ref{exp-model-materialization}(a) shows the total run-time of executing all the OpenML workloads and comparing the model of each workload with the best performing model so far.
In the baseline (OML), for every workload, we have to rerun the pipeline of the best performing model.
As a result, OML has a much higher total run-time when compared with our collaborative optimizer (CO).
Figure \ref{exp-model-materialization}(b) explains the reason for such a large difference between OML and CO.
When CO encounters a model that performs better than all the existing models, the materialization algorithm immediately stores the pipeline and the model.
As a result, we reuse the model from EG instead of re-running all the operations.
In Figure \ref{exp-model-materialization}(b), we observe that reusing the best pipeline for all the 2000 OpenML workloads has an overhead of 65 seconds.
In comparison, re-executing the best pipeline for OML has an overhead of 2000 seconds.

\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/model_materialization/cumulative-runtime.pgf}%
}
\caption{Combined run-time}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/model_materialization/bestpipeline-overhead.pgf}%
}
\caption{Best pipeline run-time}
\end{subfigure}
\caption{Effect of model materialization for OpenML workloads (CO = Collaborative Optimizer, OML = OpenML Default)}
\label{exp-model-materialization}
\end{figure}

\subsection{Reuse}
In this experiment, we evaluate the performance of our reuse algorithm.
We compare our linear time reuse algorithm (LN) with two baselines.
In the first baseline (ALL\_M), we reuse every materialized artifact.
In the second baseline (ALL\_C), we recompute every materialized artifact.
Figure \ref{reuse-experiment} shows the total run-time of the Kaggle workloads with different reuse approaches.
The performance of ALL\_C is similar to when there is no Experiment Graph.
Thus, ALL\_C finishes the execution of all the Kaggle workloads in around 2000.
ALL\_M has a similar performance to LN until workload 7.
In workload 7, there are multiple large artifacts which have a larger load cost than compute cost.
As a result, our reuse algorithm chooses to recompute these artifacts; thus, resulting in a smaller cumulative run-time when compared with ALL\_M.
In this experiment, EG is inside the memory of the machine; thus, load times are generally low.
In scenarios where EG is on disk, we expect LN to outperform ALL\_M with a larger margin due to the very large load cost of some of the materialized artifacts.
\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/reuse/simple-mat.pgf}%
}
\caption{Heuristics-based}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/kaggle_home_credit/reuse/storage-aware-mat.pgf}%
}

\caption{Storage-aware}
\end{subfigure}
\caption{Comparison of our linear reuse algorithm with two baselines under different materialization schemes (LN = linear, ALL\_C = all compute, ALL\_M = all materialized).}
\label{reuse-experiment}
\end{figure}

\subsection{Warmstarting}
In this experiment, we evaluate the effect of our warmstarting method.
We execute all the OpenML workloads with and without warmstarting.
Figure \ref{exp-model-warmstarting} shows the effect of warmstarting on the OpenML workloads.
In Figure \ref{exp-model-warmstarting}(a), we observe that without warmstarting, the cumulative run-time of the baseline (OML) and our optimizer (CO-W) is nearly identical.
Warmstarting (CO+W) reduces the cumulative run-time by a third.
In OpenML workloads, because of the small size of the datasets, the run-time of the data transformation operations is very small, i.e., typically only a few milliseconds.
The model training operations are the main contributors to the total run-time.
Another characteristic of the OpenML workloads is that nearly every model training operation has a unique set of hyperparameters.
For example, out of the 2000 workloads that we execute, only 4 have similar hyperparameters.
Thus, the result of the model training operations cannot be reused, which results in CO-W having the same cumulative run-time as OML.
To further show that warmstarting is the main reason behind the smaller cumulative run-time of CO+W, we also plot the total number of executed operations in Figure \ref{exp-model-warmstarting}(b).
There are, on average, 25 operations in every OpenML workload.
OML does not reuse any artifacts; thus, it has to execute every operation.
In CO-W or CO+W, the average number of executed operations per workload is 7, i.e., 4 times smaller than the number of operations in OML.
Since the reuse procedure in CO-W and CO+W is identical, the cumulative number of executed operations is also identical.
However, due to the small run-time of preprocessing operations in OpenML workflow, we only observe an improvement in run-time with warmstarting.  



\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/warmstarting/warmstarting.pgf}%
}
\caption{Run Time}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\centering
 \resizebox{\columnwidth}{!}{%
\input{../images/experiment-results/openml/operations_count/operations_count.pgf}%
}
\caption{Num of Operations}
\end{subfigure}
\caption{Effect of warmstarting on total run time}
\label{exp-model-warmstarting}
\end{figure}
