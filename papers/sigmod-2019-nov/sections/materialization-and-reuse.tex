\section{Materialization and Reuse of Operations}\label{sec-materializaiton-and-reuse}
With the experiment graph constructed, we can look for optimization opportunities for feature engineering and model building operations.
The experiment graph contains all the historical data operations.
By materializing the result of the historical operations we can reuse them for future workloads.
We propose three optimizations, namely, \textit{reuse}, \textit{warmstarting}, \textbf{partial-warmstarting}.

\subsection{Materialization of the Experiment Graph}
Depending on the number of the executed workloads, the generated artifacts may require a large amount of storage space.
For example, the Home Credit Default Risk Kaggle competition, we inspected X popular scripts and observed that they generate up to XXX gigabytes of artifacts. 
\todo[inline]{We need to do a quick experiment to get the proper estimate.}
herefore, materializing every artifact is not feasible.
In this section, we discuss our algorithm for materializing a subset of the artifacts under limited storage.
The goal of the algorithm is to materialize the artifacts that result in the lowest weighted recreation cost while ensuring the total size of the materialized artifacts does not exceed the storage capacity.
We define the weighted recreation cost of the graph $\mathcal{G}$, given the set of materialized vertices $\mathcal{MV}$ as 
\[
weighted\_recreation\_cost(\mathcal{G}, \mathcal{MV}) =  \sum\limits_{e \in \{e' \in \mathcal{E}  \lvert dest(e') \notin \mathcal{MV}\}}  e.f \times e.t
\]
where $dest(e)$ represent the destination vertex of the edge $e$.
The weighted recreation cost indicates how much time do we need to spend to execute all the operations in the graph, since the beginning of time.
For un-materialized artifacts, we must consider the frequency of the operations that produce the artifact.
For example, in Figure \ref{fig-experiment-graph}, if we do not materialize $v_4$ and the operation \textit{count\_vectorizer} has a frequency of 10, we must consider all 10 executions of the operation when computing the weighted cost.
Whereas, if $v_4$ is materialized, the \textit{count\_vectorizer} operation has no impact on the weighted recreation cost.
We propose a local greedy algorithm to the problem.
\todo[inline]{show that it is NP hard and cite \cite{bhattacherjee2015principles} and state the differences in their work and solution and ours}
%Bhattacherjee et al. \cite{bhattacherjee2015principles} tackle a similar problem and prove that the problem is NP-hard.
%They provide a greedy solution for the problem as well.
%However, there differences in both the use cases and problem formulation that make their solution not feasible in our case.

\begin{algorithm}[h]
\caption{Materialization of Artifacts}\label{algorithm-materialization}
\begin{algorithmic}[1]
\Require  $\mathcal{G(V,E)}=$ experiment graph, $\mathcal{B}=$ storage limit
\Ensure $\mathcal{MV}=$ set of materialized artifacts
\State $T=v_0.size$, $\mathcal{MV} =\{v_0\}$
\Do 
	\State $\mathcal{CV} = \{v \in \mathcal{V} \lvert v \notin \mathcal{MV}, T + v.s \leq \mathcal{B}\}$
	\State $v^* = \argmax\limits_{v \in \mathcal{CV}} \tfrac{\rho(\mathcal{G}, v)}{v.s}$
	\State $\mathcal{MV} = \mathcal{MV} \cup \{v^*\}$
	\State $T = T + v^*.size$
\DoWhile{$\mathcal{CV} \neq \emptyset$}
\end{algorithmic}
\end{algorithm}
Algorithm \ref{algorithm-materialization} shows the details of our method for selecting the vertices to materialize.
First, we start by initializing the materialized vertices set ($\mathcal{MV}$) to contain the root artifact ($v_0$) which represents the raw dataset.
This is essential as many of the feature engineering and model building operations are not invertible.
As a result, we cannot reconstruct the raw dataset if it is not materialized.
Then, while the storage limit is not reached, we materialize vertices with the maximum value of weighted recreation cost over size.
We compute the weighted recreation cost ($\rho$) of the vertex $v$ as, 
\[
\rho(\mathcal{G}, v) = \alpha(\mathcal{G}, v) \times \sum\limits_{e \in path(\mathcal{G}, v_0, v)}  e.t
\]
where $\alpha(\mathcal{G}, v)$ represents the access frequency of the vertex $v$ which is the same as frequency of the edge (or any of the edges in case of merge operation) connected to $v$.
The set $path(\mathcal{G}, v_0, v)$ represents the set of all edges from the root node to the vertex $v$. 
For example, in Figure \ref{fig-materialization-example}, the recreation cost of $v_4$ is $3 \times (0 + 0 + 25 + 10 + 10) = 135$.
The ratio of the weighted recreation cost over size has the unit second per megabyte.
For example, the ratio 10 s/mb for an artifact, indicates that we need to spend 10 seconds to recreate 1 megabyte of the artifact.
Figure \ref{fig-materialization-example} shows an example of the materialization process when the storage capacity is 55.
For $v_0$ we do not compute $\rho$ as $v_0$ is always materialized.

\begin{figure}
\begin{subfigure}{0.5\linewidth}
\centering
\input{../images/tikz/graph-mat-step-1.tex}
\caption{Original Graph}
\label{fig-materialization-example}
\end{subfigure}%
\begin{subfigure}{0.5\linewidth}
\centering
\input{../images/tikz/graph-mat-step-2.tex}
\caption{Materialized Graph}
\label{fig-materialization-example}
\end{subfigure}
\begin{subfigure}{\linewidth}
\begin{tabular}{l||>{\bfseries}r|r|r|r|>{\bfseries}r|>{\bfseries}r|>{\bfseries}r|>{\bfseries}r}
\hline
\textbf{vertex} & $\boldsymbol{v_0}$ & $v_1$ & $v_2$ & $v_3$ & $\boldsymbol{v_4}$ & $\boldsymbol{v_5}$ & $\boldsymbol{v_6}$ & $\boldsymbol{v_7}$  \\
\hline
\textbf{size (mb)}    & 10 & 8 & 2 & 40 & 30 & 1 & 2 & 3            \\
\textbf{$\boldsymbol{\rho}$ (s)} & ---   & 0 & 0& 75 & 135 & 50 & 105 & 150	  \\
\textbf{ratio}& --- &0.0 & 0.0 & 1.875 & 4.5 & 50.0 & 52.5 & 50.0 \\
\hline
\end{tabular}
\caption{List of vertices, their sizes, recreation costs, and the cost over size ratio (Bold vertices are materialized).}
\end{subfigure}
\caption{Artifact materialization based on Algorithm \ref{algorithm-materialization} when storage capacity is 55 (mb)}
\end{figure}

\subsubsection{The Effect of Model Quality on the Materialization Decision}
Since the goal of all the workloads in the experiment graph is to solve the same task (as described in \ref{sec-introduction}), all the machine learning models will be evaluated using the same quality metric.
Therefore, we can utilize the quality of the model in the materialization algorithm.
We propose a simple method for utilizing the model quality in the materialization decision algorithm.
We start by adding a new attribute $q$ to every edge.
The new attribute is computed as follows.
If the edge $e$ belongs to no path that leads to a predictive model, we assign $q$ to the average quality of all the predictive models in the experiment graph.
If $e$ belongs to only one path that leads to a predictive model, then we assign $q$ to the quality of the model.
If $e$ belongs to multiple paths that lead to different predictive models, we assign $q$ to the quality of the model with the maximum quality among all the models.
After computing $q$, we include it in the computation of $\rho$, by multiplying $e.q$ by $e.t$ in the summation. 

\subsection{Reuse Optimization for Feature Engineering Operations and Model building}
Having the experiment graph constructed, we now devise a strategy to detect which operations in the current workload exist in the experiment graph.
When an operation exists in the experiment graph, we directly access the resulting artifact instead of executing the operation.
Before executing a workload, we first utilize the same strategy for constructing the experiment graph, to transform the workload into its graph representation, which results in a directed graph (called $\mathcal{WG}$) that starts at $v_0$.
For every edge starting at $v_0$ in $\mathcal{WG}$, we check if the edge also exists in the experiment graph.
Then, we directly access the furthest vertex from $v_0$ (in terms of the edges in the path from $v_0$ to the vertex) that exists in the experiment graph and skip all the intermediate operations.

%\subsubsection{Non-deterministic operations}
%Some feature engineering operations are non-deterministic (such as count vectorizer).
%Therefore, before the workload is executed we cannot know the resulting columns of such operations.
%However, as described earlier, edges can be uniquely identified by the input vertex and the operation.
%Therefore, for non-deterministic operations, we can look whether the same edge exists in the experiment graph and infer the output vertex without explicitly executing the operation.

%\todo[inline]{Is this even a problem?}
%\subsubsection{Random seed for model building operations}
%Many of model building operations (such as machine learning training operations) rely on random initialization of model parameters or random shuffling of the training data.
%By reusing the existing model building operations, we are discarding the random behavior.
%In some cases, random initialization of the model parameters may yield in a sub-optimal machine learning model which is only converged to a local optima.
%If all the subsequent workloads reuse this existing model, they all have sub-optimal machine learning models.
%We alleviate this problem using two approaches.

\subsection{Warmstarting Optimization For Model Training Operations}
Model training operations include extra hyperparameters that must be set before the training procedure begins.
Two training operations on the same data artifact using the same training algorithm could potentially have very different results based on the value of the hyperparameters.
Therefore, we cannot apply the reuse optimization in cases where the hyperparameters are different.
Instead, we apply the \textit{warmstarting} optimization.
In the warmstarting optimization, before starting the training procedure, we set the model parameters (also referred to as weights) of the workload to the model parameters from the experiment graph.
Warmstarting can greatly reduce the total training time.
However, the type of the machine learning model and the termination criteria play important roles in determining the effect of the warmstarting optimization.
For iterative training algorithms that are minimizing a loss function, there are two termination criteria, namely, the convergence tolerance and the number of iterations.

\subsubsection{Convergence tolerance termination criteria}
When the termination criteria of the model training operation in the workload is set to a specific convergence tolerance value, two scenarios may occur.
In the first scenario, an existing trained model in the experiment graph has already reached the convergence tolerance value.
In this scenario, we expect a large improvement in the training time as the training procedure in the workload will immediately converge.
In the second scenario, no model in the experiment graph has reached the convergence tolerance value.
In this case, we warmstart the model in the workload, to the model in the experiment graph with the highest attained quality.
Therefore, we ensure the training procedure will converge faster.

\subsubsection{Augmenting the experiment graph}
Once the training procedure is finished, we augment the experiment graph with an edge and node representing the new model building operation and resulting model, respectively.
\todo[inline]{We may need a special edge so that we know the training operation was not run from scratch and is the result of warmstarting.}

\subsection{Partial Warmstarting Optimization For Model Training Operations}
A common approach in machine learning workloads is to repeatedly select a different subset of features or create new features from the existing ones and train models on the new features.
As a result, many model training operations operate on overlapping or different set of features.
In the partial warmstarting optimization, we aim to improve the training time (and the quality) by warmstarting only the features that exist in the experiment database.

%\subsection{Reuse Optimization for Model Building Operations}
%Reuse for model building operations is more complicated.
%There are two types of reuse opportunities in the model building operations.
%
%\subsubsection{Exact Reuse}\label{sub-sub-exact-reuse}
%For non-user-defined aggregation operations, we follow the same procedure as the feature engineering processes.
%When the corresponding edge in the experiment graph has the same vertex and (aggregation) operation type, we reuse the result of the operation directly.
%We can also reuse the existing model training operation, if the input columns, algorithm, and all the hyper-parameters are the same.
%
%\subsubsection{Model parameter and hyper parameter warmstarting}\label{sub-sub-model-reuse}
%For the model training operations, 3 scenarios can occur.
%In the \textit{first scenario}, the training algorithm used for training the model has never been used before, therefore no meta-data about it exists in the experiment graph.
%In this scenario, no optimization is possible and the model training operation has to be executed completely.
%In the \textit{second scenario}, the training algorithm and the input columns to the model already exist in the experiment graph, but the specific hyperparameter setting does not.
%In this scenario, we can warmstart the model using the parameters from the corresponding node in the experiment graph.
%This reduces the training time as the model \hl{may} converge faster.
%\todo[inline]{This requires experiment and some math ?}
%In the \textit{third scenario}, the training algorithm and the hyperparameters are the same, but all the input columns do not exist in the corresponding node in the experiment graph.
%In this scenario, we provide partial warmstarting.
%In partial warmstarting, the model parameters corresponding to the columns of the input data that already exist in the experiment graph are warmstarted, and the rest of the parameters are randomly initialized.
%\todo[inline]{This requires experiment and some math ?}


%\subsection{Materialization of Grid Search}
%\todo[inline]{Incomplete}
%In order to analyze whether or not we should materialize parts of the grid search, we first have to unpack it, and compare it with other grid search.
%Then, similar to Section \ref{sub-sec-materialization-of-transformed-data}, we materialize the parts that are executed frequently.
%
%%\subsection{Guided Grid-Search}
%%\todo[inline]{just an idea}
%%By extracting correlation between different parameters and the model quality we can provided a guided grid search, where we can provide some estimate or show the effects of a hyperparameter range on the model quality