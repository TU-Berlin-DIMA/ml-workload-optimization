\section{Introduction} \label{sec-introduction}
% Opening
Machine learning is at the core of academic and industry.
To make sense of the data, one must design and execute a machine learning pipeline which consists of a set of data transformation steps and a model building step.
Furthermore, for each step of the pipeline (data transformation or model building), one has to set several hyperparameters.
The space of the available tools, the data preprocessing methods, the training algorithms, and their hyperparameters is extremely large.
This overwhelms expert data scientist, let alone novice users.

% G
To improve the process of designing and executing machine learning pipelines, the scientific community has started to employ two collaborative approaches.
The goal of these approaches is to increase the collaboration between data scientists, ease reproducibility of experiments, ease teaching data science, and provide an intuitive way to share results.
In the first approach, data science platforms, such as Data World\footnote{https://data.world} and Google colabratory\footnote{https://colab.research.google.com} provide an intuitive way of sharing scripts and allow execution of the scripts on the platform using Jupyter notebooks \cite{Kluyver:2016aa}. 
Users can view other users' scripts and learn how they are solving the same or a similar problem.
Moreover, users can immediately execute other users' scripts to check the results for themselves or modify the scripts to include their own custom code.
This approach allows users to share results and ensures the reproducibility of the experiments.
The popular data science competition platform, Kaggle, utilizes this approach to allow competitors to share ideas and scripts, which ultimately lead to better solutions for the given data problem.
Online education platforms such as Coursera also utilize the same approach to enable hands-on practice for data science courses.
The second approach is more systematic.
In this approach, a tool stores logs of machine learning experiments which includes information about the data, the pipeline components, the training algorithms, the hyperparameters, and the evaluation results in a database, typically referred to as an \textit{experiment database} \cite{Vanschoren2012}.
Data scientists can then query other users' experiments to search for more detailed information such as the types of pipeline components (data transformations and models), hyperparameters, and evaluation for specific data problem.
Examples of these systems are OpenML \cite{vanschoren2014openml}, ModelDB \cite{vartak2016m}, and ProvDB \cite{miao2018provdb}.

% P
Data analytics and machine learning solutions are the results of many iterations of trial-and-error, where one or more users analyze a collection of datasets and apply different transformations to form different hypotheses.
Based on these hypotheses, users design data processing pipelines and training algorithms to process the datasets and train a model.
This process continues until the solution is satisfactory. 
While storing these solutions in a structured manner inside the experiment databases provide valuable insights, there are two problems with it.
First, the final solution only represents a fraction of the executed operations.
For example, a user typically performs several hours of exploratory data analysis only to gain insight into the problem she is trying to solve, before actually designing the final pipelines.
However, most of the existing experiment databases treat these initial analysis as non-essential and only store the final result.
Second, even without storing the non-essential operations, the amount of information available in the database is typically too much to digest.
Therefore, querying the experiment database may create more overhead than just simply running a new experiment, regardless of whether or not some parts of the experiments are repeated.

% S
Contrary to the existing work, we propose to store every executed operation and the resulting artifacts (we define artifacts as any raw, preprocessed, and aggregated data and machine learning models).
Furthermore, we \textbf{automatically} extract information from the experiment database to optimize the process of design and execution of the future machine learning workloads.
We limit the scope of our optimizations to the workloads belonging to the same task.
A task describes what the goal of the machine learning workloads should be (for example, train a classification model on a training dataset that maximizes the $F_1$\footnote{https://en.wikipedia.org/wiki/F1\_score} score on a given evaluation dataset).
While our solution is applicable to multiple tasks, the information gained from one task does not affect the optimizations applied to other tasks.

In order to extract the necessary information from the experiment database with ease, we represent the data inside the experiment database using a graph (called the experiment graph).
In the graph, data artifacts are represented as vertices and operations that map one artifact to the other as edges.

%Two categories of users typically utilize a machine learning system: expert data scientists and novice users.
%Each group has a different style of operation when designing and executing machine learning pipelines and models for solving tasks.
%The expert scientists typically first analyze the data by computing several statistics.
%Based on the initial analysis, they form a hypothesis which leads to the creation of a machine learning pipeline for processing and cleaning the data and a training algorithm for training the final model.
%Moreover, expert users have a good understanding of machine learning models and their hyperparameters.
%As a result, they may utilize the grid search or the random search methods to find promising hyperparameters \cite{bergstra2012random}.
%On the other hand, novice users are less aware of the standard methods for analyzing the data and forming a hypothesis about what pipeline components, what training algorithms, and what hyperparameters to use. 
%They would like to be given hints on what are the good data preprocessing steps, training algorithms, and hyperparameters to utilize for solving a machine learning task.

Our goal is to leverage the experiment graph to speed up the design and execution of machine learning pipelines.
Our solution comprises of two parts.
In the first part, based on the existing workloads in the experiment database, we devise a strategy to materialize artifacts.
In many real-world scenarios, the size of the generated artifacts may increase to the orders of 100s of Gigabytes or Terabytes.
Therefore, under a limited storage capacity, the materialization strategy should materialize the artifacts that have a high probability of reuse in future workloads.

In the second part, given the materialized experiment graph, when presented with a new workload, we detect whether the existing meta-data in the experiment graph and the materialized artifacts can be utilized to optimize the design and execution of the new workload.
Concretely, we utilize the information to optimize the workload design and execution in three ways.
We propose three main optimizations, namely, reuse, warmstarting, and improved hyperparameter tuning.
In reuse, we look for opportunities to reuse an existing materialized artifact and avoid data reprocessing.
Reuse decrease the total data processing time and can enable interactive data analysis, especially during the exploratory data analysis that we expect many repetitions of operations.
In warmstarting, we devise a method to detect if we can warmstart the model in the workload with an existing materialized model artifact.
Warmstarting reduces the convergence time, resulting in faster training time and in some case in a model with better quality.
Lastly, we propose to utilize the experiment graph to improve the process of hyperparameter tuning.
There are 3 common hyperparameter tuning techniques, namely, grid search, random search, and Bayesian hyperparameter search \cite{hutter2011sequential,snoek2012practical}.
All three methods rely on defining a search space before the process of tuning begins.
Setting the search space typically requires a combination of domain knowledge and machine learning process.
As a result, non-expert users struggle with setting an effective search space.
We utilize the meta-data in the experiment database to define (or propose) promising search space to the users.
Moreover, after setting the search space, Bayesian hyperparameter tuning techniques require many initial trials until they start to propose promising hyperparameters.
We utilize the meta-data in the experiment database to initialize the Bayesian hyperparameter tuning process to skip the trials.
As a result, we the Bayesian hyperparameter tuning process proposes promising hyperparameters faster.

%Moreover, recent works have extended the ability of the bayesian hyperparameter optimization to design machine learning pipelines from scratch, typically referred to as automatic machine learning (AutoML) \cite{Feurer15} .
%\todo[inline]{Maybe the highlighted part could be part of the next work}
%\hl{
%In automatic machine learning, instead of just focusing on the hyperparameters of machine learning model, every possible pipeline design decision (such as what preprocessing and feature engineering steps, what training algorithms, and what model hyperparameters ) are treated as hyperparameters.
%Similar to bayesian hyperparameter optimization, automatic machine learning also suffers from a long start-up time.
%Moreover, a major assumption in AutoML is that we have access to no domain knowledge or human input.
%However, by utilizing the information in the experiment database, we automatically inject domain knowledge and \textit{'human'} experience into the process of AutoML.
%Thus, we can increase the efficiency of AutoML by both increasing the performance and quality of the attained solutions.
%While useful to expert users, we argue that this feature can greatly benefit novice users.
%Based on the selected problem, we can recommend users a complete machine learning pipeline.
%Alternatively, if the users have already pre-selected a set of components of the pipeline, we can recommend the best set of components at interactive speed.}

In summary, our contributions are:
\begin{itemize}
\item A system that optimizes the process of design and execution of machine learning worklaods by utilizing an experiment database
\item An algorithm for materializing artifacts in the experiment graph under limited storage capacity
\item Enabling reuse of data operation, which enables interactive data analysis, especially during the exploratory data analysis phase
\item Enabling the warmstarting optimization, which decreases the total model training time and in some cases, may lead to models with higher quality
\item Increase in the efficiency of hyperparameter tuning by incorporating the information from the experiment database, which leads to more promising hyperparameter values
\end{itemize}

The rest of this document is organized as follows.