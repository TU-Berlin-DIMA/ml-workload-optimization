\section{Introduction} \label{sec-introduction}
% Opening
Machine learning is at the core of academic and industry.
To make sense of the data, one must design a machine learning pipeline, select a training algorithm, find appropriate parameters (typically called hyperparameters), and finally utilize the pipeline to solve a task.
The space of available tools, data preprocessing methods, training algorithms, and their parameters are typically large.
This overwhelms expert data scientist, let alone novice users.

% G
Recently, the scientific community has started to embrace a two collaborative approaches to improve the process of designing machine learning pipelines and models.
In the first approach, data science platforms, such as Kaggle\footnote{https://www.kaggle.com/} and Coursera\footnote{https://www.coursera.org}, provide intuitive way of sharing scripts and allow execution of the the scripts on the platform using Jupyter notebooks \cite{Kluyver:2016aa}. 
Users can view other scripts and learn by how other users are solving the same problem.
The second approach is more systematic.
In this approach, a tool stores logs of machine learning experiments which includes information about the data, the algorithms and their parameters, and the results in a database.
Data scientists can then query other users work and view how they solve the same or similar problems.
% P
However, the abundance of information still overwhelms users.
As a result, searching through the experiment database to find how others have solved similar tasks or what is the result of specific data operation or machine learning algorithm on a dataset creates more overhead than if the data scientist executes their hypothesis and see the result for themselves.
% S
Our solution is to utilize established database optimization techniques (such as view materialization and caching) and novel optimization strategies to enhance the user experience by automatically optimize the given workload.

We identify two groups of audiences typically utilize a machine learning system: expert data scientists and novice users.
Each group has a different mode of operation when designing and executing machine learning models for solving tasks.
The expert scientists typically first analyze the data by computing several statistics.
Based on the initial analysis, they form a hypothesis which leads to the creation of a machine learning pipeline and a training algorithm.
Moreover, they also have a good understanding of each component, therefore, they rely on grid search methods to try out many different sets of hyperparameters.
On the other hand, novice users have less knowledge and they would like to try out different models.
They would like to be given hints on what are the good data preprocessing steps, training algorithms, and hyperparameters to apply to the given dataset.

Our key idea is to leverage the experiment database to speed up the data processing and grid search for expert users and use the knowledge of the expert users to enhance the experience of the novice users.
The initial analysis of expert users typically involves many similar steps.
Moreover, the grid that they specify for searching through hyperparameters typically has overlapping branches with other experts.
By analyzing the meta-data of the existing experiments we can determine the frequent transformations and cache them.
This can save time by skipping many of the data operations of the subsequent users.
Moreover, after defining a grid search, we utilize the experiment database to inform the user about the branches that are already computed and provide the result without recomputing them.

Another method for automatically transferring knowledge from the expert to the novice users is through Bayesian optimization of hyperparameters [CITE].
Bayesian optimization of hyperparameters relies on many trials to find a more promising set of hyperparameters. 
Each trial is lengthy and as a result, even a few trials on large datasets may take hours or days [CITE].
Through experiment databases, we can utilize past experiments to warm start the Bayesian optimization process.
For every existing experiment, we extract the parameters and the quality metrics of the given pipeline.
Then, we add the parameters to the Bayesian optimization process.

Our contributions are:
\begin{itemize}
\item A system that optimizes the user experience by utilizing an experiment database
\item Predictive materialization of data transformations to eliminate the reprocessing
\item Increase the efficiency of hyperparameter search optimization  by incorporating the knowledge from the previous searches
\end{itemize}