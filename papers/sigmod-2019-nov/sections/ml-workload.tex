\section{Machine Learning Workloads} \label{sec-ml-workloads}
We define a machine learning workload as a series of exploratory data analysis steps followed by one or multiple model building steps to solve a machine learning \textit{task}.
We define a machine learning task as designing a set of data transformation steps and a machine learning model, to transform the training data and train a machine learning model, where the goal is to maximize (or minimize) a quality metric (or an error metric) on an evaluation dataset.
For example, in the \textit{Titanic: Machine Learning from Disaster} competition in Kaggle\footnote{https://www.kaggle.com/c/titanic}, the task is to create a machine learning pipeline and train a classification model on the Titanic training dataset that can predict if a traveler survived the Titanic disaster, with the goal of maximizing the prediction accuracy on a separate test dataset.
%Typically users analyze the data by first performing different transformations to gain insight into the data.
%Then, based on these analyses, they apply the right set of transformations to the data to train one or multiple models on the transformed training data.
%Therefore, machine learning workloads typically consist of both interactive (exploratory analysis) and long-running processes (hyperparameter tuning and model training).
%Using the experiment database, we can speed up the execution of the machine learning workloads.
%By analyzing the experiment database, we can extract the common transformations on data and materialize them.
%Thus, during the interactive exploratory analysis, we first analyze the workload to look for reuse opportunities and return the materialized data when possible.
%Moreover, using the experiment database, we provide the users with already trained models or promising hyperparameters which decreases the execution time of the hyperparameter search and model training.

The Kaggle platform allows users to submit their workloads in scripts (called notebooks or kernels) to the Kaggle platform.
Users can view, copy (fork) the notebooks, modify them, and submit them for evaluation.
Figure \ref{fig-titanic-script-hierarchy} shows the most popular notebooks and their relationship.
For example, in the Notebook "Best Working Classifier", the author cites the Notebook "Exploring Survival" as his/her inspiration.
The numbers show how many times each script is forked.
The top most-voted notebooks for the Titanic competition have been forked a total of 44,434 times.
This demonstrates that many of the workloads share similar operations.
By utilizing an experiment database, we plan to optimize new workload by automatically removing redundant operations, speed up the model training, and perform more efficient hyperparameter tuning.
\begin{figure}
\centering
\includegraphics[width=\columnwidth]{../images/kaggle-titanic-scripts-graph}
\caption{The fork hierarchy of some of the popular notebooks in Kaggle's Titanic competition}
\label{fig-titanic-script-hierarchy}
\end{figure}

In order to apply the materialization optimization, we first define the characteristics of the machine learning workloads.
Then, we specify how to detect reuse opportunities from the experiment database.

\subsection{Operations in Machine Learning Workloads}
We assume the main units of work are dataframe like objects that contain one or many columns, where all the data items in one column are of one data type.
We divide the operations in the ML workloads into 3 categories.
\begin{table}
\centering
\begin{tabular}{ll}
\hline
	   Feature Extraction & Feature Selection\\ \hline
        feature hasher & variance threshold  \\
        one hot encoding & select k best \\
        count vectorizer& select percentile \\ 
        tfidf transformer & recursive feature elimination \\
        hashing vectorizer & select from model \\
        extract\_patch\_2d &  \\
        \hline
\end{tabular}
\caption{List of feature extraction and feature selection operations}\label{feature-engineering-operations}
\end{table}

\textbf{1. Data transformation and Feature engineering:}
\begin{itemize}
\item simple data transformations
\item feature selection operations
\item feature extraction operations
\item user-defined feature engineering operations
\end{itemize}
Table \ref{feature-engineering-operations} shows the list common feature extraction and feature selection operations.

\textbf{2. Model Training: }
\begin{itemize}
\item model training operation that applies a training algorithm to a dataset
%\item common aggregation operations (such as mean, max, min, and percentile)
%\item user-defined aggregation operations 
\end{itemize}

Each model building operation results in objects that can either be used in other feature engineering operations (applying PCA to data or categorizing a column based on values in percentile aggregate) or can be a complete machine learning model that can be used to make predictions on unseen data.
\todo[inline]{How can we capture these in the graph? maybe special edges that connect these nodes to a data node? Do we even want this in this version or should we leave it for future work? }

\textbf{3. Hyperparameter Tuning:}
The goal of hyperparameter tuning operations are to find the models with the highest quality metric.
A hyperparameter tuning operation is defined by a budget and a search method.
The budget specifies how many models the operation should train and the search method specifies the what search strategy should be incorporated.
We limit our focus to popular search methods, namely, grid search, random search, and Bayesian hyperparameter search.

%%% Continue from here
\subsection{Graph Representation}\label{sub-graph-construction}
\todo[inline]{FIX the text based on the new Supernode concept (merge)}
To efficiently apply our optimizations, we utilize a graph data structure (called the experiment graph) to store the meta-data and logs of the machine learning workloads.
Let $\mathcal{V}=\{v_i\}, i = 1, \cdots, n$ be a collection of artifacts that exist in the experiment database.
Each artifact is either a raw dataset, a pre-processed dataset resulting from a feature engineering operation, or a model resulting from a model training operation.
Let $\mathcal{E}=\{e_i\}, i = 1, \cdots, m$ be a collection of executed operations that exist in the experiment database.
A directed edge $e$ from $v_i$ to $v_j$ in $\mathcal{G}(\mathcal{V},\mathcal{E})$ indicates that the artifact $v_j$ is (fully or partially) derived from the artifact $v_i$ by applying the operation in $e$.
Every vertex $v$ has the attribute $\langle s \rangle$ (accessed by $v.s$), which represents the storage size of artifact when materialized.
Every edge $e$ has the attributes $\langle f, t\rangle$ (accessed by $e.f$ and $e.t$), where $f$ represents the frequency of the operation (the number of times the operation has been executed) and $t$ represents the average run-time (in seconds) of the operation.
Furthermore, each vertex contains meta-data about the artifacts (such as the name and type of the columns for datasets and name, size, value of parameters and hyperparameters, and loss value of the models) and each edge contains the meta-data about the operation (such as the function name, training algorithm, hyperparameters, and in some cases even the source code of the operation).
When a new machine learning workload is executed, we extend the graph to capture the new operations and artifacts.
If an operation already exists in the graph, we update the frequency and average run-time attributes.
Otherwise, we add a new edge and vertex to the experiment graph, representing the new operation and the artifact.
Figure \ref{fig-experiment-graph} and  shows an example graph constructed from the code in Listing \ref{listing-experiment-graph} based on the Avito demand prediction challenge\footnote{https://www.kaggle.com/c/avito-demand-prediction/}.
To uniquely identify an edge, we utilize a hash function that receives as input the operation and hyperparameters.

\begin{lstlisting}[language=Python, caption=Example script,captionpos=b,label = {listing-experiment-graph}]
import numpy as np
import pandas as pd

from sklearn import svm
from sklearn.feature_selection import SelectKBest
from sklearn.feature_extraction.text import CountVectorizer

train = pd.read_csv('../input/train.csv') 
print train.columns # [ad_desc,ts,u_id,price,y]
vectorizer = CountVectorizer()
count_vectorized = vectorizer.fit_transform(train['ad_desc'])
selector =  SelectKBest(k=2)
top_features = selector.fit_transform(train[['ts','u_id','price']], 
				      train['y'])
X = pd.concat([count_vectorized,top_features], axis = 1)
model = svm.SVC()
model.fit(X, train['y'])
\end{lstlisting}

%\begin{figure}
%\centering
%\input{../images/tikz/example-graph.tex}
%\caption{Experiment graph constructed from the Listing \ref{listing-experiment-graph}}
%\label{fig-experiment-graph}
%\end{figure}
\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
\input{../images/tikz/example-graph.tex}
\caption{}
\end{subfigure}%
\begin{subfigure}[b]{0.5\linewidth}
\begin{tabular}{l|l}
\textbf{operation} & \textbf{hash}\\
\hline
project(ad..) & p1 \\
project(ts, .., y) & p2\\
project(y) & p3\\
count\_... & c1 \\
select\_... & s1 \\
concat & c2 \\
merge & m\\
fit\_svm & f1
\end{tabular}
\caption{}
\end{subfigure}
\caption{Experiment graph constructed from the Listing \ref{listing-experiment-graph} (a) and the hash of the operations in the scripts (b)}
\label{fig-experiment-graph}
\end{figure}
Table \ref{fig-experiment-graph}b shows the (arbitrary) hash of the operations and their hyperparameters.
To uniquely identify vertices we utilize the same mechanism as in the Trie data structure \cite{brass2008advanced}.
Trie is a search tree where every vertex is identified by the concatenation of edge names starting from the root node.
For example in Figure \ref{fig-experiment-graph}a, vertex $v_4$'s identifier is $p1c1$.
Our optimization methods (described) rely on locating vertices in the experiment graph quickly.
By utilizing a Trie like naming convention, we can locate any node by performing a walk from the root node using the edges names.

We make two important assumptions when constructing the graph from the workloads.
First, most of the operations are applied to one or a subset of the columns of a dataset.
In this case, we augment the graph with an edge representing a projection operation that selects the required subset of the columns.
The run-time ($t$) attribute of this edge is set to 0 as this operation is never actually executed.
Second, multiple edges merging into one vertex indicates an operation that receives as input multiple artifacts and outputs one artifact.
This is a critical assumption for our materialization algorithm in Section \ref{subsec-materialization}
Currently, we support three types of merge operations:
\begin{itemize}
\item \textbf{concat}: a \textit{physical} operation that concatenates the columns of the input datasets (e.g., concatenation of $v_4$ and $v_5$ in Figure \ref{fig-experiment-graph} which represents Pandas' \textit{concat} operation).
\item \textbf{join}: a \textit{physical} operation that joins multiple datasets on a given key.
\item \textbf{combine}: a \textit{logical} operation that combines the input datasets. The combine operation is used to simplify the representation of operations that require multiple artifacts as input (e.g., combining $v_6$ and $v_3$ in Figure \ref{fig-experiment-graph} which simplifies the representation of \textit{model.fit} of scikit-learn library).
\end{itemize}
Since the \textit{combine} operation is logical, the run-time attribute ($t$) of every participating edge is set to $0$.
For the \textit{join} and \textit{concat} operations, the frequency attribute of every participating edge is set to the frequency of the merge operation.
However, the run-time attribute of every participating edge is set to the average run-time of the merge operation divided by the number of the participating edges (e.g., in Figure \ref{fig-experiment-graph}, if the \textit{concat} operation between $v_4$ and $v_5$ is executed 6 times with an average run-time of 10 seconds, the edges connecting $v_4$ to $v_6$ and $v_5$ to $v_6$ have the attributes $\langle6, 5\rangle$).
Furthermore, to construct the unique identifier for merge vertices, we concatenate every path starting from the root node.
For example, in Figure \ref{fig-experiment-graph}a, the identifier of $v_6$ is $p1c1c2p2s1c2$.

It is important to note that based our definition of a task in Section \ref{sec-introduction} if the experiment database contains meta-data about multiple tasks, the constructed graph will contain one connected component for every task.
In the next sections, we assume that the experiment database contains information about 1 task, although, all the methods described can be applied to multiple tasks as well.

\subsection{Materialization of the Experiment Graph}\label{subsec-materialization}
\todo[inline]{Fix the figure and text based on the new merge concept (merge edge have 0 weight)}
Depending on the number of the executed workloads, the generated artifacts may require a large amount of storage space.
For example, the Home Credit Default Risk Kaggle competition, we inspected X popular scripts and observed that they generate up to XXX gigabytes of artifacts. 
\todo[inline]{We need to do a quick experiment to get the proper estimate.}
herefore, materializing every artifact is not feasible.
In this section, we discuss our algorithm for materializing a subset of the artifacts under limited storage.
The goal of the algorithm is to materialize the artifacts that result in the lowest weighted recreation cost while ensuring the total size of the materialized artifacts does not exceed the storage capacity.
We define the weighted recreation cost of the graph $\mathcal{G}$, given the set of materialized vertices $\mathcal{MV}$ as 
\[
weighted\_recreation\_cost(\mathcal{G}, \mathcal{MV}) =  \sum\limits_{e \in \{e' \in \mathcal{E}  \lvert dest(e') \notin \mathcal{MV}\}}  e.f \times e.t
\]
where $dest(e)$ represent the destination vertex of the edge $e$.
The weighted recreation cost indicates how much time do we need to spend to execute all the operations in the graph, since the beginning of time.
For un-materialized artifacts, we must consider the frequency of the operations that produce the artifact.
For example, in Figure \ref{fig-experiment-graph}, if we do not materialize $v_4$ and the operation \textit{count\_vectorizer} has a frequency of 10, we must consider all 10 executions of the operation when computing the weighted cost.
Whereas, if $v_4$ is materialized, the \textit{count\_vectorizer} operation has no impact on the weighted recreation cost.
We propose a local greedy algorithm to the problem.
\todo[inline]{show that it is NP hard and cite \cite{bhattacherjee2015principles} and state the differences in their work and solution and ours}
%Bhattacherjee et al. \cite{bhattacherjee2015principles} tackle a similar problem and prove that the problem is NP-hard.
%They provide a greedy solution for the problem as well.
%However, there differences in both the use cases and problem formulation that make their solution not feasible in our case.

\begin{algorithm}[h]
\caption{Materialization of Artifacts}\label{algorithm-materialization}
\begin{algorithmic}[1]
\Require  $\mathcal{G(V,E)}=$ experiment graph, $\mathcal{B}=$ storage limit
\Ensure $\mathcal{MV}=$ set of materialized artifacts
\State $T=v_0.size$, $\mathcal{MV} =\{v_0\}$
\Do 
	\State $\mathcal{CV} = \{v \in \mathcal{V} \lvert v \notin \mathcal{MV}, T + v.s \leq \mathcal{B}\}$
	\State $v^* = \argmax\limits_{v \in \mathcal{CV}} \tfrac{\rho(\mathcal{G}, v)}{v.s}$
	\State $\mathcal{MV} = \mathcal{MV} \cup \{v^*\}$
	\State $T = T + v^*.size$
\DoWhile{$\mathcal{CV} \neq \emptyset$}
\end{algorithmic}
\end{algorithm}
Algorithm \ref{algorithm-materialization} shows the details of our method for selecting the vertices to materialize.
First, we start by initializing the materialized vertices set ($\mathcal{MV}$) to contain the root artifact ($v_0$) which represents the raw dataset.
This is essential as many of the feature engineering and model building operations are not invertible.
As a result, we cannot reconstruct the raw dataset if it is not materialized.
Then, while the storage limit is not reached, we materialize vertices with the maximum value of weighted recreation cost over size.
We compute the weighted recreation cost ($\rho$) of the vertex $v$ as, 
\[
\rho(\mathcal{G}, v) = \alpha(\mathcal{G}, v) \times \sum\limits_{e \in path(\mathcal{G}, v_0, v)}  e.t
\]
where $\alpha(\mathcal{G}, v)$ represents the access frequency of the vertex $v$ which is the same as frequency of the edge (or any of the edges in case of merge operation) connected to $v$.
The set $path(\mathcal{G}, v_0, v)$ represents the set of all edges from the root node to the vertex $v$. 
For example, in Figure \ref{fig-materialization-example}, the recreation cost of $v_4$ is $3 \times (0 + 0 + 25 + 10 + 10) = 135$.
The ratio of the weighted recreation cost over size has the unit second per megabyte.
For example, the ratio 10 s/mb for an artifact, indicates that we need to spend 10 seconds to recreate 1 megabyte of the artifact.
Figure \ref{fig-materialization-example} shows an example of the materialization process when the storage capacity is 55.
For $v_0$ we do not compute $\rho$ as $v_0$ is always materialized.

\begin{figure}
\begin{subfigure}{0.5\linewidth}
\centering
\input{../images/tikz/graph-mat-step-1.tex}
\caption{Original Graph}
\label{fig-materialization-example}
\end{subfigure}%
\begin{subfigure}{0.5\linewidth}
\centering
\input{../images/tikz/graph-mat-step-2.tex}
\caption{Materialized Graph}
\label{fig-materialization-example}
\end{subfigure}
\begin{subfigure}{\linewidth}
\begin{tabular}{l||>{\bfseries}r|r|r|r|>{\bfseries}r|>{\bfseries}r|>{\bfseries}r|>{\bfseries}r}
\hline
\textbf{vertex} & $\boldsymbol{v_0}$ & $v_1$ & $v_2$ & $v_3$ & $\boldsymbol{v_4}$ & $\boldsymbol{v_5}$ & $\boldsymbol{v_6}$ & $\boldsymbol{v_7}$  \\
\hline
\textbf{size (mb)}    & 10 & 8 & 2 & 40 & 30 & 1 & 2 & 3            \\
\textbf{$\boldsymbol{\rho}$ (s)} & ---   & 0 & 0& 75 & 135 & 50 & 105 & 150	  \\
\textbf{ratio}& --- &0.0 & 0.0 & 1.875 & 4.5 & 50.0 & 52.5 & 50.0 \\
\hline
\end{tabular}
\caption{List of vertices, their sizes, recreation costs, and the cost over size ratio (Bold vertices are materialized).}
\end{subfigure}
\caption{Artifact materialization based on Algorithm \ref{algorithm-materialization} when storage capacity is 55 (mb)}
\end{figure}

\subsubsection{The Effect of Model Quality on the Materialization Decision}
Since the goal of all the workloads in the experiment graph is to solve the same task (as described in \ref{sec-introduction}), all the machine learning models will be evaluated using the same quality metric.
Therefore, we can utilize the quality of the model in the materialization algorithm.
We propose a simple method for utilizing the model quality in the materialization decision algorithm.
We start by adding a new attribute $q$ to every edge.
The new attribute is computed as follows.
If the edge $e$ belongs to no path that leads to a predictive model, we assign $q$ to the average quality of all the predictive models in the experiment graph.
If $e$ belongs to only one path that leads to a predictive model, then we assign $q$ to the quality of the model.
If $e$ belongs to multiple paths that lead to different predictive models, we assign $q$ to the quality of the model with the maximum quality among all the models.
After computing $q$, we include it in the computation of $\rho$, by multiplying $e.q$ by $e.t$ in the summation. 

\subsection{Optimization Workflow}
Figure \ref{fig-system-workflow} shows the workflow of our system.
First, we transform the workload script into its graph representation.
Then we utilize the experiment graph to optimize the workload (using the techniques in Sections \ref{sec-reuse-and-warmstarting} and \ref{sec-hyperparam-optimization}).
This results in a new workload graph which depending on the types of optimizations may have a fewer number of operations (\textit{reuse optimization}, faster operations (\textit{warmstarting}), or operation configurations that lead to higher quality machine learning models (\textit{improved hyperparameter tuning}).
\begin{figure}
\centering
\input{../images/tikz/system-workflow.tex}
\caption{System workflow}
\label{fig-system-workflow}
\end{figure}
%\subsection{Artifacts Unique Identifiers}
%To uniquely identify artifacts, we utilize a similar method to \textit{Trie}\footnote{https://en.wikipedia.org/wiki/Trie}.
%As a result, we can uniquely identify each artifact by performing a walk starting from the root node and concatenate the hash of every operation and its hyperparameters to name a vertex.
%For merge operations (two edges entering a vertex), we  
